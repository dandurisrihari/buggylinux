diff --cc arch/arm64/tools/cpucaps
index 10dcfa13390a,82c7e579a8ba..37b1340e9646
--- a/arch/arm64/tools/cpucaps
+++ b/arch/arm64/tools/cpucaps
@@@ -28,11 -28,10 +28,12 @@@ HAS_GENERIC_AUT
  HAS_GENERIC_AUTH_ARCH_QARMA3
  HAS_GENERIC_AUTH_ARCH_QARMA5
  HAS_GENERIC_AUTH_IMP_DEF
 -HAS_IRQ_PRIO_MASKING
 +HAS_GIC_CPUIF_SYSREGS
 +HAS_GIC_PRIO_MASKING
 +HAS_GIC_PRIO_RELAXED_SYNC
  HAS_LDAPR
  HAS_LSE_ATOMICS
+ HAS_NESTED_VIRT
  HAS_NO_FPSIMD
  HAS_NO_HW_PREFETCH
  HAS_PAN
diff --cc arch/x86/include/asm/cpufeatures.h
index 389ea336258f,cdb7e1492311..73c9672c123b
--- a/arch/x86/include/asm/cpufeatures.h
+++ b/arch/x86/include/asm/cpufeatures.h
@@@ -314,8 -312,9 +314,11 @@@
  #define X86_FEATURE_AVX_VNNI		(12*32+ 4) /* AVX VNNI instructions */
  #define X86_FEATURE_AVX512_BF16		(12*32+ 5) /* AVX512 BFLOAT16 instructions */
  #define X86_FEATURE_CMPCCXADD           (12*32+ 7) /* "" CMPccXADD instructions */
 +#define X86_FEATURE_ARCH_PERFMON_EXT	(12*32+ 8) /* "" Intel Architectural PerfMon Extension */
+ #define X86_FEATURE_FZRM		(12*32+10) /* "" Fast zero-length REP MOVSB */
+ #define X86_FEATURE_FSRS		(12*32+11) /* "" Fast short REP STOSB */
+ #define X86_FEATURE_FSRC		(12*32+12) /* "" Fast short REP {CMPSB,SCASB} */
 +#define X86_FEATURE_LKGS		(12*32+18) /* "" Load "kernel" (userspace) GS */
  #define X86_FEATURE_AMX_FP16		(12*32+21) /* "" AMX fp16 Support */
  #define X86_FEATURE_AVX_IFMA            (12*32+23) /* "" Support for VPMADD52[H,L]UQ */
  
diff --cc arch/x86/kernel/nmi.c
index c315b18ec7c8,e37faba95bb5..776f4b1e395b
--- a/arch/x86/kernel/nmi.c
+++ b/arch/x86/kernel/nmi.c
@@@ -553,96 -525,18 +553,96 @@@ nmi_restart
  
  	if (user_mode(regs))
  		mds_user_clear_cpu_buffers();
 +	if (IS_ENABLED(CONFIG_NMI_CHECK_CPU)) {
 +		WRITE_ONCE(nsp->idt_seq, nsp->idt_seq + 1);
 +		WARN_ON_ONCE(nsp->idt_seq & 0x1);
 +		WRITE_ONCE(nsp->recv_jiffies, jiffies);
 +	}
  }
  
- #if defined(CONFIG_X86_64) && IS_ENABLED(CONFIG_KVM_INTEL)
- DEFINE_IDTENTRY_RAW(exc_nmi_noist)
+ #if IS_ENABLED(CONFIG_KVM_INTEL)
+ DEFINE_IDTENTRY_RAW(exc_nmi_kvm_vmx)
  {
  	exc_nmi(regs);
  }
- #endif
  #if IS_MODULE(CONFIG_KVM_INTEL)
- EXPORT_SYMBOL_GPL(asm_exc_nmi_noist);
+ EXPORT_SYMBOL_GPL(asm_exc_nmi_kvm_vmx);
+ #endif
  #endif
  
 +#ifdef CONFIG_NMI_CHECK_CPU
 +
 +static char *nmi_check_stall_msg[] = {
 +/*									*/
 +/* +--------- nsp->idt_seq_snap & 0x1: CPU is in NMI handler.		*/
 +/* | +------ cpu_is_offline(cpu)					*/
 +/* | | +--- nsp->idt_calls_snap != atomic_long_read(&nsp->idt_calls):	*/
 +/* | | |	NMI handler has been invoked.				*/
 +/* | | |								*/
 +/* V V V								*/
 +/* 0 0 0 */ "NMIs are not reaching exc_nmi() handler",
 +/* 0 0 1 */ "exc_nmi() handler is ignoring NMIs",
 +/* 0 1 0 */ "CPU is offline and NMIs are not reaching exc_nmi() handler",
 +/* 0 1 1 */ "CPU is offline and exc_nmi() handler is legitimately ignoring NMIs",
 +/* 1 0 0 */ "CPU is in exc_nmi() handler and no further NMIs are reaching handler",
 +/* 1 0 1 */ "CPU is in exc_nmi() handler which is legitimately ignoring NMIs",
 +/* 1 1 0 */ "CPU is offline in exc_nmi() handler and no more NMIs are reaching exc_nmi() handler",
 +/* 1 1 1 */ "CPU is offline in exc_nmi() handler which is legitimately ignoring NMIs",
 +};
 +
 +void nmi_backtrace_stall_snap(const struct cpumask *btp)
 +{
 +	int cpu;
 +	struct nmi_stats *nsp;
 +
 +	for_each_cpu(cpu, btp) {
 +		nsp = per_cpu_ptr(&nmi_stats, cpu);
 +		nsp->idt_seq_snap = READ_ONCE(nsp->idt_seq);
 +		nsp->idt_nmi_seq_snap = READ_ONCE(nsp->idt_nmi_seq);
 +		nsp->idt_ignored_snap = READ_ONCE(nsp->idt_ignored);
 +		nsp->idt_calls_snap = atomic_long_read(&nsp->idt_calls);
 +	}
 +}
 +
 +void nmi_backtrace_stall_check(const struct cpumask *btp)
 +{
 +	int cpu;
 +	int idx;
 +	unsigned long nmi_seq;
 +	unsigned long j = jiffies;
 +	char *modp;
 +	char *msgp;
 +	char *msghp;
 +	struct nmi_stats *nsp;
 +
 +	for_each_cpu(cpu, btp) {
 +		nsp = per_cpu_ptr(&nmi_stats, cpu);
 +		modp = "";
 +		msghp = "";
 +		nmi_seq = READ_ONCE(nsp->idt_nmi_seq);
 +		if (nsp->idt_nmi_seq_snap + 1 == nmi_seq && (nmi_seq & 0x1)) {
 +			msgp = "CPU entered NMI handler function, but has not exited";
 +		} else if ((nsp->idt_nmi_seq_snap & 0x1) != (nmi_seq & 0x1)) {
 +			msgp = "CPU is handling NMIs";
 +		} else {
 +			idx = ((nsp->idt_seq_snap & 0x1) << 2) |
 +			      (cpu_is_offline(cpu) << 1) |
 +			      (nsp->idt_calls_snap != atomic_long_read(&nsp->idt_calls));
 +			msgp = nmi_check_stall_msg[idx];
 +			if (nsp->idt_ignored_snap != READ_ONCE(nsp->idt_ignored) && (idx & 0x1))
 +				modp = ", but OK because ignore_nmis was set";
 +			if (nmi_seq & ~0x1)
 +				msghp = " (CPU currently in NMI handler function)";
 +			else if (nsp->idt_nmi_seq_snap + 1 == nmi_seq)
 +				msghp = " (CPU exited one NMI handler function)";
 +		}
 +		pr_alert("%s: CPU %d: %s%s%s, last activity: %lu jiffies ago.\n",
 +			 __func__, cpu, msgp, modp, msghp, j - READ_ONCE(nsp->recv_jiffies));
 +	}
 +}
 +
 +#endif
 +
  void stop_nmi(void)
  {
  	ignore_nmis++;
diff --cc arch/x86/kvm/reverse_cpuid.h
index 81f4e9ce0c77,4945456fd646..a5717282bb9c
--- a/arch/x86/kvm/reverse_cpuid.h
+++ b/arch/x86/kvm/reverse_cpuid.h
@@@ -68,7 -72,7 +72,8 @@@ static const struct cpuid_reg reverse_c
  	[CPUID_12_EAX]        = {0x00000012, 0, CPUID_EAX},
  	[CPUID_8000_001F_EAX] = {0x8000001f, 0, CPUID_EAX},
  	[CPUID_7_1_EDX]       = {         7, 1, CPUID_EDX},
+ 	[CPUID_8000_0007_EDX] = {0x80000007, 0, CPUID_EDX},
 +	[CPUID_8000_0021_EAX] = {0x80000021, 0, CPUID_EAX},
  };
  
  /*