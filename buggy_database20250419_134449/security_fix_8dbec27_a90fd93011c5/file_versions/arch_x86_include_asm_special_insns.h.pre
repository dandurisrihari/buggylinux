commit 873d50d58f67ef15d2777b5e7f7a5268bb1fbae2
Author: Kees Cook <kees@kernel.org>
Date:   Mon Jun 17 21:55:02 2019 -0700

    x86/asm: Pin sensitive CR4 bits
    
    Several recent exploits have used direct calls to the native_write_cr4()
    function to disable SMEP and SMAP before then continuing their exploits
    using userspace memory access.
    
    Direct calls of this form can be mitigate by pinning bits of CR4 so that
    they cannot be changed through a common function. This is not intended to
    be a general ROP protection (which would require CFI to defend against
    properly), but rather a way to avoid trivial direct function calling (or
    CFI bypasses via a matching function prototype) as seen in:
    
    https://googleprojectzero.blogspot.com/2017/05/exploiting-linux-kernel-via-packet.html
    (https://github.com/xairy/kernel-exploits/tree/master/CVE-2017-7308)
    
    The goals of this change:
    
     - Pin specific bits (SMEP, SMAP, and UMIP) when writing CR4.
    
     - Avoid setting the bits too early (they must become pinned only after
       CPU feature detection and selection has finished).
    
     - Pinning mask needs to be read-only during normal runtime.
    
     - Pinning needs to be checked after write to validate the cr4 state
    
    Using __ro_after_init on the mask is done so it can't be first disabled
    with a malicious write.
    
    Since these bits are global state (once established by the boot CPU and
    kernel boot parameters), they are safe to write to secondary CPUs before
    those CPUs have finished feature detection. As such, the bits are set at
    the first cr4 write, so that cr4 write bugs can be detected (instead of
    silently papered over). This uses a few bytes less storage of a location we
    don't have: read-only per-CPU data.
    
    A check is performed after the register write because an attack could just
    skip directly to the register write. Such a direct jump is possible because
    of how this function may be built by the compiler (especially due to the
    removal of frame pointers) where it doesn't add a stack frame (function
    exit may only be a retq without pops) which is sufficient for trivial
    exploitation like in the timer overwrites mentioned above).
    
    The asm argument constraints gain the "+" modifier to convince the compiler
    that it shouldn't make ordering assumptions about the arguments or memory,
    and treat them as changed.
    
    Signed-off-by: Kees Cook <keescook@chromium.org>
    Signed-off-by: Thomas Gleixner <tglx@linutronix.de>
    Cc: Linus Torvalds <torvalds@linux-foundation.org>
    Cc: Peter Zijlstra <peterz@infradead.org>
    Cc: Dave Hansen <dave.hansen@intel.com>
    Cc: kernel-hardening@lists.openwall.com
    Link: https://lkml.kernel.org/r/20190618045503.39105-3-keescook@chromium.org

diff --git a/arch/x86/include/asm/special_insns.h b/arch/x86/include/asm/special_insns.h
index 0a3c4cab39db..c8c8143ab27b 100644
--- a/arch/x86/include/asm/special_insns.h
+++ b/arch/x86/include/asm/special_insns.h
@@ -6,6 +6,8 @@
 #ifdef __KERNEL__
 
 #include <asm/nops.h>
+#include <asm/processor-flags.h>
+#include <linux/jump_label.h>
 
 /*
  * Volatile isn't enough to prevent the compiler from reordering the
@@ -16,6 +18,10 @@
  */
 extern unsigned long __force_order;
 
+/* Starts false and gets enabled once CPU feature detection is done. */
+DECLARE_STATIC_KEY_FALSE(cr_pinning);
+extern unsigned long cr4_pinned_bits;
+
 static inline unsigned long native_read_cr0(void)
 {
 	unsigned long val;
@@ -74,7 +80,21 @@ static inline unsigned long native_read_cr4(void)
 
 static inline void native_write_cr4(unsigned long val)
 {
-	asm volatile("mov %0,%%cr4": : "r" (val), "m" (__force_order));
+	unsigned long bits_missing = 0;
+
+set_register:
+	asm volatile("mov %0,%%cr4": "+r" (val), "+m" (cr4_pinned_bits));
+
+	if (static_branch_likely(&cr_pinning)) {
+		if (unlikely((val & cr4_pinned_bits) != cr4_pinned_bits)) {
+			bits_missing = ~val & cr4_pinned_bits;
+			val |= bits_missing;
+			goto set_register;
+		}
+		/* Warn after we've set the missing bits. */
+		WARN_ONCE(bits_missing, "CR4 bits went missing: %lx!?\n",
+			  bits_missing);
+	}
 }
 
 #ifdef CONFIG_X86_64