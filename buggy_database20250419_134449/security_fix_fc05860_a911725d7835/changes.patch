diff --cc drivers/nvme/host/core.c
index 5eaaa51a5e30,2f45e8fcdd7c..b6f7815fa239
--- a/drivers/nvme/host/core.c
+++ b/drivers/nvme/host/core.c
@@@ -1223,9 -1146,26 +1146,9 @@@ static void nvme_keep_alive_end_io(stru
  		startka = true;
  	spin_unlock_irqrestore(&ctrl->lock, flags);
  	if (startka)
- 		queue_delayed_work(nvme_wq, &ctrl->ka_work, ctrl->kato * HZ);
+ 		nvme_queue_keep_alive_work(ctrl);
  }
  
 -static int nvme_keep_alive(struct nvme_ctrl *ctrl)
 -{
 -	struct request *rq;
 -
 -	rq = nvme_alloc_request(ctrl->admin_q, &ctrl->ka_cmd,
 -			BLK_MQ_REQ_RESERVED);
 -	if (IS_ERR(rq))
 -		return PTR_ERR(rq);
 -
 -	rq->timeout = ctrl->kato * HZ;
 -	rq->end_io_data = ctrl;
 -
 -	blk_execute_rq_nowait(NULL, rq, 0, nvme_keep_alive_end_io);
 -
 -	return 0;
 -}
 -
  static void nvme_keep_alive_work(struct work_struct *work)
  {
  	struct nvme_ctrl *ctrl = container_of(to_delayed_work(work),
diff --cc drivers/nvme/target/tcp.c
index d658c6e8263a,e14235811ba1..f9f34f6caf5e
--- a/drivers/nvme/target/tcp.c
+++ b/drivers/nvme/target/tcp.c
@@@ -1103,11 -1139,10 +1140,9 @@@ static int nvmet_tcp_try_recv_data(stru
  		nvmet_tcp_prep_recv_ddgst(cmd);
  		return 0;
  	}
 -	nvmet_tcp_unmap_pdu_iovec(cmd);
  
- 	if (!(cmd->flags & NVMET_TCP_F_INIT_FAILED) &&
- 	    cmd->rbytes_done == cmd->req.transfer_len) {
- 		cmd->req.execute(&cmd->req);
- 	}
+ 	if (cmd->rbytes_done == cmd->req.transfer_len)
+ 		nvmet_tcp_execute_request(cmd);
  
  	nvmet_prepare_receive_pdu(queue);
  	return 0;