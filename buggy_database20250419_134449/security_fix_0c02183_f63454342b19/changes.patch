diff --cc arch/arm64/include/asm/tlbflush.h
index 55b50e1d4a84,93f4b397f9a1..b149cf9f91bc
--- a/arch/arm64/include/asm/tlbflush.h
+++ b/arch/arm64/include/asm/tlbflush.h
@@@ -364,58 -370,12 +427,13 @@@ static inline void __flush_tlb_range(st
  	dsb(ishst);
  	asid = ASID(vma->vm_mm);
  
- 	/*
- 	 * When the CPU does not support TLB range operations, flush the TLB
- 	 * entries one by one at the granularity of 'stride'. If the TLB
- 	 * range ops are supported, then:
- 	 *
- 	 * 1. If 'pages' is odd, flush the first page through non-range
- 	 *    operations;
- 	 *
- 	 * 2. For remaining pages: the minimum range granularity is decided
- 	 *    by 'scale', so multiple range TLBI operations may be required.
- 	 *    Start from scale = 0, flush the corresponding number of pages
- 	 *    ((num+1)*2^(5*scale+1) starting from 'addr'), then increase it
- 	 *    until no pages left.
- 	 *
- 	 * Note that certain ranges can be represented by either num = 31 and
- 	 * scale or num = 0 and scale + 1. The loop below favours the latter
- 	 * since num is limited to 30 by the __TLBI_RANGE_NUM() macro.
- 	 */
- 	while (pages > 0) {
- 		if (!system_supports_tlb_range() ||
- 		    pages % 2 == 1) {
- 			addr = __TLBI_VADDR(start, asid);
- 			if (last_level) {
- 				__tlbi_level(vale1is, addr, tlb_level);
- 				__tlbi_user_level(vale1is, addr, tlb_level);
- 			} else {
- 				__tlbi_level(vae1is, addr, tlb_level);
- 				__tlbi_user_level(vae1is, addr, tlb_level);
- 			}
- 			start += stride;
- 			pages -= stride >> PAGE_SHIFT;
- 			continue;
- 		}
- 
- 		num = __TLBI_RANGE_NUM(pages, scale);
- 		if (num >= 0) {
- 			addr = __TLBI_VADDR_RANGE(start, asid, scale,
- 						  num, tlb_level);
- 			if (last_level) {
- 				__tlbi(rvale1is, addr);
- 				__tlbi_user(rvale1is, addr);
- 			} else {
- 				__tlbi(rvae1is, addr);
- 				__tlbi_user(rvae1is, addr);
- 			}
- 			start += __TLBI_RANGE_PAGES(num, scale) << PAGE_SHIFT;
- 			pages -= __TLBI_RANGE_PAGES(num, scale);
- 		}
- 		scale++;
- 	}
+ 	if (last_level)
+ 		__flush_tlb_range_op(vale1is, start, pages, stride, asid, tlb_level, true);
+ 	else
+ 		__flush_tlb_range_op(vae1is, start, pages, stride, asid, tlb_level, true);
+ 
  	dsb(ish);
 +	mmu_notifier_arch_invalidate_secondary_tlbs(vma->vm_mm, start, end);
  }
  
  static inline void flush_tlb_range(struct vm_area_struct *vma,
diff --cc arch/x86/include/asm/kexec.h
index 3be6a98751f0,819046974b99..c9f6a6c5de3c
--- a/arch/x86/include/asm/kexec.h
+++ b/arch/x86/include/asm/kexec.h
@@@ -205,28 -205,8 +205,26 @@@ int arch_kimage_file_post_load_cleanup(
  #endif
  #endif
  
- typedef void crash_vmclear_fn(void);
- extern crash_vmclear_fn __rcu *crash_vmclear_loaded_vmcss;
  extern void kdump_nmi_shootdown_cpus(void);
  
 +#ifdef CONFIG_CRASH_HOTPLUG
 +void arch_crash_handle_hotplug_event(struct kimage *image);
 +#define arch_crash_handle_hotplug_event arch_crash_handle_hotplug_event
 +
 +#ifdef CONFIG_HOTPLUG_CPU
 +int arch_crash_hotplug_cpu_support(void);
 +#define crash_hotplug_cpu_support arch_crash_hotplug_cpu_support
 +#endif
 +
 +#ifdef CONFIG_MEMORY_HOTPLUG
 +int arch_crash_hotplug_memory_support(void);
 +#define crash_hotplug_memory_support arch_crash_hotplug_memory_support
 +#endif
 +
 +unsigned int arch_crash_get_elfcorehdr_size(void);
 +#define crash_get_elfcorehdr_size arch_crash_get_elfcorehdr_size
 +#endif
 +
  #endif /* __ASSEMBLY__ */
  
  #endif /* _ASM_X86_KEXEC_H */