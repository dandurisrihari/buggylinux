commit cdc8fcb49905c0b67e355e027cb462ee168ffaa3
Merge: 382625d0d432 fa15bafb71fd
Author: Linus Torvalds <torvalds@linux-foundation.org>
Date:   Mon Aug 3 13:01:22 2020 -0700

    Merge tag 'for-5.9/io_uring-20200802' of git://git.kernel.dk/linux-block
    
    Pull io_uring updates from Jens Axboe:
     "Lots of cleanups in here, hardening the code and/or making it easier
      to read and fixing bugs, but a core feature/change too adding support
      for real async buffered reads. With the latter in place, we just need
      buffered write async support and we're done relying on kthreads for
      the fast path. In detail:
    
       - Cleanup how memory accounting is done on ring setup/free (Bijan)
    
       - sq array offset calculation fixup (Dmitry)
    
       - Consistently handle blocking off O_DIRECT submission path (me)
    
       - Support proper async buffered reads, instead of relying on kthread
         offload for that. This uses the page waitqueue to drive retries
         from task_work, like we handle poll based retry. (me)
    
       - IO completion optimizations (me)
    
       - Fix race with accounting and ring fd install (me)
    
       - Support EPOLLEXCLUSIVE (Jiufei)
    
       - Get rid of the io_kiocb unionizing, made possible by shrinking
         other bits (Pavel)
    
       - Completion side cleanups (Pavel)
    
       - Cleanup REQ_F_ flags handling, and kill off many of them (Pavel)
    
       - Request environment grabbing cleanups (Pavel)
    
       - File and socket read/write cleanups (Pavel)
    
       - Improve kiocb_set_rw_flags() (Pavel)
    
       - Tons of fixes and cleanups (Pavel)
    
       - IORING_SQ_NEED_WAKEUP clear fix (Xiaoguang)"
    
    * tag 'for-5.9/io_uring-20200802' of git://git.kernel.dk/linux-block: (127 commits)
      io_uring: flip if handling after io_setup_async_rw
      fs: optimise kiocb_set_rw_flags()
      io_uring: don't touch 'ctx' after installing file descriptor
      io_uring: get rid of atomic FAA for cq_timeouts
      io_uring: consolidate *_check_overflow accounting
      io_uring: fix stalled deferred requests
      io_uring: fix racy overflow count reporting
      io_uring: deduplicate __io_complete_rw()
      io_uring: de-unionise io_kiocb
      io-wq: update hash bits
      io_uring: fix missing io_queue_linked_timeout()
      io_uring: mark ->work uninitialised after cleanup
      io_uring: deduplicate io_grab_files() calls
      io_uring: don't do opcode prep twice
      io_uring: clear IORING_SQ_NEED_WAKEUP after executing task works
      io_uring: batch put_task_struct()
      tasks: add put_task_struct_many()
      io_uring: return locked and pinned page accounting
      io_uring: don't miscount pinned memory
      io_uring: don't open-code recv kbuf managment
      ...

diff --cc mm/filemap.c
index 991503bbf922,a5b1fa8f7ce4..9f131f1cfde3
--- a/mm/filemap.c
+++ b/mm/filemap.c
@@@ -987,63 -987,17 +987,46 @@@ void __init pagecache_init(void
  	page_writeback_init();
  }
  
- /* This has the same layout as wait_bit_key - see fs/cachefiles/rdwr.c */
- struct wait_page_key {
- 	struct page *page;
- 	int bit_nr;
- 	int page_match;
- };
- 
- struct wait_page_queue {
- 	struct page *page;
- 	int bit_nr;
- 	wait_queue_entry_t wait;
- };
- 
  static int wake_page_function(wait_queue_entry_t *wait, unsigned mode, int sync, void *arg)
  {
 +	int ret;
  	struct wait_page_key *key = arg;
  	struct wait_page_queue *wait_page
  		= container_of(wait, struct wait_page_queue, wait);
 -	int ret;
  
- 	if (wait_page->page != key->page)
- 	       return 0;
- 	key->page_match = 1;
- 
- 	if (wait_page->bit_nr != key->bit_nr)
 -	ret = wake_page_match(wait_page, key);
 -	if (ret != 1)
 -		return ret;
 -	return autoremove_wake_function(wait, mode, sync, key);
++	if (!wake_page_match(wait_page, key))
 +		return 0;
 +
 +	/*
 +	 * If it's an exclusive wait, we get the bit for it, and
 +	 * stop walking if we can't.
 +	 *
 +	 * If it's a non-exclusive wait, then the fact that this
 +	 * wake function was called means that the bit already
 +	 * was cleared, and we don't care if somebody then
 +	 * re-took it.
 +	 */
 +	ret = 0;
 +	if (wait->flags & WQ_FLAG_EXCLUSIVE) {
 +		if (test_and_set_bit(key->bit_nr, &key->page->flags))
 +			return -1;
 +		ret = 1;
 +	}
 +	wait->flags |= WQ_FLAG_WOKEN;
 +
 +	wake_up_state(wait->private, mode);
 +
 +	/*
 +	 * Ok, we have successfully done what we're waiting for,
 +	 * and we can unconditionally remove the wait entry.
 +	 *
 +	 * Note that this has to be the absolute last thing we do,
 +	 * since after list_del_init(&wait->entry) the wait entry
 +	 * might be de-allocated and the process might even have
 +	 * exited.
 +	 */
 +	list_del_init_careful(&wait->entry);
 +	return ret;
  }
  
  static void wake_up_page_bit(struct page *page, int bit_nr)
@@@ -2061,8 -2044,6 +2087,8 @@@ find_page
  
  		page = find_get_page(mapping, index);
  		if (!page) {
- 			if (iocb->ki_flags & (IOCB_NOWAIT | IOCB_NOIO))
++			if (iocb->ki_flags & IOCB_NOIO)
 +				goto would_block;
  			page_cache_sync_readahead(mapping,
  					ra, filp,
  					index, last_index - index);
@@@ -2197,7 -2185,7 +2234,7 @@@ page_not_up_to_date_locked
  		}
  
  readpage:
- 		if (iocb->ki_flags & IOCB_NOIO) {
 -		if (iocb->ki_flags & IOCB_NOWAIT) {
++		if (iocb->ki_flags & (IOCB_NOIO | IOCB_NOWAIT)) {
  			unlock_page(page);
  			put_page(page);
  			goto would_block;