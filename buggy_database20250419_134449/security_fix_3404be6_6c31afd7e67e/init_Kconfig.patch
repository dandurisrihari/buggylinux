commit 3404be67bf73515babd74acd8525d09dafe4234d
Author: Kees Cook <kees@kernel.org>
Date:   Thu Aug 6 23:18:20 2020 -0700

    mm/slab: expand CONFIG_SLAB_FREELIST_HARDENED to include SLAB
    
    Patch series "mm: Expand CONFIG_SLAB_FREELIST_HARDENED to include SLAB"
    
    In reviewing Vlastimil Babka's latest slub debug series, I realized[1]
    that several checks under CONFIG_SLAB_FREELIST_HARDENED weren't being
    applied to SLAB.  Fix this by expanding the Kconfig coverage, and adding a
    simple double-free test for SLAB.
    
    This patch (of 2):
    
    Include SLAB caches when performing kmem_cache pointer verification.  A
    defense against such corruption[1] should be applied to all the
    allocators.  With this added, the "SLAB_FREE_CROSS" and "SLAB_FREE_PAGE"
    LKDTM tests now pass on SLAB:
    
      lkdtm: Performing direct entry SLAB_FREE_CROSS
      lkdtm: Attempting cross-cache slab free ...
      ------------[ cut here ]------------
      cache_from_obj: Wrong slab cache. lkdtm-heap-b but object is from lkdtm-heap-a
      WARNING: CPU: 2 PID: 2195 at mm/slab.h:530 kmem_cache_free+0x8d/0x1d0
      ...
      lkdtm: Performing direct entry SLAB_FREE_PAGE
      lkdtm: Attempting non-Slab slab free ...
      ------------[ cut here ]------------
      virt_to_cache: Object is not a Slab page!
      WARNING: CPU: 1 PID: 2202 at mm/slab.h:489 kmem_cache_free+0x196/0x1d0
    
    Additionally clean up neighboring Kconfig entries for clarity,
    readability, and redundant option removal.
    
    [1] https://github.com/ThomasKing2014/slides/raw/master/Building%20universal%20Android%20rooting%20with%20a%20type%20confusion%20vulnerability.pdf
    
    Fixes: 598a0717a816 ("mm/slab: validate cache membership under freelist hardening")
    Signed-off-by: Kees Cook <keescook@chromium.org>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Acked-by: Vlastimil Babka <vbabka@suse.cz>
    Cc: Alexander Popov <alex.popov@linux.com>
    Cc: Christoph Lameter <cl@linux.com>
    Cc: David Rientjes <rientjes@google.com>
    Cc: Jann Horn <jannh@google.com>
    Cc: Joonsoo Kim <iamjoonsoo.kim@lge.com>
    Cc: Matthew Garrett <mjg59@google.com>
    Cc: Pekka Enberg <penberg@kernel.org>
    Cc: Roman Gushchin <guro@fb.com>
    Cc: Vijayanand Jitta <vjitta@codeaurora.org>
    Cc: Vinayak Menon <vinmenon@codeaurora.org>
    Link: http://lkml.kernel.org/r/20200625215548.389774-1-keescook@chromium.org
    Link: http://lkml.kernel.org/r/20200625215548.389774-2-keescook@chromium.org
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/init/Kconfig b/init/Kconfig
index 9082ed33a9cd..d6a0b31b13dc 100644
--- a/init/Kconfig
+++ b/init/Kconfig
@@ -1913,9 +1913,8 @@ config SLAB_MERGE_DEFAULT
 	  command line.
 
 config SLAB_FREELIST_RANDOM
-	default n
+	bool "Randomize slab freelist"
 	depends on SLAB || SLUB
-	bool "SLAB freelist randomization"
 	help
 	  Randomizes the freelist order used on creating new pages. This
 	  security feature reduces the predictability of the kernel slab
@@ -1923,12 +1922,14 @@ config SLAB_FREELIST_RANDOM
 
 config SLAB_FREELIST_HARDENED
 	bool "Harden slab freelist metadata"
-	depends on SLUB
+	depends on SLAB || SLUB
 	help
 	  Many kernel heap attacks try to target slab cache metadata and
 	  other infrastructure. This options makes minor performance
 	  sacrifices to harden the kernel slab allocator against common
-	  freelist exploit methods.
+	  freelist exploit methods. Some slab implementations have more
+	  sanity-checking than others. This option is most effective with
+	  CONFIG_SLUB.
 
 config SHUFFLE_PAGE_ALLOCATOR
 	bool "Page allocator randomization"