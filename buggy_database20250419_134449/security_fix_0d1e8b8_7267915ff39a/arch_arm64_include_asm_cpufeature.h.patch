commit 0d1e8b8d2bcd3150d51754d8d0fdbf44dc88b0d3
Merge: 83c4087ce468 22a7cdcae6a4
Author: Linus Torvalds <torvalds@linux-foundation.org>
Date:   Thu Oct 25 17:57:35 2018 -0700

    Merge tag 'kvm-4.20-1' of git://git.kernel.org/pub/scm/virt/kvm/kvm
    
    Pull KVM updates from Radim Krčmář:
     "ARM:
       - Improved guest IPA space support (32 to 52 bits)
    
       - RAS event delivery for 32bit
    
       - PMU fixes
    
       - Guest entry hardening
    
       - Various cleanups
    
       - Port of dirty_log_test selftest
    
      PPC:
       - Nested HV KVM support for radix guests on POWER9. The performance
         is much better than with PR KVM. Migration and arbitrary level of
         nesting is supported.
    
       - Disable nested HV-KVM on early POWER9 chips that need a particular
         hardware bug workaround
    
       - One VM per core mode to prevent potential data leaks
    
       - PCI pass-through optimization
    
       - merge ppc-kvm topic branch and kvm-ppc-fixes to get a better base
    
      s390:
       - Initial version of AP crypto virtualization via vfio-mdev
    
       - Improvement for vfio-ap
    
       - Set the host program identifier
    
       - Optimize page table locking
    
      x86:
       - Enable nested virtualization by default
    
       - Implement Hyper-V IPI hypercalls
    
       - Improve #PF and #DB handling
    
       - Allow guests to use Enlightened VMCS
    
       - Add migration selftests for VMCS and Enlightened VMCS
    
       - Allow coalesced PIO accesses
    
       - Add an option to perform nested VMCS host state consistency check
         through hardware
    
       - Automatic tuning of lapic_timer_advance_ns
    
       - Many fixes, minor improvements, and cleanups"
    
    * tag 'kvm-4.20-1' of git://git.kernel.org/pub/scm/virt/kvm/kvm: (204 commits)
      KVM/nVMX: Do not validate that posted_intr_desc_addr is page aligned
      Revert "kvm: x86: optimize dr6 restore"
      KVM: PPC: Optimize clearing TCEs for sparse tables
      x86/kvm/nVMX: tweak shadow fields
      selftests/kvm: add missing executables to .gitignore
      KVM: arm64: Safety check PSTATE when entering guest and handle IL
      KVM: PPC: Book3S HV: Don't use streamlined entry path on early POWER9 chips
      arm/arm64: KVM: Enable 32 bits kvm vcpu events support
      arm/arm64: KVM: Rename function kvm_arch_dev_ioctl_check_extension()
      KVM: arm64: Fix caching of host MDCR_EL2 value
      KVM: VMX: enable nested virtualization by default
      KVM/x86: Use 32bit xor to clear registers in svm.c
      kvm: x86: Introduce KVM_CAP_EXCEPTION_PAYLOAD
      kvm: vmx: Defer setting of DR6 until #DB delivery
      kvm: x86: Defer setting of CR2 until #PF delivery
      kvm: x86: Add payload operands to kvm_multiple_exception
      kvm: x86: Add exception payload fields to kvm_vcpu_events
      kvm: x86: Add has_payload and payload to kvm_queued_exception
      KVM: Documentation: Fix omission in struct kvm_vcpu_events
      KVM: selftests: add Enlightened VMCS test
      ...

diff --cc arch/arm64/include/asm/cpufeature.h
index 6db48d90ad63,072cc1c970c2..7e2ec64aa414
--- a/arch/arm64/include/asm/cpufeature.h
+++ b/arch/arm64/include/asm/cpufeature.h
@@@ -536,7 -530,26 +536,28 @@@ void arm64_set_ssbd_mitigation(bool sta
  static inline void arm64_set_ssbd_mitigation(bool state) {}
  #endif
  
 +extern int do_emulate_mrs(struct pt_regs *regs, u32 sys_reg, u32 rt);
++
+ static inline u32 id_aa64mmfr0_parange_to_phys_shift(int parange)
+ {
+ 	switch (parange) {
+ 	case 0: return 32;
+ 	case 1: return 36;
+ 	case 2: return 40;
+ 	case 3: return 42;
+ 	case 4: return 44;
+ 	case 5: return 48;
+ 	case 6: return 52;
+ 	/*
+ 	 * A future PE could use a value unknown to the kernel.
+ 	 * However, by the "D10.1.4 Principles of the ID scheme
+ 	 * for fields in ID registers", ARM DDI 0487C.a, any new
+ 	 * value is guaranteed to be higher than what we know already.
+ 	 * As a safe limit, we return the limit supported by the kernel.
+ 	 */
+ 	default: return CONFIG_ARM64_PA_BITS;
+ 	}
+ }
  #endif /* __ASSEMBLY__ */
  
  #endif