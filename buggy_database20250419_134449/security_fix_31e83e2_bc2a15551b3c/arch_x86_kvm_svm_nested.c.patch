commit 31e83e21cf00fe5b669eb352ff3ed70e74b40fad
Author: Paolo Bonzini <pbonzini@redhat.com>
Date:   Thu Sep 29 13:20:14 2022 -0400

    KVM: x86: compile out vendor-specific code if SMM is disabled
    
    Vendor-specific code that deals with SMI injection and saving/restoring
    SMM state is not needed if CONFIG_KVM_SMM is disabled, so remove the
    four callbacks smi_allowed, enter_smm, leave_smm and enable_smi_window.
    The users in svm/nested.c and x86.c also have to be compiled out; the
    amount of #ifdef'ed code is small and it's not worth moving it to
    smm.c.
    
    enter_smm is now used only within #ifdef CONFIG_KVM_SMM, and the stub
    can therefore be removed.
    
    Signed-off-by: Paolo Bonzini <pbonzini@redhat.com>
    Reviewed-by: Maxim Levitsky <mlevitsk@redhat.com>
    Message-Id: <20220929172016.319443-7-pbonzini@redhat.com>
    Signed-off-by: Paolo Bonzini <pbonzini@redhat.com>

diff --git a/arch/x86/kvm/svm/nested.c b/arch/x86/kvm/svm/nested.c
index cc0fd75f7cba..b258d6988f5d 100644
--- a/arch/x86/kvm/svm/nested.c
+++ b/arch/x86/kvm/svm/nested.c
@@ -1378,6 +1378,7 @@ static int svm_check_nested_events(struct kvm_vcpu *vcpu)
 		return 0;
 	}
 
+#ifdef CONFIG_KVM_SMM
 	if (vcpu->arch.smi_pending && !svm_smi_blocked(vcpu)) {
 		if (block_nested_events)
 			return -EBUSY;
@@ -1386,6 +1387,7 @@ static int svm_check_nested_events(struct kvm_vcpu *vcpu)
 		nested_svm_simple_vmexit(svm, SVM_EXIT_SMI);
 		return 0;
 	}
+#endif
 
 	if (vcpu->arch.nmi_pending && !svm_nmi_blocked(vcpu)) {
 		if (block_nested_events)