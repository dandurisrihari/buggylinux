commit 31e83e21cf00fe5b669eb352ff3ed70e74b40fad
Author: Paolo Bonzini <pbonzini@redhat.com>
Date:   Thu Sep 29 13:20:14 2022 -0400

    KVM: x86: compile out vendor-specific code if SMM is disabled
    
    Vendor-specific code that deals with SMI injection and saving/restoring
    SMM state is not needed if CONFIG_KVM_SMM is disabled, so remove the
    four callbacks smi_allowed, enter_smm, leave_smm and enable_smi_window.
    The users in svm/nested.c and x86.c also have to be compiled out; the
    amount of #ifdef'ed code is small and it's not worth moving it to
    smm.c.
    
    enter_smm is now used only within #ifdef CONFIG_KVM_SMM, and the stub
    can therefore be removed.
    
    Signed-off-by: Paolo Bonzini <pbonzini@redhat.com>
    Reviewed-by: Maxim Levitsky <mlevitsk@redhat.com>
    Message-Id: <20220929172016.319443-7-pbonzini@redhat.com>
    Signed-off-by: Paolo Bonzini <pbonzini@redhat.com>

diff --git a/arch/x86/kvm/vmx/vmx.c b/arch/x86/kvm/vmx/vmx.c
index 6a0b65815206..6be991b29bb7 100644
--- a/arch/x86/kvm/vmx/vmx.c
+++ b/arch/x86/kvm/vmx/vmx.c
@@ -7932,6 +7932,7 @@ static void vmx_setup_mce(struct kvm_vcpu *vcpu)
 			~FEAT_CTL_LMCE_ENABLED;
 }
 
+#ifdef CONFIG_KVM_SMM
 static int vmx_smi_allowed(struct kvm_vcpu *vcpu, bool for_injection)
 {
 	/* we need a nested vmexit to enter SMM, postpone if run is pending */
@@ -7986,6 +7987,7 @@ static void vmx_enable_smi_window(struct kvm_vcpu *vcpu)
 {
 	/* RSM will cause a vmexit anyway.  */
 }
+#endif
 
 static bool vmx_apic_init_signal_blocked(struct kvm_vcpu *vcpu)
 {
@@ -8153,10 +8155,12 @@ static struct kvm_x86_ops vmx_x86_ops __initdata = {
 
 	.setup_mce = vmx_setup_mce,
 
+#ifdef CONFIG_KVM_SMM
 	.smi_allowed = vmx_smi_allowed,
 	.enter_smm = vmx_enter_smm,
 	.leave_smm = vmx_leave_smm,
 	.enable_smi_window = vmx_enable_smi_window,
+#endif
 
 	.can_emulate_instruction = vmx_can_emulate_instruction,
 	.apic_init_signal_blocked = vmx_apic_init_signal_blocked,