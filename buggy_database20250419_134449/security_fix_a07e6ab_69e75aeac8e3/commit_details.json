{
  "hash": "a07e6ab41be179cf1ed728a4f41368435508b550",
  "hash_short": "a07e6ab4",
  "subject": "md: the md RAID10 resync thread could cause a md RAID10 array deadlock",
  "body": "This message describes another issue about md RAID10 found by testing the\n2.6.24 md RAID10 using new scsi fault injection framework.\n\nAbstract:\n\nWhen a scsi error results in disabling a disk during RAID10 recovery, the\nresync threads of md RAID10 could stall.\n\nThis case, the raid array has already been broken and it may not matter.  But\nI think stall is not preferable.  If it occurs, even shutdown or reboot will\nfail because of resource busy.\n\nThe deadlock mechanism:\n\nThe r10bio_s structure has a \"remaining\" member to keep track of BIOs yet to\nbe handled when recovering.  The \"remaining\" counter is incremented when\nbuilding a BIO in sync_request() and is decremented when finish a BIO in\nend_sync_write().\n\nIf building a BIO fails for some reasons in sync_request(), the \"remaining\"\nshould be decremented if it has already been incremented.  I found a case\nwhere this decrement is forgotten.  This causes a md_do_sync() deadlock\nbecause md_do_sync() waits for md_done_sync() called by end_sync_write(), but\nend_sync_write() never calls md_done_sync() because of the \"remaining\" counter\nmismatch.\n\nFor example, this problem would be reproduced in the following case:\n\nPersonalities : [raid10]\nmd0 : active raid10 sdf1[4] sde1[5](F) sdd1[2] sdc1[1] sdb1[6](F)\n      3919616 blocks 64K chunks 2 near-copies [4/2] [_UU_]\n      [>....................]  recovery =  2.2% (45376/1959808) finish=0.7min speed=45376K/sec\n\nThis case, sdf1 is recovering, sdb1 and sde1 are disabled.\nAn additional error with detaching sdd will cause a deadlock.\n\nmd0 : active raid10 sdf1[4] sde1[5](F) sdd1[6](F) sdc1[1] sdb1[7](F)\n      3919616 blocks 64K chunks 2 near-copies [4/1] [_U__]\n      [=>...................]  recovery =  5.0% (99520/1959808) finish=5.9min speed=5237K/sec\n\n 2739 ?        S<     0:17 [md0_raid10]\n28608 ?        D<     0:00 [md0_resync]\n28629 pts/1    Ss     0:00 bash\n28830 pts/1    R+     0:00 ps ax\n31819 ?        D<     0:00 [kjournald]\n\nThe resync thread keeps working, but actually it is deadlocked.\n\nPatch:\nBy this patch, the remaining counter will be decremented if needed.\n\nSigned-off-by: Neil Brown <neilb@suse.de>\nSigned-off-by: Andrew Morton <akpm@linux-foundation.org>\nSigned-off-by: Linus Torvalds <torvalds@linux-foundation.org>",
  "full_message": "md: the md RAID10 resync thread could cause a md RAID10 array deadlock\n\nThis message describes another issue about md RAID10 found by testing the\n2.6.24 md RAID10 using new scsi fault injection framework.\n\nAbstract:\n\nWhen a scsi error results in disabling a disk during RAID10 recovery, the\nresync threads of md RAID10 could stall.\n\nThis case, the raid array has already been broken and it may not matter.  But\nI think stall is not preferable.  If it occurs, even shutdown or reboot will\nfail because of resource busy.\n\nThe deadlock mechanism:\n\nThe r10bio_s structure has a \"remaining\" member to keep track of BIOs yet to\nbe handled when recovering.  The \"remaining\" counter is incremented when\nbuilding a BIO in sync_request() and is decremented when finish a BIO in\nend_sync_write().\n\nIf building a BIO fails for some reasons in sync_request(), the \"remaining\"\nshould be decremented if it has already been incremented.  I found a case\nwhere this decrement is forgotten.  This causes a md_do_sync() deadlock\nbecause md_do_sync() waits for md_done_sync() called by end_sync_write(), but\nend_sync_write() never calls md_done_sync() because of the \"remaining\" counter\nmismatch.\n\nFor example, this problem would be reproduced in the following case:\n\nPersonalities : [raid10]\nmd0 : active raid10 sdf1[4] sde1[5](F) sdd1[2] sdc1[1] sdb1[6](F)\n      3919616 blocks 64K chunks 2 near-copies [4/2] [_UU_]\n      [>....................]  recovery =  2.2% (45376/1959808) finish=0.7min speed=45376K/sec\n\nThis case, sdf1 is recovering, sdb1 and sde1 are disabled.\nAn additional error with detaching sdd will cause a deadlock.\n\nmd0 : active raid10 sdf1[4] sde1[5](F) sdd1[6](F) sdc1[1] sdb1[7](F)\n      3919616 blocks 64K chunks 2 near-copies [4/1] [_U__]\n      [=>...................]  recovery =  5.0% (99520/1959808) finish=5.9min speed=5237K/sec\n\n 2739 ?        S<     0:17 [md0_raid10]\n28608 ?        D<     0:00 [md0_resync]\n28629 pts/1    Ss     0:00 bash\n28830 pts/1    R+     0:00 ps ax\n31819 ?        D<     0:00 [kjournald]\n\nThe resync thread keeps working, but actually it is deadlocked.\n\nPatch:\nBy this patch, the remaining counter will be decremented if needed.\n\nSigned-off-by: Neil Brown <neilb@suse.de>\nSigned-off-by: Andrew Morton <akpm@linux-foundation.org>\nSigned-off-by: Linus Torvalds <torvalds@linux-foundation.org>",
  "author_name": "K.Tanaka",
  "author_email": "k-tanaka@ce.jp.nec.com",
  "author_date": "Tue Mar 4 14:29:37 2008 -0800",
  "author_date_iso": "2008-03-04T14:29:37-08:00",
  "committer_name": "Linus Torvalds",
  "committer_email": "torvalds@woody.linux-foundation.org",
  "committer_date": "Tue Mar 4 16:35:18 2008 -0800",
  "committer_date_iso": "2008-03-04T16:35:18-08:00",
  "files_changed": [
    "drivers/md/raid10.c"
  ],
  "files_changed_count": 1,
  "stats": [
    {
      "file": "drivers/md/raid10.c",
      "insertions": 2,
      "deletions": 0
    }
  ],
  "total_insertions": 2,
  "total_deletions": 0,
  "total_changes": 2,
  "parents": [
    "1c830532f6b44d10a1743ccd00e990c6b83396f5"
  ],
  "branches": [
    "* development",
    "master",
    "remotes/origin/HEAD -> origin/master",
    "remotes/origin/master"
  ],
  "tags": [
    "v2.6.25",
    "v2.6.25-rc4",
    "v2.6.25-rc5",
    "v2.6.25-rc6",
    "v2.6.25-rc7",
    "v2.6.25-rc8",
    "v2.6.25-rc9",
    "v2.6.26",
    "v2.6.26-rc1",
    "v2.6.26-rc2"
  ],
  "is_merge": false,
  "security_info": {
    "cve_ids": [],
    "security_keywords": [
      "injection"
    ]
  },
  "fix_type": "security",
  "file_results": [
    {
      "file": "drivers/md/raid10.c",
      "pre_version": true,
      "post_version": true,
      "patch": true
    }
  ]
}