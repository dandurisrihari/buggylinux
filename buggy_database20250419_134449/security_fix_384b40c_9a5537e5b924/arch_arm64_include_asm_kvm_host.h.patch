commit 384b40caa8afae44a54e8f69bd37097c0279fdce
Author: Mark Rutland <mark.rutland@arm.com>
Date:   Tue Apr 23 10:12:35 2019 +0530

    KVM: arm/arm64: Context-switch ptrauth registers
    
    When pointer authentication is supported, a guest may wish to use it.
    This patch adds the necessary KVM infrastructure for this to work, with
    a semi-lazy context switch of the pointer auth state.
    
    Pointer authentication feature is only enabled when VHE is built
    in the kernel and present in the CPU implementation so only VHE code
    paths are modified.
    
    When we schedule a vcpu, we disable guest usage of pointer
    authentication instructions and accesses to the keys. While these are
    disabled, we avoid context-switching the keys. When we trap the guest
    trying to use pointer authentication functionality, we change to eagerly
    context-switching the keys, and enable the feature. The next time the
    vcpu is scheduled out/in, we start again. However the host key save is
    optimized and implemented inside ptrauth instruction/register access
    trap.
    
    Pointer authentication consists of address authentication and generic
    authentication, and CPUs in a system might have varied support for
    either. Where support for either feature is not uniform, it is hidden
    from guests via ID register emulation, as a result of the cpufeature
    framework in the host.
    
    Unfortunately, address authentication and generic authentication cannot
    be trapped separately, as the architecture provides a single EL2 trap
    covering both. If we wish to expose one without the other, we cannot
    prevent a (badly-written) guest from intermittently using a feature
    which is not uniformly supported (when scheduled on a physical CPU which
    supports the relevant feature). Hence, this patch expects both type of
    authentication to be present in a cpu.
    
    This switch of key is done from guest enter/exit assembly as preparation
    for the upcoming in-kernel pointer authentication support. Hence, these
    key switching routines are not implemented in C code as they may cause
    pointer authentication key signing error in some situations.
    
    Signed-off-by: Mark Rutland <mark.rutland@arm.com>
    [Only VHE, key switch in full assembly, vcpu_has_ptrauth checks
    , save host key in ptrauth exception trap]
    Signed-off-by: Amit Daniel Kachhap <amit.kachhap@arm.com>
    Reviewed-by: Julien Thierry <julien.thierry@arm.com>
    Cc: Christoffer Dall <christoffer.dall@arm.com>
    Cc: kvmarm@lists.cs.columbia.edu
    [maz: various fixups]
    Signed-off-by: Marc Zyngier <marc.zyngier@arm.com>

diff --git a/arch/arm64/include/asm/kvm_host.h b/arch/arm64/include/asm/kvm_host.h
index 7ccac42a91a6..7eebea7059c6 100644
--- a/arch/arm64/include/asm/kvm_host.h
+++ b/arch/arm64/include/asm/kvm_host.h
@@ -161,6 +161,18 @@ enum vcpu_sysreg {
 	PMSWINC_EL0,	/* Software Increment Register */
 	PMUSERENR_EL0,	/* User Enable Register */
 
+	/* Pointer Authentication Registers in a strict increasing order. */
+	APIAKEYLO_EL1,
+	APIAKEYHI_EL1,
+	APIBKEYLO_EL1,
+	APIBKEYHI_EL1,
+	APDAKEYLO_EL1,
+	APDAKEYHI_EL1,
+	APDBKEYLO_EL1,
+	APDBKEYHI_EL1,
+	APGAKEYLO_EL1,
+	APGAKEYHI_EL1,
+
 	/* 32bit specific registers. Keep them at the end of the range */
 	DACR32_EL2,	/* Domain Access Control Register */
 	IFSR32_EL2,	/* Instruction Fault Status Register */
@@ -530,6 +542,8 @@ static inline bool kvm_arch_requires_vhe(void)
 	return false;
 }
 
+void kvm_arm_vcpu_ptrauth_trap(struct kvm_vcpu *vcpu);
+
 static inline void kvm_arch_hardware_unsetup(void) {}
 static inline void kvm_arch_sync_events(struct kvm *kvm) {}
 static inline void kvm_arch_sched_in(struct kvm_vcpu *vcpu, int cpu) {}