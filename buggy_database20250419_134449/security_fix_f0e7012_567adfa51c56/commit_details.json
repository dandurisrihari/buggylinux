{
  "hash": "f0e7012c4b938606c7ca230154f181f8eed683eb",
  "hash_short": "f0e7012c",
  "subject": "KVM: x86: Bypass register cache when querying CPL from kvm_sched_out()",
  "body": "When querying guest CPL to determine if a vCPU was preempted while in\nkernel mode, bypass the register cache, i.e. always read SS.AR_BYTES from\nthe VMCS on Intel CPUs.  If the kernel is running with full preemption\nenabled, using the register cache in the preemption path can result in\nstale and/or uninitialized data being cached in the segment cache.\n\nIn particular the following scenario is currently possible:\n\n - vCPU is just created, and the vCPU thread is preempted before\n   SS.AR_BYTES is written in vmx_vcpu_reset().\n\n - When scheduling out the vCPU task, kvm_arch_vcpu_in_kernel() =>\n   vmx_get_cpl() reads and caches '0' for SS.AR_BYTES.\n\n - vmx_vcpu_reset() => seg_setup() configures SS.AR_BYTES, but doesn't\n   invoke vmx_segment_cache_clear() to invalidate the cache.\n\nAs a result, KVM retains a stale value in the cache, which can be read,\ne.g. via KVM_GET_SREGS.  Usually this is not a problem because the VMX\nsegment cache is reset on each VM-Exit, but if the userspace VMM (e.g KVM\nselftests) reads and writes system registers just after the vCPU was\ncreated, _without_ modifying SS.AR_BYTES, userspace will write back the\nstale '0' value and ultimately will trigger a VM-Entry failure due to\nincorrect SS segment type.\n\nNote, the VM-Enter failure can also be avoided by moving the call to\nvmx_segment_cache_clear() until after the vmx_vcpu_reset() initializes all\nsegments.  However, while that change is correct and desirable (and will\ncome along shortly), it does not address the underlying problem that\naccessing KVM's register caches from !task context is generally unsafe.\n\nIn addition to fixing the immediate bug, bypassing the cache for this\nparticular case will allow hardening KVM register caching log to assert\nthat the caches are accessed only when KVM _knows_ it is safe to do so.\n\nFixes: de63ad4cf497 (\"KVM: X86: implement the logic for spinlock optimization\")\nReported-by: Maxim Levitsky <mlevitsk@redhat.com>\nCloses: https://lore.kernel.org/all/20240716022014.240960-3-mlevitsk@redhat.com\nReviewed-by: Maxim Levitsky <mlevitsk@redhat.com>\nLink: https://lore.kernel.org/r/20241009175002.1118178-2-seanjc@google.com\nSigned-off-by: Sean Christopherson <seanjc@google.com>",
  "full_message": "KVM: x86: Bypass register cache when querying CPL from kvm_sched_out()\n\nWhen querying guest CPL to determine if a vCPU was preempted while in\nkernel mode, bypass the register cache, i.e. always read SS.AR_BYTES from\nthe VMCS on Intel CPUs.  If the kernel is running with full preemption\nenabled, using the register cache in the preemption path can result in\nstale and/or uninitialized data being cached in the segment cache.\n\nIn particular the following scenario is currently possible:\n\n - vCPU is just created, and the vCPU thread is preempted before\n   SS.AR_BYTES is written in vmx_vcpu_reset().\n\n - When scheduling out the vCPU task, kvm_arch_vcpu_in_kernel() =>\n   vmx_get_cpl() reads and caches '0' for SS.AR_BYTES.\n\n - vmx_vcpu_reset() => seg_setup() configures SS.AR_BYTES, but doesn't\n   invoke vmx_segment_cache_clear() to invalidate the cache.\n\nAs a result, KVM retains a stale value in the cache, which can be read,\ne.g. via KVM_GET_SREGS.  Usually this is not a problem because the VMX\nsegment cache is reset on each VM-Exit, but if the userspace VMM (e.g KVM\nselftests) reads and writes system registers just after the vCPU was\ncreated, _without_ modifying SS.AR_BYTES, userspace will write back the\nstale '0' value and ultimately will trigger a VM-Entry failure due to\nincorrect SS segment type.\n\nNote, the VM-Enter failure can also be avoided by moving the call to\nvmx_segment_cache_clear() until after the vmx_vcpu_reset() initializes all\nsegments.  However, while that change is correct and desirable (and will\ncome along shortly), it does not address the underlying problem that\naccessing KVM's register caches from !task context is generally unsafe.\n\nIn addition to fixing the immediate bug, bypassing the cache for this\nparticular case will allow hardening KVM register caching log to assert\nthat the caches are accessed only when KVM _knows_ it is safe to do so.\n\nFixes: de63ad4cf497 (\"KVM: X86: implement the logic for spinlock optimization\")\nReported-by: Maxim Levitsky <mlevitsk@redhat.com>\nCloses: https://lore.kernel.org/all/20240716022014.240960-3-mlevitsk@redhat.com\nReviewed-by: Maxim Levitsky <mlevitsk@redhat.com>\nLink: https://lore.kernel.org/r/20241009175002.1118178-2-seanjc@google.com\nSigned-off-by: Sean Christopherson <seanjc@google.com>",
  "author_name": "Sean Christopherson",
  "author_email": "seanjc@google.com",
  "author_date": "Wed Oct 9 10:49:59 2024 -0700",
  "author_date_iso": "2024-10-09T10:49:59-07:00",
  "committer_name": "Sean Christopherson",
  "committer_email": "seanjc@google.com",
  "committer_date": "Fri Nov 1 09:22:21 2024 -0700",
  "committer_date_iso": "2024-11-01T09:22:21-07:00",
  "files_changed": [
    "arch/x86/include/asm/kvm-x86-ops.h",
    "arch/x86/include/asm/kvm_host.h",
    "arch/x86/kvm/svm/svm.c",
    "arch/x86/kvm/vmx/main.c",
    "arch/x86/kvm/vmx/vmx.c",
    "arch/x86/kvm/vmx/vmx.h",
    "arch/x86/kvm/x86.c"
  ],
  "files_changed_count": 7,
  "stats": [
    {
      "file": "arch/x86/include/asm/kvm-x86-ops.h",
      "insertions": 1,
      "deletions": 0
    },
    {
      "file": "arch/x86/include/asm/kvm_host.h",
      "insertions": 1,
      "deletions": 0
    },
    {
      "file": "arch/x86/kvm/svm/svm.c",
      "insertions": 1,
      "deletions": 0
    },
    {
      "file": "arch/x86/kvm/vmx/main.c",
      "insertions": 1,
      "deletions": 0
    },
    {
      "file": "arch/x86/kvm/vmx/vmx.c",
      "insertions": 18,
      "deletions": 5
    },
    {
      "file": "arch/x86/kvm/vmx/vmx.h",
      "insertions": 1,
      "deletions": 0
    },
    {
      "file": "arch/x86/kvm/x86.c",
      "insertions": 7,
      "deletions": 1
    }
  ],
  "total_insertions": 30,
  "total_deletions": 6,
  "total_changes": 36,
  "parents": [
    "de572491a97567c6aeb25ab620d2f9e6635bd50e"
  ],
  "branches": [
    "* development",
    "remotes/origin/HEAD -> origin/master",
    "remotes/origin/master"
  ],
  "tags": [],
  "is_merge": false,
  "security_info": {
    "cve_ids": [],
    "security_keywords": [
      "hardening",
      "Bypass"
    ]
  },
  "fix_type": "security",
  "file_results": [
    {
      "file": "arch/x86/include/asm/kvm-x86-ops.h",
      "pre_version": false,
      "post_version": true,
      "patch": true
    },
    {
      "file": "arch/x86/include/asm/kvm_host.h",
      "pre_version": false,
      "post_version": true,
      "patch": true
    },
    {
      "file": "arch/x86/kvm/svm/svm.c",
      "pre_version": false,
      "post_version": true,
      "patch": true
    },
    {
      "file": "arch/x86/kvm/vmx/vmx.h",
      "pre_version": false,
      "post_version": true,
      "patch": true
    },
    {
      "file": "arch/x86/kvm/vmx/vmx.c",
      "pre_version": false,
      "post_version": true,
      "patch": true
    },
    {
      "file": "arch/x86/kvm/vmx/main.c",
      "pre_version": false,
      "post_version": true,
      "patch": true
    },
    {
      "file": "arch/x86/kvm/x86.c",
      "pre_version": false,
      "post_version": true,
      "patch": true
    }
  ]
}