commit 0c92385dc05ee9637c04372ea95a11bbf6e010ff
Author: Peter Zijlstra <peterz@infradead.org>
Date:   Mon Feb 24 13:37:12 2025 +0100

    x86/ibt: Implement FineIBT-BHI mitigation
    
    While WAIT_FOR_ENDBR is specified to be a full speculation stop; it
    has been shown that some implementations are 'leaky' to such an extend
    that speculation can escape even the FineIBT preamble.
    
    To deal with this, add additional hardening to the FineIBT preamble.
    
    Notably, using a new LLVM feature:
    
      https://github.com/llvm/llvm-project/commit/e223485c9b38a5579991b8cebb6a200153eee245
    
    which encodes the number of arguments in the kCFI preamble's register.
    
    Using this register<->arity mapping, have the FineIBT preamble CALL
    into a stub clobbering the relevant argument registers in the
    speculative case.
    
    Scott sayeth thusly:
    
    Microarchitectural attacks such as Branch History Injection (BHI) and
    Intra-mode Branch Target Injection (IMBTI) [1] can cause an indirect
    call to mispredict to an adversary-influenced target within the same
    hardware domain (e.g., within the kernel). Instructions at the
    mispredicted target may execute speculatively and potentially expose
    kernel data (e.g., to a user-mode adversary) through a
    microarchitectural covert channel such as CPU cache state.
    
    CET-IBT [2] is a coarse-grained control-flow integrity (CFI) ISA
    extension that enforces that each indirect call (or indirect jump)
    must land on an ENDBR (end branch) instruction, even speculatively*.
    FineIBT is a software technique that refines CET-IBT by associating
    each function type with a 32-bit hash and enforcing (at the callee)
    that the hash of the caller's function pointer type matches the hash
    of the callee's function type. However, recent research [3] has
    demonstrated that the conditional branch that enforces FineIBT's hash
    check can be coerced to mispredict, potentially allowing an adversary
    to speculatively bypass the hash check:
    
    __cfi_foo:
      ENDBR64
      SUB R10d, 0x01234567
      JZ foo    # Even if the hash check fails and ZF=0, this branch could still mispredict as taken
      UD2
    foo:
      ...
    
    The techniques demonstrated in [3] require the attacker to be able to
    control the contents of at least one live register at the mispredicted
    target. Therefore, this patch set introduces a sequence of CMOV
    instructions at each indirect-callable target that poisons every live
    register with data that the attacker cannot control whenever the
    FineIBT hash check fails, thus mitigating any potential attack.
    
    The security provided by this scheme has been discussed in detail on
    an earlier thread [4].
    
     [1] https://www.intel.com/content/www/us/en/developer/articles/technical/software-security-guidance/technical-documentation/branch-history-injection.html
     [2] Intel Software Developer's Manual, Volume 1, Chapter 18
     [3] https://www.vusec.net/projects/native-bhi/
     [4] https://lore.kernel.org/lkml/20240927194925.707462984@infradead.org/
     *There are some caveats for certain processors, see [1] for more info
    
    Suggested-by: Scott Constable <scott.d.constable@intel.com>
    Signed-off-by: Peter Zijlstra (Intel) <peterz@infradead.org>
    Signed-off-by: Ingo Molnar <mingo@kernel.org>
    Reviewed-by: Kees Cook <kees@kernel.org>
    Link: https://lore.kernel.org/r/20250224124200.820402212@infradead.org

diff --git a/arch/x86/net/bpf_jit_comp.c b/arch/x86/net/bpf_jit_comp.c
index ce033e63bc27..72776dcb75aa 100644
--- a/arch/x86/net/bpf_jit_comp.c
+++ b/arch/x86/net/bpf_jit_comp.c
@@ -410,15 +410,20 @@ static void emit_nops(u8 **pprog, int len)
  * Emit the various CFI preambles, see asm/cfi.h and the comments about FineIBT
  * in arch/x86/kernel/alternative.c
  */
+static int emit_call(u8 **prog, void *func, void *ip);
 
-static void emit_fineibt(u8 **pprog, u32 hash)
+static void emit_fineibt(u8 **pprog, u8 *ip, u32 hash, int arity)
 {
 	u8 *prog = *pprog;
 
 	EMIT_ENDBR();
 	EMIT3_off32(0x41, 0x81, 0xea, hash);		/* subl $hash, %r10d	*/
-	EMIT2(0x75, 0xf9);				/* jne.d8 .-7		*/
-	EMIT3(0x0f, 0x1f, 0x00);			/* nop3			*/
+	if (cfi_bhi) {
+		emit_call(&prog, __bhi_args[arity], ip + 11);
+	} else {
+		EMIT2(0x75, 0xf9);			/* jne.d8 .-7		*/
+		EMIT3(0x0f, 0x1f, 0x00);		/* nop3			*/
+	}
 	EMIT_ENDBR_POISON();
 
 	*pprog = prog;
@@ -447,13 +452,13 @@ static void emit_kcfi(u8 **pprog, u32 hash)
 	*pprog = prog;
 }
 
-static void emit_cfi(u8 **pprog, u32 hash)
+static void emit_cfi(u8 **pprog, u8 *ip, u32 hash, int arity)
 {
 	u8 *prog = *pprog;
 
 	switch (cfi_mode) {
 	case CFI_FINEIBT:
-		emit_fineibt(&prog, hash);
+		emit_fineibt(&prog, ip, hash, arity);
 		break;
 
 	case CFI_KCFI:
@@ -504,13 +509,17 @@ static void emit_prologue_tail_call(u8 **pprog, bool is_subprog)
  * bpf_tail_call helper will skip the first X86_TAIL_CALL_OFFSET bytes
  * while jumping to another program
  */
-static void emit_prologue(u8 **pprog, u32 stack_depth, bool ebpf_from_cbpf,
+static void emit_prologue(u8 **pprog, u8 *ip, u32 stack_depth, bool ebpf_from_cbpf,
 			  bool tail_call_reachable, bool is_subprog,
 			  bool is_exception_cb)
 {
 	u8 *prog = *pprog;
 
-	emit_cfi(&prog, is_subprog ? cfi_bpf_subprog_hash : cfi_bpf_hash);
+	if (is_subprog) {
+		emit_cfi(&prog, ip, cfi_bpf_subprog_hash, 5);
+	} else {
+		emit_cfi(&prog, ip, cfi_bpf_hash, 1);
+	}
 	/* BPF trampoline can be made to work without these nops,
 	 * but let's waste 5 bytes for now and optimize later
 	 */
@@ -1479,7 +1488,7 @@ static int do_jit(struct bpf_prog *bpf_prog, int *addrs, u8 *image, u8 *rw_image
 
 	detect_reg_usage(insn, insn_cnt, callee_regs_used);
 
-	emit_prologue(&prog, stack_depth,
+	emit_prologue(&prog, image, stack_depth,
 		      bpf_prog_was_classic(bpf_prog), tail_call_reachable,
 		      bpf_is_subprog(bpf_prog), bpf_prog->aux->exception_cb);
 	/* Exception callback will clobber callee regs for its own use, and
@@ -3046,7 +3055,9 @@ static int __arch_prepare_bpf_trampoline(struct bpf_tramp_image *im, void *rw_im
 		/*
 		 * Indirect call for bpf_struct_ops
 		 */
-		emit_cfi(&prog, cfi_get_func_hash(func_addr));
+		emit_cfi(&prog, image,
+			 cfi_get_func_hash(func_addr),
+			 cfi_get_func_arity(func_addr));
 	} else {
 		/*
 		 * Direct-call fentry stub, as such it needs accounting for the