commit 67f2df3b82d091ed095d0e47e1f3a9d3e18e4e41
Author: Kees Cook <kees@kernel.org>
Date:   Mon Jul 1 12:12:59 2024 -0700

    mm/slab: Plumb kmem_buckets into __do_kmalloc_node()
    
    Introduce CONFIG_SLAB_BUCKETS which provides the infrastructure to
    support separated kmalloc buckets (in the following kmem_buckets_create()
    patches and future codetag-based separation). Since this will provide
    a mitigation for a very common case of exploits, it is recommended to
    enable this feature for general purpose distros. By default, the new
    Kconfig will be enabled if CONFIG_SLAB_FREELIST_HARDENED is enabled (and
    it is added to the hardening.config Kconfig fragment).
    
    To be able to choose which buckets to allocate from, make the buckets
    available to the internal kmalloc interfaces by adding them as the
    second argument, rather than depending on the buckets being chosen from
    the fixed set of global buckets. Where the bucket is not available,
    pass NULL, which means "use the default system kmalloc bucket set"
    (the prior existing behavior), as implemented in kmalloc_slab().
    
    To avoid adding the extra argument when !CONFIG_SLAB_BUCKETS, only the
    top-level macros and static inlines use the buckets argument (where
    they are stripped out and compiled out respectively). The actual extern
    functions can then be built without the argument, and the internals
    fall back to the global kmalloc buckets unconditionally.
    
    Co-developed-by: Vlastimil Babka <vbabka@suse.cz>
    Signed-off-by: Kees Cook <kees@kernel.org>
    Signed-off-by: Vlastimil Babka <vbabka@suse.cz>

diff --git a/mm/slub.c b/mm/slub.c
index 3d19a0ee411f..80f0a51242d1 100644
--- a/mm/slub.c
+++ b/mm/slub.c
@@ -4117,7 +4117,7 @@ void *__kmalloc_large_node_noprof(size_t size, gfp_t flags, int node)
 EXPORT_SYMBOL(__kmalloc_large_node_noprof);
 
 static __always_inline
-void *__do_kmalloc_node(size_t size, gfp_t flags, int node,
+void *__do_kmalloc_node(size_t size, kmem_buckets *b, gfp_t flags, int node,
 			unsigned long caller)
 {
 	struct kmem_cache *s;
@@ -4133,32 +4133,32 @@ void *__do_kmalloc_node(size_t size, gfp_t flags, int node,
 	if (unlikely(!size))
 		return ZERO_SIZE_PTR;
 
-	s = kmalloc_slab(size, flags, caller);
+	s = kmalloc_slab(size, b, flags, caller);
 
 	ret = slab_alloc_node(s, NULL, flags, node, caller, size);
 	ret = kasan_kmalloc(s, ret, size, flags);
 	trace_kmalloc(caller, ret, size, s->size, flags, node);
 	return ret;
 }
-
-void *__kmalloc_node_noprof(size_t size, gfp_t flags, int node)
+void *__kmalloc_node_noprof(DECL_BUCKET_PARAMS(size, b), gfp_t flags, int node)
 {
-	return __do_kmalloc_node(size, flags, node, _RET_IP_);
+	return __do_kmalloc_node(size, PASS_BUCKET_PARAM(b), flags, node, _RET_IP_);
 }
 EXPORT_SYMBOL(__kmalloc_node_noprof);
 
 void *__kmalloc_noprof(size_t size, gfp_t flags)
 {
-	return __do_kmalloc_node(size, flags, NUMA_NO_NODE, _RET_IP_);
+	return __do_kmalloc_node(size, NULL, flags, NUMA_NO_NODE, _RET_IP_);
 }
 EXPORT_SYMBOL(__kmalloc_noprof);
 
-void *kmalloc_node_track_caller_noprof(size_t size, gfp_t flags,
-				       int node, unsigned long caller)
+void *__kmalloc_node_track_caller_noprof(DECL_BUCKET_PARAMS(size, b), gfp_t flags,
+					 int node, unsigned long caller)
 {
-	return __do_kmalloc_node(size, flags, node, caller);
+	return __do_kmalloc_node(size, PASS_BUCKET_PARAM(b), flags, node, caller);
+
 }
-EXPORT_SYMBOL(kmalloc_node_track_caller_noprof);
+EXPORT_SYMBOL(__kmalloc_node_track_caller_noprof);
 
 void *__kmalloc_cache_noprof(struct kmem_cache *s, gfp_t gfpflags, size_t size)
 {