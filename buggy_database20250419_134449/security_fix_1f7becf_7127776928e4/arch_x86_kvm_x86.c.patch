commit 1f7becf1b7e21794fc9d460765fe09679bc9b9e0
Author: Jay Zhou <jianjay.zhou@huawei.com>
Date:   Mon Jan 18 16:47:20 2021 +0800

    KVM: x86: get smi pending status correctly
    
    The injection process of smi has two steps:
    
        Qemu                        KVM
    Step1:
        cpu->interrupt_request &= \
            ~CPU_INTERRUPT_SMI;
        kvm_vcpu_ioctl(cpu, KVM_SMI)
    
                                    call kvm_vcpu_ioctl_smi() and
                                    kvm_make_request(KVM_REQ_SMI, vcpu);
    
    Step2:
        kvm_vcpu_ioctl(cpu, KVM_RUN, 0)
    
                                    call process_smi() if
                                    kvm_check_request(KVM_REQ_SMI, vcpu) is
                                    true, mark vcpu->arch.smi_pending = true;
    
    The vcpu->arch.smi_pending will be set true in step2, unfortunately if
    vcpu paused between step1 and step2, the kvm_run->immediate_exit will be
    set and vcpu has to exit to Qemu immediately during step2 before mark
    vcpu->arch.smi_pending true.
    During VM migration, Qemu will get the smi pending status from KVM using
    KVM_GET_VCPU_EVENTS ioctl at the downtime, then the smi pending status
    will be lost.
    
    Signed-off-by: Jay Zhou <jianjay.zhou@huawei.com>
    Signed-off-by: Shengen Zhuang <zhuangshengen@huawei.com>
    Message-Id: <20210118084720.1585-1-jianjay.zhou@huawei.com>
    Cc: stable@vger.kernel.org
    Signed-off-by: Paolo Bonzini <pbonzini@redhat.com>

diff --git a/arch/x86/kvm/x86.c b/arch/x86/kvm/x86.c
index 9a8969a6dd06..9025c7673af6 100644
--- a/arch/x86/kvm/x86.c
+++ b/arch/x86/kvm/x86.c
@@ -105,6 +105,7 @@ static u64 __read_mostly cr4_reserved_bits = CR4_RESERVED_BITS;
 
 static void update_cr8_intercept(struct kvm_vcpu *vcpu);
 static void process_nmi(struct kvm_vcpu *vcpu);
+static void process_smi(struct kvm_vcpu *vcpu);
 static void enter_smm(struct kvm_vcpu *vcpu);
 static void __kvm_set_rflags(struct kvm_vcpu *vcpu, unsigned long rflags);
 static void store_regs(struct kvm_vcpu *vcpu);
@@ -4230,6 +4231,9 @@ static void kvm_vcpu_ioctl_x86_get_vcpu_events(struct kvm_vcpu *vcpu,
 {
 	process_nmi(vcpu);
 
+	if (kvm_check_request(KVM_REQ_SMI, vcpu))
+		process_smi(vcpu);
+
 	/*
 	 * In guest mode, payload delivery should be deferred,
 	 * so that the L1 hypervisor can intercept #PF before