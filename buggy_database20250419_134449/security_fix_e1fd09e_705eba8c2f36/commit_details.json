{
  "hash": "e1fd09e3d1dd4a1a8b3b33bc1fd647eee9f4e475",
  "hash_short": "e1fd09e3",
  "subject": "mm: x86, arm64: add arch_has_hw_pte_young()",
  "body": "Patch series \"Multi-Gen LRU Framework\", v14.\n\nWhat's new\n==========\n1. OpenWrt, in addition to Android, Arch Linux Zen, Armbian, ChromeOS,\n   Liquorix, post-factum and XanMod, is now shipping MGLRU on 5.15.\n2. Fixed long-tailed direct reclaim latency seen on high-memory (TBs)\n   machines. The old direct reclaim backoff, which tries to enforce a\n   minimum fairness among all eligible memcgs, over-swapped by about\n   (total_mem>>DEF_PRIORITY)-nr_to_reclaim. The new backoff, which\n   pulls the plug on swapping once the target is met, trades some\n   fairness for curtailed latency:\n   https://lore.kernel.org/r/20220918080010.2920238-10-yuzhao@google.com/\n3. Fixed minior build warnings and conflicts. More comments and nits.\n\nTLDR\n====\nThe current page reclaim is too expensive in terms of CPU usage and it\noften makes poor choices about what to evict. This patchset offers an\nalternative solution that is performant, versatile and\nstraightforward.\n\nPatchset overview\n=================\nThe design and implementation overview is in patch 14:\nhttps://lore.kernel.org/r/20220918080010.2920238-15-yuzhao@google.com/\n\n01. mm: x86, arm64: add arch_has_hw_pte_young()\n02. mm: x86: add CONFIG_ARCH_HAS_NONLEAF_PMD_YOUNG\nTake advantage of hardware features when trying to clear the accessed\nbit in many PTEs.\n\n03. mm/vmscan.c: refactor shrink_node()\n04. Revert \"include/linux/mm_inline.h: fold __update_lru_size() into\n    its sole caller\"\nMinor refactors to improve readability for the following patches.\n\n05. mm: multi-gen LRU: groundwork\nAdds the basic data structure and the functions that insert pages to\nand remove pages from the multi-gen LRU (MGLRU) lists.\n\n06. mm: multi-gen LRU: minimal implementation\nA minimal implementation without optimizations.\n\n07. mm: multi-gen LRU: exploit locality in rmap\nExploits spatial locality to improve efficiency when using the rmap.\n\n08. mm: multi-gen LRU: support page table walks\nFurther exploits spatial locality by optionally scanning page tables.\n\n09. mm: multi-gen LRU: optimize multiple memcgs\nOptimizes the overall performance for multiple memcgs running mixed\ntypes of workloads.\n\n10. mm: multi-gen LRU: kill switch\nAdds a kill switch to enable or disable MGLRU at runtime.\n\n11. mm: multi-gen LRU: thrashing prevention\n12. mm: multi-gen LRU: debugfs interface\nProvide userspace with features like thrashing prevention, working set\nestimation and proactive reclaim.\n\n13. mm: multi-gen LRU: admin guide\n14. mm: multi-gen LRU: design doc\nAdd an admin guide and a design doc.\n\nBenchmark results\n=================\nIndependent lab results\n-----------------------\nBased on the popularity of searches [01] and the memory usage in\nGoogle's public cloud, the most popular open-source memory-hungry\napplications, in alphabetical order, are:\n      Apache Cassandra      Memcached\n      Apache Hadoop         MongoDB\n      Apache Spark          PostgreSQL\n      MariaDB (MySQL)       Redis\n\nAn independent lab evaluated MGLRU with the most widely used benchmark\nsuites for the above applications. They posted 960 data points along\nwith kernel metrics and perf profiles collected over more than 500\nhours of total benchmark time. Their final reports show that, with 95%\nconfidence intervals (CIs), the above applications all performed\nsignificantly better for at least part of their benchmark matrices.\n\nOn 5.14:\n1. Apache Spark [02] took 95% CIs [9.28, 11.19]% and [12.20, 14.93]%\n   less wall time to sort three billion random integers, respectively,\n   under the medium- and the high-concurrency conditions, when\n   overcommitting memory. There were no statistically significant\n   changes in wall time for the rest of the benchmark matrix.\n2. MariaDB [03] achieved 95% CIs [5.24, 10.71]% and [20.22, 25.97]%\n   more transactions per minute (TPM), respectively, under the medium-\n   and the high-concurrency conditions, when overcommitting memory.\n   There were no statistically significant changes in TPM for the rest\n   of the benchmark matrix.\n3. Memcached [04] achieved 95% CIs [23.54, 32.25]%, [20.76, 41.61]%\n   and [21.59, 30.02]% more operations per second (OPS), respectively,\n   for sequential access, random access and Gaussian (distribution)\n   access, when THP=always; 95% CIs [13.85, 15.97]% and\n   [23.94, 29.92]% more OPS, respectively, for random access and\n   Gaussian access, when THP=never. There were no statistically\n   significant changes in OPS for the rest of the benchmark matrix.\n4. MongoDB [05] achieved 95% CIs [2.23, 3.44]%, [6.97, 9.73]% and\n   [2.16, 3.55]% more operations per second (OPS), respectively, for\n   exponential (distribution) access, random access and Zipfian\n   (distribution) access, when underutilizing memory; 95% CIs\n   [8.83, 10.03]%, [21.12, 23.14]% and [5.53, 6.46]% more OPS,\n   respectively, for exponential access, random access and Zipfian\n   access, when overcommitting memory.\n\nOn 5.15:\n5. Apache Cassandra [06] achieved 95% CIs [1.06, 4.10]%, [1.94, 5.43]%\n   and [4.11, 7.50]% more operations per second (OPS), respectively,\n   for exponential (distribution) access, random access and Zipfian\n   (distribution) access, when swap was off; 95% CIs [0.50, 2.60]%,\n   [6.51, 8.77]% and [3.29, 6.75]% more OPS, respectively, for\n   exponential access, random access and Zipfian access, when swap was\n   on.\n6. Apache Hadoop [07] took 95% CIs [5.31, 9.69]% and [2.02, 7.86]%\n   less average wall time to finish twelve parallel TeraSort jobs,\n   respectively, under the medium- and the high-concurrency\n   conditions, when swap was on. There were no statistically\n   significant changes in average wall time for the rest of the\n   benchmark matrix.\n7. PostgreSQL [08] achieved 95% CI [1.75, 6.42]% more transactions per\n   minute (TPM) under the high-concurrency condition, when swap was\n   off; 95% CIs [12.82, 18.69]% and [22.70, 46.86]% more TPM,\n   respectively, under the medium- and the high-concurrency\n   conditions, when swap was on. There were no statistically\n   significant changes in TPM for the rest of the benchmark matrix.\n8. Redis [09] achieved 95% CIs [0.58, 5.94]%, [6.55, 14.58]% and\n   [11.47, 19.36]% more total operations per second (OPS),\n   respectively, for sequential access, random access and Gaussian\n   (distribution) access, when THP=always; 95% CIs [1.27, 3.54]%,\n   [10.11, 14.81]% and [8.75, 13.64]% more total OPS, respectively,\n   for sequential access, random access and Gaussian access, when\n   THP=never.\n\nOur lab results\n---------------\nTo supplement the above results, we ran the following benchmark suites\non 5.16-rc7 and found no regressions [10].\n      fs_fio_bench_hdd_mq      pft\n      fs_lmbench               pgsql-hammerdb\n      fs_parallelio            redis\n      fs_postmark              stream\n      hackbench                sysbenchthread\n      kernbench                tpcc_spark\n      memcached                unixbench\n      multichase               vm-scalability\n      mutilate                 will-it-scale\n      nginx\n\n[01] https://trends.google.com\n[02] https://lore.kernel.org/r/20211102002002.92051-1-bot@edi.works/\n[03] https://lore.kernel.org/r/20211009054315.47073-1-bot@edi.works/\n[04] https://lore.kernel.org/r/20211021194103.65648-1-bot@edi.works/\n[05] https://lore.kernel.org/r/20211109021346.50266-1-bot@edi.works/\n[06] https://lore.kernel.org/r/20211202062806.80365-1-bot@edi.works/\n[07] https://lore.kernel.org/r/20211209072416.33606-1-bot@edi.works/\n[08] https://lore.kernel.org/r/20211218071041.24077-1-bot@edi.works/\n[09] https://lore.kernel.org/r/20211122053248.57311-1-bot@edi.works/\n[10] https://lore.kernel.org/r/20220104202247.2903702-1-yuzhao@google.com/\n\nRead-world applications\n=======================\nThird-party testimonials\n------------------------\nKonstantin reported [11]:\n   I have Archlinux with 8G RAM + zswap + swap. While developing, I\n   have lots of apps opened such as multiple LSP-servers for different\n   langs, chats, two browsers, etc... Usually, my system gets quickly\n   to a point of SWAP-storms, where I have to kill LSP-servers,\n   restart browsers to free memory, etc, otherwise the system lags\n   heavily and is barely usable.\n   \n   1.5 day ago I migrated from 5.11.15 kernel to 5.12 + the LRU\n   patchset, and I started up by opening lots of apps to create memory\n   pressure, and worked for a day like this. Till now I had not a\n   single SWAP-storm, and mind you I got 3.4G in SWAP. I was never\n   getting to the point of 3G in SWAP before without a single\n   SWAP-storm.\n\nVaibhav from IBM reported [12]:\n   In a synthetic MongoDB Benchmark, seeing an average of ~19%\n   throughput improvement on POWER10(Radix MMU + 64K Page Size) with\n   MGLRU patches on top of 5.16 kernel for MongoDB + YCSB across\n   three different request distributions, namely, Exponential, Uniform\n   and Zipfan.\n\nShuang from U of Rochester reported [13]:\n   With the MGLRU, fio achieved 95% CIs [38.95, 40.26]%, [4.12, 6.64]%\n   and [9.26, 10.36]% higher throughput, respectively, for random\n   access, Zipfian (distribution) access and Gaussian (distribution)\n   access, when the average number of jobs per CPU is 1; 95% CIs\n   [42.32, 49.15]%, [9.44, 9.89]% and [20.99, 22.86]% higher\n   throughput, respectively, for random access, Zipfian access and\n   Gaussian access, when the average number of jobs per CPU is 2.\n\nDaniel from Michigan Tech reported [14]:\n   With Memcached allocating ~100GB of byte-addressable Optante,\n   performance improvement in terms of throughput (measured as queries\n   per second) was about 10% for a series of workloads.\n\nLarge-scale deployments\n-----------------------\nWe've rolled out MGLRU to tens of millions of ChromeOS users and\nabout a million Android users. Google's fleetwide profiling [15] shows\nan overall 40% decrease in kswapd CPU usage, in addition to\nimprovements in other UX metrics, e.g., an 85% decrease in the number\nof low-memory kills at the 75th percentile and an 18% decrease in\napp launch time at the 50th percentile.\n\nThe downstream kernels that have been using MGLRU include:\n1. Android [16]\n2. Arch Linux Zen [17]\n3. Armbian [18]\n4. ChromeOS [19]\n5. Liquorix [20]\n6. OpenWrt [21]\n7. post-factum [22]\n8. XanMod [23]\n\n[11] https://lore.kernel.org/r/140226722f2032c86301fbd326d91baefe3d7d23.camel@yandex.ru/\n[12] https://lore.kernel.org/r/87czj3mux0.fsf@vajain21.in.ibm.com/\n[13] https://lore.kernel.org/r/20220105024423.26409-1-szhai2@cs.rochester.edu/\n[14] https://lore.kernel.org/r/CA+4-3vksGvKd18FgRinxhqHetBS1hQekJE2gwco8Ja-bJWKtFw@mail.gmail.com/\n[15] https://dl.acm.org/doi/10.1145/2749469.2750392\n[16] https://android.com\n[17] https://archlinux.org\n[18] https://armbian.com\n[19] https://chromium.org\n[20] https://liquorix.net\n[21] https://openwrt.org\n[22] https://codeberg.org/pf-kernel\n[23] https://xanmod.org\n\nSummary\n=======\nThe facts are:\n1. The independent lab results and the real-world applications\n   indicate substantial improvements; there are no known regressions.\n2. Thrashing prevention, working set estimation and proactive reclaim\n   work out of the box; there are no equivalent solutions.\n3. There is a lot of new code; no smaller changes have been\n   demonstrated similar effects.\n\nOur options, accordingly, are:\n1. Given the amount of evidence, the reported improvements will likely\n   materialize for a wide range of workloads.\n2. Gauging the interest from the past discussions, the new features\n   will likely be put to use for both personal computers and data\n   centers.\n3. Based on Google's track record, the new code will likely be well\n   maintained in the long term. It'd be more difficult if not\n   impossible to achieve similar effects with other approaches.\n\n\nThis patch (of 14):\n\nSome architectures automatically set the accessed bit in PTEs, e.g., x86\nand arm64 v8.2.  On architectures that do not have this capability,\nclearing the accessed bit in a PTE usually triggers a page fault following\nthe TLB miss of this PTE (to emulate the accessed bit).\n\nBeing aware of this capability can help make better decisions, e.g.,\nwhether to spread the work out over a period of time to reduce bursty page\nfaults when trying to clear the accessed bit in many PTEs.\n\nNote that theoretically this capability can be unreliable, e.g.,\nhotplugged CPUs might be different from builtin ones.  Therefore it should\nnot be used in architecture-independent code that involves correctness,\ne.g., to determine whether TLB flushes are required (in combination with\nthe accessed bit).\n\nLink: https://lkml.kernel.org/r/20220918080010.2920238-1-yuzhao@google.com\nLink: https://lkml.kernel.org/r/20220918080010.2920238-2-yuzhao@google.com\nSigned-off-by: Yu Zhao <yuzhao@google.com>\nReviewed-by: Barry Song <baohua@kernel.org>\nAcked-by: Brian Geffon <bgeffon@google.com>\nAcked-by: Jan Alexander Steffens (heftig) <heftig@archlinux.org>\nAcked-by: Oleksandr Natalenko <oleksandr@natalenko.name>\nAcked-by: Steven Barrett <steven@liquorix.net>\nAcked-by: Suleiman Souhlal <suleiman@google.com>\nAcked-by: Will Deacon <will@kernel.org>\nTested-by: Daniel Byrne <djbyrne@mtu.edu>\nTested-by: Donald Carr <d@chaos-reins.com>\nTested-by: Holger Hoffst\u00e4tte <holger@applied-asynchrony.com>\nTested-by: Konstantin Kharlamov <Hi-Angel@yandex.ru>\nTested-by: Shuang Zhai <szhai2@cs.rochester.edu>\nTested-by: Sofia Trinh <sofia.trinh@edi.works>\nTested-by: Vaibhav Jain <vaibhav@linux.ibm.com>\nCc: Andi Kleen <ak@linux.intel.com>\nCc: Aneesh Kumar K.V <aneesh.kumar@linux.ibm.com>\nCc: Catalin Marinas <catalin.marinas@arm.com>\nCc: Dave Hansen <dave.hansen@linux.intel.com>\nCc: Hillf Danton <hdanton@sina.com>\nCc: Jens Axboe <axboe@kernel.dk>\nCc: Johannes Weiner <hannes@cmpxchg.org>\nCc: Jonathan Corbet <corbet@lwn.net>\nCc: Linus Torvalds <torvalds@linux-foundation.org>\nCc: linux-arm-kernel@lists.infradead.org\nCc: Matthew Wilcox <willy@infradead.org>\nCc: Mel Gorman <mgorman@suse.de>\nCc: Michael Larabel <Michael@MichaelLarabel.com>\nCc: Michal Hocko <mhocko@kernel.org>\nCc: Mike Rapoport <rppt@kernel.org>\nCc: Peter Zijlstra <peterz@infradead.org>\nCc: Tejun Heo <tj@kernel.org>\nCc: Vlastimil Babka <vbabka@suse.cz>\nCc: Miaohe Lin <linmiaohe@huawei.com>\nCc: Mike Rapoport <rppt@linux.ibm.com>\nCc: Qi Zheng <zhengqi.arch@bytedance.com>\nSigned-off-by: Andrew Morton <akpm@linux-foundation.org>",
  "full_message": "mm: x86, arm64: add arch_has_hw_pte_young()\n\nPatch series \"Multi-Gen LRU Framework\", v14.\n\nWhat's new\n==========\n1. OpenWrt, in addition to Android, Arch Linux Zen, Armbian, ChromeOS,\n   Liquorix, post-factum and XanMod, is now shipping MGLRU on 5.15.\n2. Fixed long-tailed direct reclaim latency seen on high-memory (TBs)\n   machines. The old direct reclaim backoff, which tries to enforce a\n   minimum fairness among all eligible memcgs, over-swapped by about\n   (total_mem>>DEF_PRIORITY)-nr_to_reclaim. The new backoff, which\n   pulls the plug on swapping once the target is met, trades some\n   fairness for curtailed latency:\n   https://lore.kernel.org/r/20220918080010.2920238-10-yuzhao@google.com/\n3. Fixed minior build warnings and conflicts. More comments and nits.\n\nTLDR\n====\nThe current page reclaim is too expensive in terms of CPU usage and it\noften makes poor choices about what to evict. This patchset offers an\nalternative solution that is performant, versatile and\nstraightforward.\n\nPatchset overview\n=================\nThe design and implementation overview is in patch 14:\nhttps://lore.kernel.org/r/20220918080010.2920238-15-yuzhao@google.com/\n\n01. mm: x86, arm64: add arch_has_hw_pte_young()\n02. mm: x86: add CONFIG_ARCH_HAS_NONLEAF_PMD_YOUNG\nTake advantage of hardware features when trying to clear the accessed\nbit in many PTEs.\n\n03. mm/vmscan.c: refactor shrink_node()\n04. Revert \"include/linux/mm_inline.h: fold __update_lru_size() into\n    its sole caller\"\nMinor refactors to improve readability for the following patches.\n\n05. mm: multi-gen LRU: groundwork\nAdds the basic data structure and the functions that insert pages to\nand remove pages from the multi-gen LRU (MGLRU) lists.\n\n06. mm: multi-gen LRU: minimal implementation\nA minimal implementation without optimizations.\n\n07. mm: multi-gen LRU: exploit locality in rmap\nExploits spatial locality to improve efficiency when using the rmap.\n\n08. mm: multi-gen LRU: support page table walks\nFurther exploits spatial locality by optionally scanning page tables.\n\n09. mm: multi-gen LRU: optimize multiple memcgs\nOptimizes the overall performance for multiple memcgs running mixed\ntypes of workloads.\n\n10. mm: multi-gen LRU: kill switch\nAdds a kill switch to enable or disable MGLRU at runtime.\n\n11. mm: multi-gen LRU: thrashing prevention\n12. mm: multi-gen LRU: debugfs interface\nProvide userspace with features like thrashing prevention, working set\nestimation and proactive reclaim.\n\n13. mm: multi-gen LRU: admin guide\n14. mm: multi-gen LRU: design doc\nAdd an admin guide and a design doc.\n\nBenchmark results\n=================\nIndependent lab results\n-----------------------\nBased on the popularity of searches [01] and the memory usage in\nGoogle's public cloud, the most popular open-source memory-hungry\napplications, in alphabetical order, are:\n      Apache Cassandra      Memcached\n      Apache Hadoop         MongoDB\n      Apache Spark          PostgreSQL\n      MariaDB (MySQL)       Redis\n\nAn independent lab evaluated MGLRU with the most widely used benchmark\nsuites for the above applications. They posted 960 data points along\nwith kernel metrics and perf profiles collected over more than 500\nhours of total benchmark time. Their final reports show that, with 95%\nconfidence intervals (CIs), the above applications all performed\nsignificantly better for at least part of their benchmark matrices.\n\nOn 5.14:\n1. Apache Spark [02] took 95% CIs [9.28, 11.19]% and [12.20, 14.93]%\n   less wall time to sort three billion random integers, respectively,\n   under the medium- and the high-concurrency conditions, when\n   overcommitting memory. There were no statistically significant\n   changes in wall time for the rest of the benchmark matrix.\n2. MariaDB [03] achieved 95% CIs [5.24, 10.71]% and [20.22, 25.97]%\n   more transactions per minute (TPM), respectively, under the medium-\n   and the high-concurrency conditions, when overcommitting memory.\n   There were no statistically significant changes in TPM for the rest\n   of the benchmark matrix.\n3. Memcached [04] achieved 95% CIs [23.54, 32.25]%, [20.76, 41.61]%\n   and [21.59, 30.02]% more operations per second (OPS), respectively,\n   for sequential access, random access and Gaussian (distribution)\n   access, when THP=always; 95% CIs [13.85, 15.97]% and\n   [23.94, 29.92]% more OPS, respectively, for random access and\n   Gaussian access, when THP=never. There were no statistically\n   significant changes in OPS for the rest of the benchmark matrix.\n4. MongoDB [05] achieved 95% CIs [2.23, 3.44]%, [6.97, 9.73]% and\n   [2.16, 3.55]% more operations per second (OPS), respectively, for\n   exponential (distribution) access, random access and Zipfian\n   (distribution) access, when underutilizing memory; 95% CIs\n   [8.83, 10.03]%, [21.12, 23.14]% and [5.53, 6.46]% more OPS,\n   respectively, for exponential access, random access and Zipfian\n   access, when overcommitting memory.\n\nOn 5.15:\n5. Apache Cassandra [06] achieved 95% CIs [1.06, 4.10]%, [1.94, 5.43]%\n   and [4.11, 7.50]% more operations per second (OPS), respectively,\n   for exponential (distribution) access, random access and Zipfian\n   (distribution) access, when swap was off; 95% CIs [0.50, 2.60]%,\n   [6.51, 8.77]% and [3.29, 6.75]% more OPS, respectively, for\n   exponential access, random access and Zipfian access, when swap was\n   on.\n6. Apache Hadoop [07] took 95% CIs [5.31, 9.69]% and [2.02, 7.86]%\n   less average wall time to finish twelve parallel TeraSort jobs,\n   respectively, under the medium- and the high-concurrency\n   conditions, when swap was on. There were no statistically\n   significant changes in average wall time for the rest of the\n   benchmark matrix.\n7. PostgreSQL [08] achieved 95% CI [1.75, 6.42]% more transactions per\n   minute (TPM) under the high-concurrency condition, when swap was\n   off; 95% CIs [12.82, 18.69]% and [22.70, 46.86]% more TPM,\n   respectively, under the medium- and the high-concurrency\n   conditions, when swap was on. There were no statistically\n   significant changes in TPM for the rest of the benchmark matrix.\n8. Redis [09] achieved 95% CIs [0.58, 5.94]%, [6.55, 14.58]% and\n   [11.47, 19.36]% more total operations per second (OPS),\n   respectively, for sequential access, random access and Gaussian\n   (distribution) access, when THP=always; 95% CIs [1.27, 3.54]%,\n   [10.11, 14.81]% and [8.75, 13.64]% more total OPS, respectively,\n   for sequential access, random access and Gaussian access, when\n   THP=never.\n\nOur lab results\n---------------\nTo supplement the above results, we ran the following benchmark suites\non 5.16-rc7 and found no regressions [10].\n      fs_fio_bench_hdd_mq      pft\n      fs_lmbench               pgsql-hammerdb\n      fs_parallelio            redis\n      fs_postmark              stream\n      hackbench                sysbenchthread\n      kernbench                tpcc_spark\n      memcached                unixbench\n      multichase               vm-scalability\n      mutilate                 will-it-scale\n      nginx\n\n[01] https://trends.google.com\n[02] https://lore.kernel.org/r/20211102002002.92051-1-bot@edi.works/\n[03] https://lore.kernel.org/r/20211009054315.47073-1-bot@edi.works/\n[04] https://lore.kernel.org/r/20211021194103.65648-1-bot@edi.works/\n[05] https://lore.kernel.org/r/20211109021346.50266-1-bot@edi.works/\n[06] https://lore.kernel.org/r/20211202062806.80365-1-bot@edi.works/\n[07] https://lore.kernel.org/r/20211209072416.33606-1-bot@edi.works/\n[08] https://lore.kernel.org/r/20211218071041.24077-1-bot@edi.works/\n[09] https://lore.kernel.org/r/20211122053248.57311-1-bot@edi.works/\n[10] https://lore.kernel.org/r/20220104202247.2903702-1-yuzhao@google.com/\n\nRead-world applications\n=======================\nThird-party testimonials\n------------------------\nKonstantin reported [11]:\n   I have Archlinux with 8G RAM + zswap + swap. While developing, I\n   have lots of apps opened such as multiple LSP-servers for different\n   langs, chats, two browsers, etc... Usually, my system gets quickly\n   to a point of SWAP-storms, where I have to kill LSP-servers,\n   restart browsers to free memory, etc, otherwise the system lags\n   heavily and is barely usable.\n   \n   1.5 day ago I migrated from 5.11.15 kernel to 5.12 + the LRU\n   patchset, and I started up by opening lots of apps to create memory\n   pressure, and worked for a day like this. Till now I had not a\n   single SWAP-storm, and mind you I got 3.4G in SWAP. I was never\n   getting to the point of 3G in SWAP before without a single\n   SWAP-storm.\n\nVaibhav from IBM reported [12]:\n   In a synthetic MongoDB Benchmark, seeing an average of ~19%\n   throughput improvement on POWER10(Radix MMU + 64K Page Size) with\n   MGLRU patches on top of 5.16 kernel for MongoDB + YCSB across\n   three different request distributions, namely, Exponential, Uniform\n   and Zipfan.\n\nShuang from U of Rochester reported [13]:\n   With the MGLRU, fio achieved 95% CIs [38.95, 40.26]%, [4.12, 6.64]%\n   and [9.26, 10.36]% higher throughput, respectively, for random\n   access, Zipfian (distribution) access and Gaussian (distribution)\n   access, when the average number of jobs per CPU is 1; 95% CIs\n   [42.32, 49.15]%, [9.44, 9.89]% and [20.99, 22.86]% higher\n   throughput, respectively, for random access, Zipfian access and\n   Gaussian access, when the average number of jobs per CPU is 2.\n\nDaniel from Michigan Tech reported [14]:\n   With Memcached allocating ~100GB of byte-addressable Optante,\n   performance improvement in terms of throughput (measured as queries\n   per second) was about 10% for a series of workloads.\n\nLarge-scale deployments\n-----------------------\nWe've rolled out MGLRU to tens of millions of ChromeOS users and\nabout a million Android users. Google's fleetwide profiling [15] shows\nan overall 40% decrease in kswapd CPU usage, in addition to\nimprovements in other UX metrics, e.g., an 85% decrease in the number\nof low-memory kills at the 75th percentile and an 18% decrease in\napp launch time at the 50th percentile.\n\nThe downstream kernels that have been using MGLRU include:\n1. Android [16]\n2. Arch Linux Zen [17]\n3. Armbian [18]\n4. ChromeOS [19]\n5. Liquorix [20]\n6. OpenWrt [21]\n7. post-factum [22]\n8. XanMod [23]\n\n[11] https://lore.kernel.org/r/140226722f2032c86301fbd326d91baefe3d7d23.camel@yandex.ru/\n[12] https://lore.kernel.org/r/87czj3mux0.fsf@vajain21.in.ibm.com/\n[13] https://lore.kernel.org/r/20220105024423.26409-1-szhai2@cs.rochester.edu/\n[14] https://lore.kernel.org/r/CA+4-3vksGvKd18FgRinxhqHetBS1hQekJE2gwco8Ja-bJWKtFw@mail.gmail.com/\n[15] https://dl.acm.org/doi/10.1145/2749469.2750392\n[16] https://android.com\n[17] https://archlinux.org\n[18] https://armbian.com\n[19] https://chromium.org\n[20] https://liquorix.net\n[21] https://openwrt.org\n[22] https://codeberg.org/pf-kernel\n[23] https://xanmod.org\n\nSummary\n=======\nThe facts are:\n1. The independent lab results and the real-world applications\n   indicate substantial improvements; there are no known regressions.\n2. Thrashing prevention, working set estimation and proactive reclaim\n   work out of the box; there are no equivalent solutions.\n3. There is a lot of new code; no smaller changes have been\n   demonstrated similar effects.\n\nOur options, accordingly, are:\n1. Given the amount of evidence, the reported improvements will likely\n   materialize for a wide range of workloads.\n2. Gauging the interest from the past discussions, the new features\n   will likely be put to use for both personal computers and data\n   centers.\n3. Based on Google's track record, the new code will likely be well\n   maintained in the long term. It'd be more difficult if not\n   impossible to achieve similar effects with other approaches.\n\n\nThis patch (of 14):\n\nSome architectures automatically set the accessed bit in PTEs, e.g., x86\nand arm64 v8.2.  On architectures that do not have this capability,\nclearing the accessed bit in a PTE usually triggers a page fault following\nthe TLB miss of this PTE (to emulate the accessed bit).\n\nBeing aware of this capability can help make better decisions, e.g.,\nwhether to spread the work out over a period of time to reduce bursty page\nfaults when trying to clear the accessed bit in many PTEs.\n\nNote that theoretically this capability can be unreliable, e.g.,\nhotplugged CPUs might be different from builtin ones.  Therefore it should\nnot be used in architecture-independent code that involves correctness,\ne.g., to determine whether TLB flushes are required (in combination with\nthe accessed bit).\n\nLink: https://lkml.kernel.org/r/20220918080010.2920238-1-yuzhao@google.com\nLink: https://lkml.kernel.org/r/20220918080010.2920238-2-yuzhao@google.com\nSigned-off-by: Yu Zhao <yuzhao@google.com>\nReviewed-by: Barry Song <baohua@kernel.org>\nAcked-by: Brian Geffon <bgeffon@google.com>\nAcked-by: Jan Alexander Steffens (heftig) <heftig@archlinux.org>\nAcked-by: Oleksandr Natalenko <oleksandr@natalenko.name>\nAcked-by: Steven Barrett <steven@liquorix.net>\nAcked-by: Suleiman Souhlal <suleiman@google.com>\nAcked-by: Will Deacon <will@kernel.org>\nTested-by: Daniel Byrne <djbyrne@mtu.edu>\nTested-by: Donald Carr <d@chaos-reins.com>\nTested-by: Holger Hoffst\u00e4tte <holger@applied-asynchrony.com>\nTested-by: Konstantin Kharlamov <Hi-Angel@yandex.ru>\nTested-by: Shuang Zhai <szhai2@cs.rochester.edu>\nTested-by: Sofia Trinh <sofia.trinh@edi.works>\nTested-by: Vaibhav Jain <vaibhav@linux.ibm.com>\nCc: Andi Kleen <ak@linux.intel.com>\nCc: Aneesh Kumar K.V <aneesh.kumar@linux.ibm.com>\nCc: Catalin Marinas <catalin.marinas@arm.com>\nCc: Dave Hansen <dave.hansen@linux.intel.com>\nCc: Hillf Danton <hdanton@sina.com>\nCc: Jens Axboe <axboe@kernel.dk>\nCc: Johannes Weiner <hannes@cmpxchg.org>\nCc: Jonathan Corbet <corbet@lwn.net>\nCc: Linus Torvalds <torvalds@linux-foundation.org>\nCc: linux-arm-kernel@lists.infradead.org\nCc: Matthew Wilcox <willy@infradead.org>\nCc: Mel Gorman <mgorman@suse.de>\nCc: Michael Larabel <Michael@MichaelLarabel.com>\nCc: Michal Hocko <mhocko@kernel.org>\nCc: Mike Rapoport <rppt@kernel.org>\nCc: Peter Zijlstra <peterz@infradead.org>\nCc: Tejun Heo <tj@kernel.org>\nCc: Vlastimil Babka <vbabka@suse.cz>\nCc: Miaohe Lin <linmiaohe@huawei.com>\nCc: Mike Rapoport <rppt@linux.ibm.com>\nCc: Qi Zheng <zhengqi.arch@bytedance.com>\nSigned-off-by: Andrew Morton <akpm@linux-foundation.org>",
  "author_name": "Yu Zhao",
  "author_email": "yuzhao@google.com",
  "author_date": "Sun Sep 18 01:59:58 2022 -0600",
  "author_date_iso": "2022-09-18T01:59:58-06:00",
  "committer_name": "Andrew Morton",
  "committer_email": "akpm@linux-foundation.org",
  "committer_date": "Mon Sep 26 19:46:08 2022 -0700",
  "committer_date_iso": "2022-09-26T19:46:08-07:00",
  "files_changed": [
    "arch/arm64/include/asm/pgtable.h",
    "arch/x86/include/asm/pgtable.h",
    "include/linux/pgtable.h",
    "mm/memory.c"
  ],
  "files_changed_count": 4,
  "stats": [
    {
      "file": "arch/arm64/include/asm/pgtable.h",
      "insertions": 2,
      "deletions": 13
    },
    {
      "file": "arch/x86/include/asm/pgtable.h",
      "insertions": 3,
      "deletions": 3
    },
    {
      "file": "include/linux/pgtable.h",
      "insertions": 13,
      "deletions": 0
    },
    {
      "file": "mm/memory.c",
      "insertions": 1,
      "deletions": 13
    }
  ],
  "total_insertions": 19,
  "total_deletions": 29,
  "total_changes": 48,
  "parents": [
    "3a9bb7b1879bef057a5dbff1dac1fa1411638064"
  ],
  "branches": [
    "* development",
    "remotes/origin/HEAD -> origin/master",
    "remotes/origin/master"
  ],
  "tags": [],
  "is_merge": false,
  "security_info": {
    "cve_ids": [],
    "security_keywords": [
      "exploit"
    ]
  },
  "fix_type": "security",
  "file_results": [
    {
      "file": "arch/arm64/include/asm/pgtable.h",
      "pre_version": false,
      "post_version": true,
      "patch": true
    },
    {
      "file": "arch/x86/include/asm/pgtable.h",
      "pre_version": false,
      "post_version": true,
      "patch": true
    },
    {
      "file": "include/linux/pgtable.h",
      "pre_version": false,
      "post_version": true,
      "patch": true
    },
    {
      "file": "mm/memory.c",
      "pre_version": false,
      "post_version": true,
      "patch": true
    }
  ]
}