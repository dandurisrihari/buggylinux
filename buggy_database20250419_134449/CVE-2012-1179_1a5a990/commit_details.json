{
  "hash": "1a5a9906d4e8d1976b701f889d8f35d54b928f25",
  "hash_short": "1a5a9906",
  "subject": "mm: thp: fix pmd_bad() triggering in code paths holding mmap_sem read mode",
  "body": "In some cases it may happen that pmd_none_or_clear_bad() is called with\nthe mmap_sem hold in read mode.  In those cases the huge page faults can\nallocate hugepmds under pmd_none_or_clear_bad() and that can trigger a\nfalse positive from pmd_bad() that will not like to see a pmd\nmaterializing as trans huge.\n\nIt's not khugepaged causing the problem, khugepaged holds the mmap_sem\nin write mode (and all those sites must hold the mmap_sem in read mode\nto prevent pagetables to go away from under them, during code review it\nseems vm86 mode on 32bit kernels requires that too unless it's\nrestricted to 1 thread per process or UP builds).  The race is only with\nthe huge pagefaults that can convert a pmd_none() into a\npmd_trans_huge().\n\nEffectively all these pmd_none_or_clear_bad() sites running with\nmmap_sem in read mode are somewhat speculative with the page faults, and\nthe result is always undefined when they run simultaneously.  This is\nprobably why it wasn't common to run into this.  For example if the\nmadvise(MADV_DONTNEED) runs zap_page_range() shortly before the page\nfault, the hugepage will not be zapped, if the page fault runs first it\nwill be zapped.\n\nAltering pmd_bad() not to error out if it finds hugepmds won't be enough\nto fix this, because zap_pmd_range would then proceed to call\nzap_pte_range (which would be incorrect if the pmd become a\npmd_trans_huge()).\n\nThe simplest way to fix this is to read the pmd in the local stack\n(regardless of what we read, no need of actual CPU barriers, only\ncompiler barrier needed), and be sure it is not changing under the code\nthat computes its value.  Even if the real pmd is changing under the\nvalue we hold on the stack, we don't care.  If we actually end up in\nzap_pte_range it means the pmd was not none already and it was not huge,\nand it can't become huge from under us (khugepaged locking explained\nabove).\n\nAll we need is to enforce that there is no way anymore that in a code\npath like below, pmd_trans_huge can be false, but pmd_none_or_clear_bad\ncan run into a hugepmd.  The overhead of a barrier() is just a compiler\ntweak and should not be measurable (I only added it for THP builds).  I\ndon't exclude different compiler versions may have prevented the race\ntoo by caching the value of *pmd on the stack (that hasn't been\nverified, but it wouldn't be impossible considering\npmd_none_or_clear_bad, pmd_bad, pmd_trans_huge, pmd_none are all inlines\nand there's no external function called in between pmd_trans_huge and\npmd_none_or_clear_bad).\n\n\t\tif (pmd_trans_huge(*pmd)) {\n\t\t\tif (next-addr != HPAGE_PMD_SIZE) {\n\t\t\t\tVM_BUG_ON(!rwsem_is_locked(&tlb->mm->mmap_sem));\n\t\t\t\tsplit_huge_page_pmd(vma->vm_mm, pmd);\n\t\t\t} else if (zap_huge_pmd(tlb, vma, pmd, addr))\n\t\t\t\tcontinue;\n\t\t\t/* fall through */\n\t\t}\n\t\tif (pmd_none_or_clear_bad(pmd))\n\nBecause this race condition could be exercised without special\nprivileges this was reported in CVE-2012-1179.\n\nThe race was identified and fully explained by Ulrich who debugged it.\nI'm quoting his accurate explanation below, for reference.\n\n====== start quote =======\n      mapcount 0 page_mapcount 1\n      kernel BUG at mm/huge_memory.c:1384!\n\n    At some point prior to the panic, a \"bad pmd ...\" message similar to the\n    following is logged on the console:\n\n      mm/memory.c:145: bad pmd ffff8800376e1f98(80000000314000e7).\n\n    The \"bad pmd ...\" message is logged by pmd_clear_bad() before it clears\n    the page's PMD table entry.\n\n        143 void pmd_clear_bad(pmd_t *pmd)\n        144 {\n    ->  145         pmd_ERROR(*pmd);\n        146         pmd_clear(pmd);\n        147 }\n\n    After the PMD table entry has been cleared, there is an inconsistency\n    between the actual number of PMD table entries that are mapping the page\n    and the page's map count (_mapcount field in struct page). When the page\n    is subsequently reclaimed, __split_huge_page() detects this inconsistency.\n\n       1381         if (mapcount != page_mapcount(page))\n       1382                 printk(KERN_ERR \"mapcount %d page_mapcount %d\\n\",\n       1383                        mapcount, page_mapcount(page));\n    -> 1384         BUG_ON(mapcount != page_mapcount(page));\n\n    The root cause of the problem is a race of two threads in a multithreaded\n    process. Thread B incurs a page fault on a virtual address that has never\n    been accessed (PMD entry is zero) while Thread A is executing an madvise()\n    system call on a virtual address within the same 2 MB (huge page) range.\n\n               virtual address space\n              .---------------------.\n              |                     |\n              |                     |\n            .-|---------------------|\n            | |                     |\n            | |                     |<-- B(fault)\n            | |                     |\n      2 MB  | |/////////////////////|-.\n      huge <  |/////////////////////|  > A(range)\n      page  | |/////////////////////|-'\n            | |                     |\n            | |                     |\n            '-|---------------------|\n              |                     |\n              |                     |\n              '---------------------'\n\n    - Thread A is executing an madvise(..., MADV_DONTNEED) system call\n      on the virtual address range \"A(range)\" shown in the picture.\n\n    sys_madvise\n      // Acquire the semaphore in shared mode.\n      down_read(&current->mm->mmap_sem)\n      ...\n      madvise_vma\n        switch (behavior)\n        case MADV_DONTNEED:\n             madvise_dontneed\n               zap_page_range\n                 unmap_vmas\n                   unmap_page_range\n                     zap_pud_range\n                       zap_pmd_range\n                         //\n                         // Assume that this huge page has never been accessed.\n                         // I.e. content of the PMD entry is zero (not mapped).\n                         //\n                         if (pmd_trans_huge(*pmd)) {\n                             // We don't get here due to the above assumption.\n                         }\n                         //\n                         // Assume that Thread B incurred a page fault and\n             .---------> // sneaks in here as shown below.\n             |           //\n             |           if (pmd_none_or_clear_bad(pmd))\n             |               {\n             |                 if (unlikely(pmd_bad(*pmd)))\n             |                     pmd_clear_bad\n             |                     {\n             |                       pmd_ERROR\n             |                         // Log \"bad pmd ...\" message here.\n             |                       pmd_clear\n             |                         // Clear the page's PMD entry.\n             |                         // Thread B incremented the map count\n             |                         // in page_add_new_anon_rmap(), but\n             |                         // now the page is no longer mapped\n             |                         // by a PMD entry (-> inconsistency).\n             |                     }\n             |               }\n             |\n             v\n    - Thread B is handling a page fault on virtual address \"B(fault)\" shown\n      in the picture.\n\n    ...\n    do_page_fault\n      __do_page_fault\n        // Acquire the semaphore in shared mode.\n        down_read_trylock(&mm->mmap_sem)\n        ...\n        handle_mm_fault\n          if (pmd_none(*pmd) && transparent_hugepage_enabled(vma))\n              // We get here due to the above assumption (PMD entry is zero).\n              do_huge_pmd_anonymous_page\n                alloc_hugepage_vma\n                  // Allocate a new transparent huge page here.\n                ...\n                __do_huge_pmd_anonymous_page\n                  ...\n                  spin_lock(&mm->page_table_lock)\n                  ...\n                  page_add_new_anon_rmap\n                    // Here we increment the page's map count (starts at -1).\n                    atomic_set(&page->_mapcount, 0)\n                  set_pmd_at\n                    // Here we set the page's PMD entry which will be cleared\n                    // when Thread A calls pmd_clear_bad().\n                  ...\n                  spin_unlock(&mm->page_table_lock)\n\n    The mmap_sem does not prevent the race because both threads are acquiring\n    it in shared mode (down_read).  Thread B holds the page_table_lock while\n    the page's map count and PMD table entry are updated.  However, Thread A\n    does not synchronize on that lock.\n\n====== end quote =======\n\n[akpm@linux-foundation.org: checkpatch fixes]\nReported-by: Ulrich Obergfell <uobergfe@redhat.com>\nSigned-off-by: Andrea Arcangeli <aarcange@redhat.com>\nAcked-by: Johannes Weiner <hannes@cmpxchg.org>\nCc: Mel Gorman <mgorman@suse.de>\nCc: Hugh Dickins <hughd@google.com>\nCc: Dave Jones <davej@redhat.com>\nAcked-by: Larry Woodman <lwoodman@redhat.com>\nAcked-by: Rik van Riel <riel@redhat.com>\nCc: <stable@vger.kernel.org>\t\t[2.6.38+]\nCc: Mark Salter <msalter@redhat.com>\nSigned-off-by: Andrew Morton <akpm@linux-foundation.org>\nSigned-off-by: Linus Torvalds <torvalds@linux-foundation.org>",
  "full_message": "mm: thp: fix pmd_bad() triggering in code paths holding mmap_sem read mode\n\nIn some cases it may happen that pmd_none_or_clear_bad() is called with\nthe mmap_sem hold in read mode.  In those cases the huge page faults can\nallocate hugepmds under pmd_none_or_clear_bad() and that can trigger a\nfalse positive from pmd_bad() that will not like to see a pmd\nmaterializing as trans huge.\n\nIt's not khugepaged causing the problem, khugepaged holds the mmap_sem\nin write mode (and all those sites must hold the mmap_sem in read mode\nto prevent pagetables to go away from under them, during code review it\nseems vm86 mode on 32bit kernels requires that too unless it's\nrestricted to 1 thread per process or UP builds).  The race is only with\nthe huge pagefaults that can convert a pmd_none() into a\npmd_trans_huge().\n\nEffectively all these pmd_none_or_clear_bad() sites running with\nmmap_sem in read mode are somewhat speculative with the page faults, and\nthe result is always undefined when they run simultaneously.  This is\nprobably why it wasn't common to run into this.  For example if the\nmadvise(MADV_DONTNEED) runs zap_page_range() shortly before the page\nfault, the hugepage will not be zapped, if the page fault runs first it\nwill be zapped.\n\nAltering pmd_bad() not to error out if it finds hugepmds won't be enough\nto fix this, because zap_pmd_range would then proceed to call\nzap_pte_range (which would be incorrect if the pmd become a\npmd_trans_huge()).\n\nThe simplest way to fix this is to read the pmd in the local stack\n(regardless of what we read, no need of actual CPU barriers, only\ncompiler barrier needed), and be sure it is not changing under the code\nthat computes its value.  Even if the real pmd is changing under the\nvalue we hold on the stack, we don't care.  If we actually end up in\nzap_pte_range it means the pmd was not none already and it was not huge,\nand it can't become huge from under us (khugepaged locking explained\nabove).\n\nAll we need is to enforce that there is no way anymore that in a code\npath like below, pmd_trans_huge can be false, but pmd_none_or_clear_bad\ncan run into a hugepmd.  The overhead of a barrier() is just a compiler\ntweak and should not be measurable (I only added it for THP builds).  I\ndon't exclude different compiler versions may have prevented the race\ntoo by caching the value of *pmd on the stack (that hasn't been\nverified, but it wouldn't be impossible considering\npmd_none_or_clear_bad, pmd_bad, pmd_trans_huge, pmd_none are all inlines\nand there's no external function called in between pmd_trans_huge and\npmd_none_or_clear_bad).\n\n\t\tif (pmd_trans_huge(*pmd)) {\n\t\t\tif (next-addr != HPAGE_PMD_SIZE) {\n\t\t\t\tVM_BUG_ON(!rwsem_is_locked(&tlb->mm->mmap_sem));\n\t\t\t\tsplit_huge_page_pmd(vma->vm_mm, pmd);\n\t\t\t} else if (zap_huge_pmd(tlb, vma, pmd, addr))\n\t\t\t\tcontinue;\n\t\t\t/* fall through */\n\t\t}\n\t\tif (pmd_none_or_clear_bad(pmd))\n\nBecause this race condition could be exercised without special\nprivileges this was reported in CVE-2012-1179.\n\nThe race was identified and fully explained by Ulrich who debugged it.\nI'm quoting his accurate explanation below, for reference.\n\n====== start quote =======\n      mapcount 0 page_mapcount 1\n      kernel BUG at mm/huge_memory.c:1384!\n\n    At some point prior to the panic, a \"bad pmd ...\" message similar to the\n    following is logged on the console:\n\n      mm/memory.c:145: bad pmd ffff8800376e1f98(80000000314000e7).\n\n    The \"bad pmd ...\" message is logged by pmd_clear_bad() before it clears\n    the page's PMD table entry.\n\n        143 void pmd_clear_bad(pmd_t *pmd)\n        144 {\n    ->  145         pmd_ERROR(*pmd);\n        146         pmd_clear(pmd);\n        147 }\n\n    After the PMD table entry has been cleared, there is an inconsistency\n    between the actual number of PMD table entries that are mapping the page\n    and the page's map count (_mapcount field in struct page). When the page\n    is subsequently reclaimed, __split_huge_page() detects this inconsistency.\n\n       1381         if (mapcount != page_mapcount(page))\n       1382                 printk(KERN_ERR \"mapcount %d page_mapcount %d\\n\",\n       1383                        mapcount, page_mapcount(page));\n    -> 1384         BUG_ON(mapcount != page_mapcount(page));\n\n    The root cause of the problem is a race of two threads in a multithreaded\n    process. Thread B incurs a page fault on a virtual address that has never\n    been accessed (PMD entry is zero) while Thread A is executing an madvise()\n    system call on a virtual address within the same 2 MB (huge page) range.\n\n               virtual address space\n              .---------------------.\n              |                     |\n              |                     |\n            .-|---------------------|\n            | |                     |\n            | |                     |<-- B(fault)\n            | |                     |\n      2 MB  | |/////////////////////|-.\n      huge <  |/////////////////////|  > A(range)\n      page  | |/////////////////////|-'\n            | |                     |\n            | |                     |\n            '-|---------------------|\n              |                     |\n              |                     |\n              '---------------------'\n\n    - Thread A is executing an madvise(..., MADV_DONTNEED) system call\n      on the virtual address range \"A(range)\" shown in the picture.\n\n    sys_madvise\n      // Acquire the semaphore in shared mode.\n      down_read(&current->mm->mmap_sem)\n      ...\n      madvise_vma\n        switch (behavior)\n        case MADV_DONTNEED:\n             madvise_dontneed\n               zap_page_range\n                 unmap_vmas\n                   unmap_page_range\n                     zap_pud_range\n                       zap_pmd_range\n                         //\n                         // Assume that this huge page has never been accessed.\n                         // I.e. content of the PMD entry is zero (not mapped).\n                         //\n                         if (pmd_trans_huge(*pmd)) {\n                             // We don't get here due to the above assumption.\n                         }\n                         //\n                         // Assume that Thread B incurred a page fault and\n             .---------> // sneaks in here as shown below.\n             |           //\n             |           if (pmd_none_or_clear_bad(pmd))\n             |               {\n             |                 if (unlikely(pmd_bad(*pmd)))\n             |                     pmd_clear_bad\n             |                     {\n             |                       pmd_ERROR\n             |                         // Log \"bad pmd ...\" message here.\n             |                       pmd_clear\n             |                         // Clear the page's PMD entry.\n             |                         // Thread B incremented the map count\n             |                         // in page_add_new_anon_rmap(), but\n             |                         // now the page is no longer mapped\n             |                         // by a PMD entry (-> inconsistency).\n             |                     }\n             |               }\n             |\n             v\n    - Thread B is handling a page fault on virtual address \"B(fault)\" shown\n      in the picture.\n\n    ...\n    do_page_fault\n      __do_page_fault\n        // Acquire the semaphore in shared mode.\n        down_read_trylock(&mm->mmap_sem)\n        ...\n        handle_mm_fault\n          if (pmd_none(*pmd) && transparent_hugepage_enabled(vma))\n              // We get here due to the above assumption (PMD entry is zero).\n              do_huge_pmd_anonymous_page\n                alloc_hugepage_vma\n                  // Allocate a new transparent huge page here.\n                ...\n                __do_huge_pmd_anonymous_page\n                  ...\n                  spin_lock(&mm->page_table_lock)\n                  ...\n                  page_add_new_anon_rmap\n                    // Here we increment the page's map count (starts at -1).\n                    atomic_set(&page->_mapcount, 0)\n                  set_pmd_at\n                    // Here we set the page's PMD entry which will be cleared\n                    // when Thread A calls pmd_clear_bad().\n                  ...\n                  spin_unlock(&mm->page_table_lock)\n\n    The mmap_sem does not prevent the race because both threads are acquiring\n    it in shared mode (down_read).  Thread B holds the page_table_lock while\n    the page's map count and PMD table entry are updated.  However, Thread A\n    does not synchronize on that lock.\n\n====== end quote =======\n\n[akpm@linux-foundation.org: checkpatch fixes]\nReported-by: Ulrich Obergfell <uobergfe@redhat.com>\nSigned-off-by: Andrea Arcangeli <aarcange@redhat.com>\nAcked-by: Johannes Weiner <hannes@cmpxchg.org>\nCc: Mel Gorman <mgorman@suse.de>\nCc: Hugh Dickins <hughd@google.com>\nCc: Dave Jones <davej@redhat.com>\nAcked-by: Larry Woodman <lwoodman@redhat.com>\nAcked-by: Rik van Riel <riel@redhat.com>\nCc: <stable@vger.kernel.org>\t\t[2.6.38+]\nCc: Mark Salter <msalter@redhat.com>\nSigned-off-by: Andrew Morton <akpm@linux-foundation.org>\nSigned-off-by: Linus Torvalds <torvalds@linux-foundation.org>",
  "author_name": "Andrea Arcangeli",
  "author_email": "aarcange@redhat.com",
  "author_date": "Wed Mar 21 16:33:42 2012 -0700",
  "author_date_iso": "2012-03-21T16:33:42-07:00",
  "committer_name": "Linus Torvalds",
  "committer_email": "torvalds@linux-foundation.org",
  "committer_date": "Wed Mar 21 17:54:54 2012 -0700",
  "committer_date_iso": "2012-03-21T17:54:54-07:00",
  "files_changed": [
    "arch/x86/kernel/vm86_32.c",
    "fs/proc/task_mmu.c",
    "include/asm-generic/pgtable.h",
    "mm/memcontrol.c",
    "mm/memory.c",
    "mm/mempolicy.c",
    "mm/mincore.c",
    "mm/pagewalk.c",
    "mm/swapfile.c"
  ],
  "files_changed_count": 9,
  "stats": [
    {
      "file": "arch/x86/kernel/vm86_32.c",
      "insertions": 2,
      "deletions": 0
    },
    {
      "file": "fs/proc/task_mmu.c",
      "insertions": 9,
      "deletions": 0
    },
    {
      "file": "include/asm-generic/pgtable.h",
      "insertions": 61,
      "deletions": 0
    },
    {
      "file": "mm/memcontrol.c",
      "insertions": 4,
      "deletions": 0
    },
    {
      "file": "mm/memory.c",
      "insertions": 12,
      "deletions": 4
    },
    {
      "file": "mm/mempolicy.c",
      "insertions": 1,
      "deletions": 1
    },
    {
      "file": "mm/mincore.c",
      "insertions": 1,
      "deletions": 1
    },
    {
      "file": "mm/pagewalk.c",
      "insertions": 1,
      "deletions": 1
    },
    {
      "file": "mm/swapfile.c",
      "insertions": 1,
      "deletions": 3
    }
  ],
  "total_insertions": 92,
  "total_deletions": 10,
  "total_changes": 102,
  "parents": [
    "31f6765266417c0d99f0e922fe82848a7c9c2ae9"
  ],
  "branches": [
    "* development",
    "master",
    "remotes/origin/HEAD -> origin/master",
    "remotes/origin/master"
  ],
  "tags": [
    "v3.10",
    "v3.10-rc1",
    "v3.10-rc2",
    "v3.10-rc3",
    "v3.10-rc4",
    "v3.10-rc5",
    "v3.10-rc6",
    "v3.10-rc7",
    "v3.11",
    "v3.11-rc1"
  ],
  "is_merge": false,
  "security_info": {
    "cve_ids": [
      "CVE-2012-1179"
    ],
    "security_keywords": []
  },
  "fix_type": "cve",
  "file_results": [
    {
      "file": "arch/x86/kernel/vm86_32.c",
      "pre_version": false,
      "post_version": true,
      "patch": true
    },
    {
      "file": "fs/proc/task_mmu.c",
      "pre_version": false,
      "post_version": true,
      "patch": true
    },
    {
      "file": "include/asm-generic/pgtable.h",
      "pre_version": false,
      "post_version": true,
      "patch": true
    },
    {
      "file": "mm/pagewalk.c",
      "pre_version": false,
      "post_version": true,
      "patch": true
    },
    {
      "file": "mm/memory.c",
      "pre_version": false,
      "post_version": true,
      "patch": true
    },
    {
      "file": "mm/mempolicy.c",
      "pre_version": false,
      "post_version": true,
      "patch": true
    },
    {
      "file": "mm/memcontrol.c",
      "pre_version": false,
      "post_version": true,
      "patch": true
    },
    {
      "file": "mm/mincore.c",
      "pre_version": false,
      "post_version": true,
      "patch": true
    },
    {
      "file": "mm/swapfile.c",
      "pre_version": false,
      "post_version": true,
      "patch": true
    }
  ]
}