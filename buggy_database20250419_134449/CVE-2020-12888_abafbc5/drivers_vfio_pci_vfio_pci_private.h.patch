commit abafbc551fddede3e0a08dee1dcde08fc0eb8476
Author: Alex Williamson <alex.williamson@redhat.com>
Date:   Wed Apr 22 13:48:11 2020 -0600

    vfio-pci: Invalidate mmaps and block MMIO access on disabled memory
    
    Accessing the disabled memory space of a PCI device would typically
    result in a master abort response on conventional PCI, or an
    unsupported request on PCI express.  The user would generally see
    these as a -1 response for the read return data and the write would be
    silently discarded, possibly with an uncorrected, non-fatal AER error
    triggered on the host.  Some systems however take it upon themselves
    to bring down the entire system when they see something that might
    indicate a loss of data, such as this discarded write to a disabled
    memory space.
    
    To avoid this, we want to try to block the user from accessing memory
    spaces while they're disabled.  We start with a semaphore around the
    memory enable bit, where writers modify the memory enable state and
    must be serialized, while readers make use of the memory region and
    can access in parallel.  Writers include both direct manipulation via
    the command register, as well as any reset path where the internal
    mechanics of the reset may both explicitly and implicitly disable
    memory access, and manipulation of the MSI-X configuration, where the
    MSI-X vector table resides in MMIO space of the device.  Readers
    include the read and write file ops to access the vfio device fd
    offsets as well as memory mapped access.  In the latter case, we make
    use of our new vma list support to zap, or invalidate, those memory
    mappings in order to force them to be faulted back in on access.
    
    Our semaphore usage will stall user access to MMIO spaces across
    internal operations like reset, but the user might experience new
    behavior when trying to access the MMIO space while disabled via the
    PCI command register.  Access via read or write while disabled will
    return -EIO and access via memory maps will result in a SIGBUS.  This
    is expected to be compatible with known use cases and potentially
    provides better error handling capabilities than present in the
    hardware, while avoiding the more readily accessible and severe
    platform error responses that might otherwise occur.
    
    Fixes: CVE-2020-12888
    Reviewed-by: Peter Xu <peterx@redhat.com>
    Signed-off-by: Alex Williamson <alex.williamson@redhat.com>

diff --git a/drivers/vfio/pci/vfio_pci_private.h b/drivers/vfio/pci/vfio_pci_private.h
index 9b25f9f6ce1d..86a02aff8735 100644
--- a/drivers/vfio/pci/vfio_pci_private.h
+++ b/drivers/vfio/pci/vfio_pci_private.h
@@ -139,6 +139,7 @@ struct vfio_pci_device {
 	struct notifier_block	nb;
 	struct mutex		vma_lock;
 	struct list_head	vma_list;
+	struct rw_semaphore	memory_lock;
 };
 
 #define is_intx(vdev) (vdev->irq_type == VFIO_PCI_INTX_IRQ_INDEX)
@@ -181,6 +182,13 @@ extern int vfio_pci_register_dev_region(struct vfio_pci_device *vdev,
 extern int vfio_pci_set_power_state(struct vfio_pci_device *vdev,
 				    pci_power_t state);
 
+extern bool __vfio_pci_memory_enabled(struct vfio_pci_device *vdev);
+extern void vfio_pci_zap_and_down_write_memory_lock(struct vfio_pci_device
+						    *vdev);
+extern u16 vfio_pci_memory_lock_and_enable(struct vfio_pci_device *vdev);
+extern void vfio_pci_memory_unlock_and_restore(struct vfio_pci_device *vdev,
+					       u16 cmd);
+
 #ifdef CONFIG_VFIO_PCI_IGD
 extern int vfio_pci_igd_init(struct vfio_pci_device *vdev);
 #else