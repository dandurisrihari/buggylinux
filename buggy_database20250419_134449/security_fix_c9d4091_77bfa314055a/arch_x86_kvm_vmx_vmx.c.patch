commit c9d40913ac5a21eb2b976bb221a4677540e84eba
Author: Paolo Bonzini <pbonzini@redhat.com>
Date:   Fri May 22 11:21:49 2020 -0400

    KVM: x86: enable event window in inject_pending_event
    
    In case an interrupt arrives after nested.check_events but before the
    call to kvm_cpu_has_injectable_intr, we could end up enabling the interrupt
    window even if the interrupt is actually going to be a vmexit.  This is
    useless rather than harmful, but it really complicates reasoning about
    SVM's handling of the VINTR intercept.  We'd like to never bother with
    the VINTR intercept if V_INTR_MASKING=1 && INTERCEPT_INTR=1, because in
    that case there is no interrupt window and we can just exit the nested
    guest whenever we want.
    
    This patch moves the opening of the interrupt window inside
    inject_pending_event.  This consolidates the check for pending
    interrupt/NMI/SMI in one place, and makes KVM's usage of immediate
    exits more consistent, extending it beyond just nested virtualization.
    
    There are two functional changes here.  They only affect corner cases,
    but overall they simplify the inject_pending_event.
    
    - re-injection of still-pending events will also use req_immediate_exit
    instead of using interrupt-window intercepts.  This should have no impact
    on performance on Intel since it simply replaces an interrupt-window
    or NMI-window exit for a preemption-timer exit.  On AMD, which has no
    equivalent of the preemption time, it may incur some overhead but an
    actual effect on performance should only be visible in pathological cases.
    
    - kvm_arch_interrupt_allowed and kvm_vcpu_has_events will return true
    if an interrupt, NMI or SMI is blocked by nested_run_pending.  This
    makes sense because entering the VM will allow it to make progress
    and deliver the event.
    
    Signed-off-by: Paolo Bonzini <pbonzini@redhat.com>

diff --git a/arch/x86/kvm/vmx/vmx.c b/arch/x86/kvm/vmx/vmx.c
index 4e76e30b661c..b3a41645e157 100644
--- a/arch/x86/kvm/vmx/vmx.c
+++ b/arch/x86/kvm/vmx/vmx.c
@@ -4551,14 +4551,14 @@ bool vmx_nmi_blocked(struct kvm_vcpu *vcpu)
 		 GUEST_INTR_STATE_NMI));
 }
 
-static bool vmx_nmi_allowed(struct kvm_vcpu *vcpu, bool for_injection)
+static int vmx_nmi_allowed(struct kvm_vcpu *vcpu, bool for_injection)
 {
 	if (to_vmx(vcpu)->nested.nested_run_pending)
-		return false;
+		return -EBUSY;
 
 	/* An NMI must not be injected into L2 if it's supposed to VM-Exit.  */
 	if (for_injection && is_guest_mode(vcpu) && nested_exit_on_nmi(vcpu))
-		return false;
+		return -EBUSY;
 
 	return !vmx_nmi_blocked(vcpu);
 }
@@ -4573,17 +4573,17 @@ bool vmx_interrupt_blocked(struct kvm_vcpu *vcpu)
 		(GUEST_INTR_STATE_STI | GUEST_INTR_STATE_MOV_SS));
 }
 
-static bool vmx_interrupt_allowed(struct kvm_vcpu *vcpu, bool for_injection)
+static int vmx_interrupt_allowed(struct kvm_vcpu *vcpu, bool for_injection)
 {
 	if (to_vmx(vcpu)->nested.nested_run_pending)
-		return false;
+		return -EBUSY;
 
        /*
         * An IRQ must not be injected into L2 if it's supposed to VM-Exit,
         * e.g. if the IRQ arrived asynchronously after checking nested events.
         */
 	if (for_injection && is_guest_mode(vcpu) && nested_exit_on_intr(vcpu))
-		return false;
+		return -EBUSY;
 
 	return !vmx_interrupt_blocked(vcpu);
 }
@@ -7757,11 +7757,11 @@ static void vmx_setup_mce(struct kvm_vcpu *vcpu)
 			~FEAT_CTL_LMCE_ENABLED;
 }
 
-static bool vmx_smi_allowed(struct kvm_vcpu *vcpu, bool for_injection)
+static int vmx_smi_allowed(struct kvm_vcpu *vcpu, bool for_injection)
 {
 	/* we need a nested vmexit to enter SMM, postpone if run is pending */
 	if (to_vmx(vcpu)->nested.nested_run_pending)
-		return false;
+		return -EBUSY;
 	return !is_smm(vcpu);
 }
 
@@ -7799,9 +7799,9 @@ static int vmx_pre_leave_smm(struct kvm_vcpu *vcpu, const char *smstate)
 	return 0;
 }
 
-static int enable_smi_window(struct kvm_vcpu *vcpu)
+static void enable_smi_window(struct kvm_vcpu *vcpu)
 {
-	return 0;
+	/* RSM will cause a vmexit anyway.  */
 }
 
 static bool vmx_need_emulation_on_page_fault(struct kvm_vcpu *vcpu)