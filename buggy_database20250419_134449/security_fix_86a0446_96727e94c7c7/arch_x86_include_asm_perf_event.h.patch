commit 86a04461a99fb857bd7d7f87b234cae27df07f8a
Author: Andi Kleen <ak@linux.intel.com>
Date:   Mon Aug 11 21:27:10 2014 +0200

    perf/x86: Revamp PEBS event selection
    
    The basic idea is that it does not make sense to list all PEBS
    events individually. The list is very long, sometimes outdated
    and the hardware doesn't need it. If an event does not support
    PEBS it will just not count, there is no security issue.
    
    We need to only list events that something special, like
    supporting load or store addresses.
    
    This vastly simplifies the PEBS event selection. It also
    speeds up the scheduling because the scheduler doesn't
    have to walk as many constraints.
    
    Bugs fixed:
    
     - We do not allow setting forbidden flags with PEBS anymore
       (SDM 18.9.4), except for the special cycle event.
       This is done using a new constraint macro that also
       matches on the event flags.
    
     - Correct DataLA and load/store/na flags reporting on Haswell
       [Requires a followon patch]
    
     - We did not allow all PEBS events on Haswell:
       We were missing some valid subevents in d1-d2 (MEM_LOAD_UOPS_RETIRED.*,
       MEM_LOAD_UOPS_RETIRED_L3_HIT_RETIRED.*)
    
    This includes the changes proposed by Stephane earlier and obsoletes
    his patchkit (except for some changes on pre Sandy Bridge/Silvermont
    CPUs)
    
    I only did Sandy Bridge and Silvermont and later so far, mostly because these
    are the parts I could directly confirm the hardware behavior with hardware
    architects. Also I do not believe the older CPUs have any
    missing events in their PEBS list, so there's no pressing
    need to change them.
    
    I did not implement the flag proposed by Peter to allow
    setting forbidden flags. If really needed this could
    be implemented on to of this patch.
    
    v2: Fix broken store events on SNB/IVB (Stephane Eranian)
    v3: More fixes. Rename some arguments (Stephane Eranian)
    v4: List most Haswell events individually again to report
    memory operation type correctly.
    Add new flags to describe load/store/na for datala.
    Update description.
    
    Signed-off-by: Andi Kleen <ak@linux.intel.com>
    Reviewed-by: Stephane Eranian <eranian@google.com>
    Signed-off-by: Peter Zijlstra <peterz@infradead.org>
    Link: http://lkml.kernel.org/r/1407785233-32193-2-git-send-email-eranian@google.com
    Cc: Arnaldo Carvalho de Melo <acme@kernel.org>
    Cc: Kan Liang <kan.liang@intel.com>
    Cc: Linus Torvalds <torvalds@linux-foundation.org>
    Cc: Maria Dimakopoulou <maria.n.dimakopoulou@gmail.com>
    Cc: Mark Davies <junk@eslaf.co.uk>
    Cc: Paul Mackerras <paulus@samba.org>
    Cc: Stephane Eranian <eranian@google.com>
    Cc: Yan, Zheng <zheng.z.yan@intel.com>
    Signed-off-by: Ingo Molnar <mingo@kernel.org>

diff --git a/arch/x86/include/asm/perf_event.h b/arch/x86/include/asm/perf_event.h
index 8249df45d2f2..8dfc9fd094a3 100644
--- a/arch/x86/include/asm/perf_event.h
+++ b/arch/x86/include/asm/perf_event.h
@@ -51,6 +51,14 @@
 	 ARCH_PERFMON_EVENTSEL_EDGE  |	\
 	 ARCH_PERFMON_EVENTSEL_INV   |	\
 	 ARCH_PERFMON_EVENTSEL_CMASK)
+#define X86_ALL_EVENT_FLAGS  			\
+	(ARCH_PERFMON_EVENTSEL_EDGE |  		\
+	 ARCH_PERFMON_EVENTSEL_INV | 		\
+	 ARCH_PERFMON_EVENTSEL_CMASK | 		\
+	 ARCH_PERFMON_EVENTSEL_ANY | 		\
+	 ARCH_PERFMON_EVENTSEL_PIN_CONTROL | 	\
+	 HSW_IN_TX | 				\
+	 HSW_IN_TX_CHECKPOINTED)
 #define AMD64_RAW_EVENT_MASK		\
 	(X86_RAW_EVENT_MASK          |  \
 	 AMD64_EVENTSEL_EVENT)