commit ac12cf85d682a2c1948210c65f7fb21ef01dd9f6
Merge: f32c7a8e4510 b333b0ba2346 d06fa5a118f1 42d038c4fb00 3724e186fead d55c5f28afaf dd753d961c48 ebef746543fd 92af2b696119 5c062ef4155b
Author: Will Deacon <will@kernel.org>
Date:   Fri Aug 30 12:46:12 2019 +0100

    Merge branches 'for-next/52-bit-kva', 'for-next/cpu-topology', 'for-next/error-injection', 'for-next/perf', 'for-next/psci-cpuidle', 'for-next/rng', 'for-next/smpboot', 'for-next/tbi' and 'for-next/tlbi' into for-next/core
    
    * for-next/52-bit-kva: (25 commits)
      Support for 52-bit virtual addressing in kernel space
    
    * for-next/cpu-topology: (9 commits)
      Move CPU topology parsing into core code and add support for ACPI 6.3
    
    * for-next/error-injection: (2 commits)
      Support for function error injection via kprobes
    
    * for-next/perf: (8 commits)
      Support for i.MX8 DDR PMU and proper SMMUv3 group validation
    
    * for-next/psci-cpuidle: (7 commits)
      Move PSCI idle code into a new CPUidle driver
    
    * for-next/rng: (4 commits)
      Support for 'rng-seed' property being passed in the devicetree
    
    * for-next/smpboot: (3 commits)
      Reduce fragility of secondary CPU bringup in debug configurations
    
    * for-next/tbi: (10 commits)
      Introduce new syscall ABI with relaxed requirements for pointer tags
    
    * for-next/tlbi: (6 commits)
      Handle spurious page faults arising from kernel space

diff --cc arch/arm64/include/asm/memory.h
index fb04f10a78ab,a713bad71db5,fb04f10a78ab,fb04f10a78ab,b7ba75809751,fb04f10a78ab,fb04f10a78ab,fb04f10a78ab,76e0b232a473,fb04f10a78ab..b61b50bf68b1
--- a/arch/arm64/include/asm/memory.h
+++ b/arch/arm64/include/asm/memory.h
@@@@@@@@@@@ -53,10 -58,18 -53,10 -53,10 -53,10 -53,10 -53,10 -53,10 -53,10 -53,10 +58,18 @@@@@@@@@@@
          #define PCI_IO_START		(PCI_IO_END - PCI_IO_SIZE)
          #define FIXADDR_TOP		(PCI_IO_START - SZ_2M)
          
- --------#define KERNEL_START      _text
- --------#define KERNEL_END        _end
+ ++++++++#if VA_BITS > 48
+ ++++++++#define VA_BITS_MIN		(48)
+ ++++++++#else
+ ++++++++#define VA_BITS_MIN		(VA_BITS)
+ ++++++++#endif
+ ++++++++
+ ++++++++#define _PAGE_END(va)		(-(UL(1) << ((va) - 1)))
    +     
- -- -----#ifdef CONFIG_ARM64_USER_VA_BITS_52
+ ++++++++#define KERNEL_START		_text
+ ++++++++#define KERNEL_END		_end
+ ++ +++++
    -     #ifdef CONFIG_ARM64_USER_VA_BITS_52
+ ++++++++#ifdef CONFIG_ARM64_VA_BITS_52
          #define MAX_USER_VA_BITS	52
          #else
          #define MAX_USER_VA_BITS	VA_BITS
@@@@@@@@@@@ -208,17 -223,17 -208,17 -208,17 -208,13 -208,17 -208,17 -208,17 -208,17 -208,17 +223,17 @@@@@@@@@@@ static inline unsigned long kaslr_offse
          #define __tag_reset(addr)	untagged_addr(addr)
          #define __tag_get(addr)		(__u8)((u64)(addr) >> 56)
          #else
    -     #define __tag_set(addr, tag)	(addr)
+ ++++++++#define __tag_shifted(tag)	0UL
+ ++ +++++#define __tag_reset(addr)	(addr)
+ ++ +++++#define __tag_get(addr)		0
    -     #endif
+ ++++++++#endif /* CONFIG_KASAN_SW_TAGS */
+ ++++++++
    +     static inline const void *__tag_set(const void *addr, u8 tag)
    +     {
- -- -----	return addr;
+ ++++++++	u64 __addr = (u64)addr & ~__tag_shifted(0xff);
+ ++++++++	return (const void *)(__addr | __tag_shifted(tag));
    +     }
          
- -- -----#define __tag_reset(addr)	(addr)
- -- -----#define __tag_get(addr)		0
- -- -----#endif
- -- -----
          /*
           * Physical vs virtual RAM address space conversion.  These are
           * private definitions which should NOT be used outside memory.h
@@@@@@@@@@@ -296,31 -310,28 -296,31 -296,31 -292,31 -296,31 -296,31 -296,31 -296,31 -296,31 +310,28 @@@@@@@@@@@ static inline void *phys_to_virt(phys_a
          #define ARCH_PFN_OFFSET		((unsigned long)PHYS_PFN_OFFSET)
          
          #if !defined(CONFIG_SPARSEMEM_VMEMMAP) || defined(CONFIG_DEBUG_VIRTUAL)
- --------#define virt_to_page(kaddr)	pfn_to_page(__pa(kaddr) >> PAGE_SHIFT)
- --------#define _virt_addr_valid(kaddr)	pfn_valid(__pa(kaddr) >> PAGE_SHIFT)
+ ++++++++#define virt_to_page(x)		pfn_to_page(virt_to_pfn(x))
          #else
- --------#define __virt_to_pgoff(kaddr)	(((u64)(kaddr) & ~PAGE_OFFSET) / PAGE_SIZE * sizeof(struct page))
- --------#define __page_to_voff(kaddr)	(((u64)(kaddr) & ~VMEMMAP_START) * PAGE_SIZE / sizeof(struct page))
- --------
- --------#define page_to_virt(page)	({					\
- --------	unsigned long __addr =						\
- --------		((__page_to_voff(page)) | PAGE_OFFSET);			\
- -- -----	const void *__addr_tag =					\
- -- -----		__tag_set((void *)__addr, page_kasan_tag(page));	\
    -     	unsigned long __addr_tag =					\
    -     		 __tag_set(__addr, page_kasan_tag(page));		\
- --------	((void *)__addr_tag);						\
+ ++++++++#define page_to_virt(x)	({						\
+ ++++++++	__typeof__(x) __page = x;					\
+ ++++++++	u64 __idx = ((u64)__page - VMEMMAP_START) / sizeof(struct page);\
+ ++++++++	u64 __addr = PAGE_OFFSET + (__idx * PAGE_SIZE);			\
+ ++++++++	(void *)__tag_set((const void *)__addr, page_kasan_tag(__page));\
          })
          
- --------#define virt_to_page(vaddr)	((struct page *)((__virt_to_pgoff(vaddr)) | VMEMMAP_START))
+ ++++++++#define virt_to_page(x)	({						\
+ ++++++++	u64 __idx = (__tag_reset((u64)x) - PAGE_OFFSET) / PAGE_SIZE;	\
+ ++++++++	u64 __addr = VMEMMAP_START + (__idx * sizeof(struct page));	\
+ ++++++++	(struct page *)__addr;						\
+ ++++++++})
+ ++++++++#endif /* !CONFIG_SPARSEMEM_VMEMMAP || CONFIG_DEBUG_VIRTUAL */
          
- --------#define _virt_addr_valid(kaddr)	pfn_valid((((u64)(kaddr) & ~PAGE_OFFSET) \
- --------					   + PHYS_OFFSET) >> PAGE_SHIFT)
- --------#endif
- --------#endif
+ ++++++++#define virt_addr_valid(addr)	({					\
+ ++++++++	__typeof__(addr) __addr = addr;					\
+ ++++++++	__is_lm_address(__addr) && pfn_valid(virt_to_pfn(__addr));	\
+ ++++++++})
          
- --------#define _virt_addr_is_linear(kaddr)	\
- --------	(__tag_reset((u64)(kaddr)) >= PAGE_OFFSET)
- --------#define virt_addr_valid(kaddr)		\
- --------	(_virt_addr_is_linear(kaddr) && _virt_addr_valid(kaddr))
+ ++++++++#endif /* !ASSEMBLY */
          
          /*
           * Given that the GIC architecture permits ITS implementations that can only be