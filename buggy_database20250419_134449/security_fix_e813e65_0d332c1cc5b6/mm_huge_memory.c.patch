commit e813e65038389b66d2f8dd87588694caf8dc2923
Merge: ccaaaf6fe5a5 4cbc418a44d5
Author: Linus Torvalds <torvalds@linux-foundation.org>
Date:   Fri Jan 31 09:30:41 2020 -0800

    Merge tag 'kvm-5.6-1' of git://git.kernel.org/pub/scm/virt/kvm/kvm
    
    Pull KVM updates from Paolo Bonzini:
     "This is the first batch of KVM changes.
    
      ARM:
       - cleanups and corner case fixes.
    
      PPC:
       - Bugfixes
    
      x86:
       - Support for mapping DAX areas with large nested page table entries.
    
       - Cleanups and bugfixes here too. A particularly important one is a
         fix for FPU load when the thread has TIF_NEED_FPU_LOAD. There is
         also a race condition which could be used in guest userspace to
         exploit the guest kernel, for which the embargo expired today.
    
       - Fast path for IPI delivery vmexits, shaving about 200 clock cycles
         from IPI latency.
    
       - Protect against "Spectre-v1/L1TF" (bring data in the cache via
         speculative out of bound accesses, use L1TF on the sibling
         hyperthread to read it), which unfortunately is an even bigger
         whack-a-mole game than SpectreV1.
    
      Sean continues his mission to rewrite KVM. In addition to a sizable
      number of x86 patches, this time he contributed a pretty large
      refactoring of vCPU creation that affects all architectures but should
      not have any visible effect.
    
      s390 will come next week together with some more x86 patches"
    
    * tag 'kvm-5.6-1' of git://git.kernel.org/pub/scm/virt/kvm/kvm: (204 commits)
      x86/KVM: Clean up host's steal time structure
      x86/KVM: Make sure KVM_VCPU_FLUSH_TLB flag is not missed
      x86/kvm: Cache gfn to pfn translation
      x86/kvm: Introduce kvm_(un)map_gfn()
      x86/kvm: Be careful not to clear KVM_VCPU_FLUSH_TLB bit
      KVM: PPC: Book3S PR: Fix -Werror=return-type build failure
      KVM: PPC: Book3S HV: Release lock on page-out failure path
      KVM: arm64: Treat emulated TVAL TimerValue as a signed 32-bit integer
      KVM: arm64: pmu: Only handle supported event counters
      KVM: arm64: pmu: Fix chained SW_INCR counters
      KVM: arm64: pmu: Don't mark a counter as chained if the odd one is disabled
      KVM: arm64: pmu: Don't increment SW_INCR if PMCR.E is unset
      KVM: x86: Use a typedef for fastop functions
      KVM: X86: Add 'else' to unify fastop and execute call path
      KVM: x86: inline memslot_valid_for_gpte
      KVM: x86/mmu: Use huge pages for DAX-backed files
      KVM: x86/mmu: Remove lpage_is_disallowed() check from set_spte()
      KVM: x86/mmu: Fold max_mapping_level() into kvm_mmu_hugepage_adjust()
      KVM: x86/mmu: Zap any compound page when collapsing sptes
      KVM: x86/mmu: Remove obsolete gfn restoration in FNAME(fetch)
      ...

diff --cc mm/huge_memory.c
index a88093213674,9b3ee79d0edf..f39689a29128
--- a/mm/huge_memory.c
+++ b/mm/huge_memory.c
@@@ -527,13 -527,24 +527,24 @@@ void prep_transhuge_page(struct page *p
  	set_compound_page_dtor(page, TRANSHUGE_PAGE_DTOR);
  }
  
+ bool is_transparent_hugepage(struct page *page)
+ {
+ 	if (!PageCompound(page))
+ 		return 0;
+ 
+ 	page = compound_head(page);
+ 	return is_huge_zero_page(page) ||
+ 	       page[1].compound_dtor == TRANSHUGE_PAGE_DTOR;
+ }
+ EXPORT_SYMBOL_GPL(is_transparent_hugepage);
+ 
 -static unsigned long __thp_get_unmapped_area(struct file *filp, unsigned long len,
 +static unsigned long __thp_get_unmapped_area(struct file *filp,
 +		unsigned long addr, unsigned long len,
  		loff_t off, unsigned long flags, unsigned long size)
  {
 -	unsigned long addr;
  	loff_t off_end = off + len;
  	loff_t off_align = round_up(off, size);
 -	unsigned long len_pad;
 +	unsigned long len_pad, ret;
  
  	if (off_end <= off_align || (off_end - off_align) < size)
  		return 0;