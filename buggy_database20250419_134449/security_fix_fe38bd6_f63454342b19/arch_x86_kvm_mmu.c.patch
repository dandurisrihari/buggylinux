commit fe38bd6862074c0a2b9be7f31f043aaa70b2af5f
Merge: 404e634fdb96 fb3925d06c28
Author: Linus Torvalds <torvalds@linux-foundation.org>
Date:   Wed Sep 18 09:49:13 2019 -0700

    Merge tag 'for-linus' of git://git.kernel.org/pub/scm/virt/kvm/kvm
    
    Pull KVM updates from Paolo Bonzini:
     "s390:
       - ioctl hardening
       - selftests
    
      ARM:
       - ITS translation cache
       - support for 512 vCPUs
       - various cleanups and bugfixes
    
      PPC:
       - various minor fixes and preparation
    
      x86:
       - bugfixes all over the place (posted interrupts, SVM, emulation
         corner cases, blocked INIT)
       - some IPI optimizations"
    
    * tag 'for-linus' of git://git.kernel.org/pub/scm/virt/kvm/kvm: (75 commits)
      KVM: X86: Use IPI shorthands in kvm guest when support
      KVM: x86: Fix INIT signal handling in various CPU states
      KVM: VMX: Introduce exit reason for receiving INIT signal on guest-mode
      KVM: VMX: Stop the preemption timer during vCPU reset
      KVM: LAPIC: Micro optimize IPI latency
      kvm: Nested KVM MMUs need PAE root too
      KVM: x86: set ctxt->have_exception in x86_decode_insn()
      KVM: x86: always stop emulation on page fault
      KVM: nVMX: trace nested VM-Enter failures detected by H/W
      KVM: nVMX: add tracepoint for failed nested VM-Enter
      x86: KVM: svm: Fix a check in nested_svm_vmrun()
      KVM: x86: Return to userspace with internal error on unexpected exit reason
      KVM: x86: Add kvm_emulate_{rd,wr}msr() to consolidate VXM/SVM code
      KVM: x86: Refactor up kvm_{g,s}et_msr() to simplify callers
      doc: kvm: Fix return description of KVM_SET_MSRS
      KVM: X86: Tune PLE Window tracepoint
      KVM: VMX: Change ple_window type to unsigned int
      KVM: X86: Remove tailing newline for tracepoints
      KVM: X86: Trace vcpu_id for vmexit
      KVM: x86: Manually calculate reserved bits when loading PDPTRS
      ...

diff --cc arch/x86/kvm/mmu.c
index a63964e7cec7,9086ee4b64cb..a10af9c87f8a
--- a/arch/x86/kvm/mmu.c
+++ b/arch/x86/kvm/mmu.c
@@@ -5665,87 -5650,21 +5669,99 @@@ int kvm_mmu_create(struct kvm_vcpu *vcp
  		vcpu->arch.guest_mmu.prev_roots[i] = KVM_MMU_ROOT_INFO_INVALID;
  
  	vcpu->arch.nested_mmu.translate_gpa = translate_nested_gpa;
- 	return alloc_mmu_pages(vcpu);
+ 
+ 	ret = alloc_mmu_pages(vcpu, &vcpu->arch.guest_mmu);
+ 	if (ret)
+ 		return ret;
+ 
+ 	ret = alloc_mmu_pages(vcpu, &vcpu->arch.root_mmu);
+ 	if (ret)
+ 		goto fail_allocate_root;
+ 
+ 	return ret;
+  fail_allocate_root:
+ 	free_mmu_pages(&vcpu->arch.guest_mmu);
+ 	return ret;
  }
  
 +
 +static void kvm_zap_obsolete_pages(struct kvm *kvm)
 +{
 +	struct kvm_mmu_page *sp, *node;
 +	LIST_HEAD(invalid_list);
 +	int ign;
 +
 +restart:
 +	list_for_each_entry_safe_reverse(sp, node,
 +	      &kvm->arch.active_mmu_pages, link) {
 +		/*
 +		 * No obsolete valid page exists before a newly created page
 +		 * since active_mmu_pages is a FIFO list.
 +		 */
 +		if (!is_obsolete_sp(kvm, sp))
 +			break;
 +
 +		/*
 +		 * Do not repeatedly zap a root page to avoid unnecessary
 +		 * KVM_REQ_MMU_RELOAD, otherwise we may not be able to
 +		 * progress:
 +		 *    vcpu 0                        vcpu 1
 +		 *                         call vcpu_enter_guest():
 +		 *                            1): handle KVM_REQ_MMU_RELOAD
 +		 *                                and require mmu-lock to
 +		 *                                load mmu
 +		 * repeat:
 +		 *    1): zap root page and
 +		 *        send KVM_REQ_MMU_RELOAD
 +		 *
 +		 *    2): if (cond_resched_lock(mmu-lock))
 +		 *
 +		 *                            2): hold mmu-lock and load mmu
 +		 *
 +		 *                            3): see KVM_REQ_MMU_RELOAD bit
 +		 *                                on vcpu->requests is set
 +		 *                                then return 1 to call
 +		 *                                vcpu_enter_guest() again.
 +		 *            goto repeat;
 +		 *
 +		 * Since we are reversely walking the list and the invalid
 +		 * list will be moved to the head, skip the invalid page
 +		 * can help us to avoid the infinity list walking.
 +		 */
 +		if (sp->role.invalid)
 +			continue;
 +
 +		if (need_resched() || spin_needbreak(&kvm->mmu_lock)) {
 +			kvm_mmu_commit_zap_page(kvm, &invalid_list);
 +			cond_resched_lock(&kvm->mmu_lock);
 +			goto restart;
 +		}
 +
 +		if (__kvm_mmu_prepare_zap_page(kvm, sp, &invalid_list, &ign))
 +			goto restart;
 +	}
 +
 +	kvm_mmu_commit_zap_page(kvm, &invalid_list);
 +}
 +
 +/*
 + * Fast invalidate all shadow pages and use lock-break technique
 + * to zap obsolete pages.
 + *
 + * It's required when memslot is being deleted or VM is being
 + * destroyed, in these cases, we should ensure that KVM MMU does
 + * not use any resource of the being-deleted slot or all slots
 + * after calling the function.
 + */
 +static void kvm_mmu_zap_all_fast(struct kvm *kvm)
 +{
 +	spin_lock(&kvm->mmu_lock);
 +	kvm->arch.mmu_valid_gen++;
 +
 +	kvm_zap_obsolete_pages(kvm);
 +	spin_unlock(&kvm->mmu_lock);
 +}
 +
  static void kvm_mmu_invalidate_zap_pages_in_memslot(struct kvm *kvm,
  			struct kvm_memory_slot *slot,
  			struct kvm_page_track_notifier_node *node)