{
  "hash": "75ef7184053989118d3814c558a9af62e7376a58",
  "hash_short": "75ef7184",
  "subject": "mm, vmstat: add infrastructure for per-node vmstats",
  "body": "Patchset: \"Move LRU page reclaim from zones to nodes v9\"\n\nThis series moves LRUs from the zones to the node.  While this is a\ncurrent rebase, the test results were based on mmotm as of June 23rd.\nConceptually, this series is simple but there are a lot of details.\nSome of the broad motivations for this are;\n\n1. The residency of a page partially depends on what zone the page was\n   allocated from.  This is partially combatted by the fair zone allocation\n   policy but that is a partial solution that introduces overhead in the\n   page allocator paths.\n\n2. Currently, reclaim on node 0 behaves slightly different to node 1. For\n   example, direct reclaim scans in zonelist order and reclaims even if\n   the zone is over the high watermark regardless of the age of pages\n   in that LRU. Kswapd on the other hand starts reclaim on the highest\n   unbalanced zone. A difference in distribution of file/anon pages due\n   to when they were allocated results can result in a difference in\n   again. While the fair zone allocation policy mitigates some of the\n   problems here, the page reclaim results on a multi-zone node will\n   always be different to a single-zone node.\n   it was scheduled on as a result.\n\n3. kswapd and the page allocator scan zones in the opposite order to\n   avoid interfering with each other but it's sensitive to timing.  This\n   mitigates the page allocator using pages that were allocated very recently\n   in the ideal case but it's sensitive to timing. When kswapd is allocating\n   from lower zones then it's great but during the rebalancing of the highest\n   zone, the page allocator and kswapd interfere with each other. It's worse\n   if the highest zone is small and difficult to balance.\n\n4. slab shrinkers are node-based which makes it harder to identify the exact\n   relationship between slab reclaim and LRU reclaim.\n\nThe reason we have zone-based reclaim is that we used to have\nlarge highmem zones in common configurations and it was necessary\nto quickly find ZONE_NORMAL pages for reclaim. Today, this is much\nless of a concern as machines with lots of memory will (or should) use\n64-bit kernels. Combinations of 32-bit hardware and 64-bit hardware are\nrare. Machines that do use highmem should have relatively low highmem:lowmem\nratios than we worried about in the past.\n\nConceptually, moving to node LRUs should be easier to understand. The\npage allocator plays fewer tricks to game reclaim and reclaim behaves\nsimilarly on all nodes.\n\nThe series has been tested on a 16 core UMA machine and a 2-socket 48\ncore NUMA machine. The UMA results are presented in most cases as the NUMA\nmachine behaved similarly.\n\npagealloc\n---------\n\nThis is a microbenchmark that shows the benefit of removing the fair zone\nallocation policy. It was tested uip to order-4 but only orders 0 and 1 are\nshown as the other orders were comparable.\n\n                                           4.7.0-rc4                  4.7.0-rc4\n                                      mmotm-20160623                 nodelru-v9\nMin      total-odr0-1               490.00 (  0.00%)           457.00 (  6.73%)\nMin      total-odr0-2               347.00 (  0.00%)           329.00 (  5.19%)\nMin      total-odr0-4               288.00 (  0.00%)           273.00 (  5.21%)\nMin      total-odr0-8               251.00 (  0.00%)           239.00 (  4.78%)\nMin      total-odr0-16              234.00 (  0.00%)           222.00 (  5.13%)\nMin      total-odr0-32              223.00 (  0.00%)           211.00 (  5.38%)\nMin      total-odr0-64              217.00 (  0.00%)           208.00 (  4.15%)\nMin      total-odr0-128             214.00 (  0.00%)           204.00 (  4.67%)\nMin      total-odr0-256             250.00 (  0.00%)           230.00 (  8.00%)\nMin      total-odr0-512             271.00 (  0.00%)           269.00 (  0.74%)\nMin      total-odr0-1024            291.00 (  0.00%)           282.00 (  3.09%)\nMin      total-odr0-2048            303.00 (  0.00%)           296.00 (  2.31%)\nMin      total-odr0-4096            311.00 (  0.00%)           309.00 (  0.64%)\nMin      total-odr0-8192            316.00 (  0.00%)           314.00 (  0.63%)\nMin      total-odr0-16384           317.00 (  0.00%)           315.00 (  0.63%)\nMin      total-odr1-1               742.00 (  0.00%)           712.00 (  4.04%)\nMin      total-odr1-2               562.00 (  0.00%)           530.00 (  5.69%)\nMin      total-odr1-4               457.00 (  0.00%)           433.00 (  5.25%)\nMin      total-odr1-8               411.00 (  0.00%)           381.00 (  7.30%)\nMin      total-odr1-16              381.00 (  0.00%)           356.00 (  6.56%)\nMin      total-odr1-32              372.00 (  0.00%)           346.00 (  6.99%)\nMin      total-odr1-64              372.00 (  0.00%)           343.00 (  7.80%)\nMin      total-odr1-128             375.00 (  0.00%)           351.00 (  6.40%)\nMin      total-odr1-256             379.00 (  0.00%)           351.00 (  7.39%)\nMin      total-odr1-512             385.00 (  0.00%)           355.00 (  7.79%)\nMin      total-odr1-1024            386.00 (  0.00%)           358.00 (  7.25%)\nMin      total-odr1-2048            390.00 (  0.00%)           362.00 (  7.18%)\nMin      total-odr1-4096            390.00 (  0.00%)           362.00 (  7.18%)\nMin      total-odr1-8192            388.00 (  0.00%)           363.00 (  6.44%)\n\nThis shows a steady improvement throughout. The primary benefit is from\nreduced system CPU usage which is obvious from the overall times;\n\n           4.7.0-rc4   4.7.0-rc4\n        mmotm-20160623nodelru-v8\nUser          189.19      191.80\nSystem       2604.45     2533.56\nElapsed      2855.30     2786.39\n\nThe vmstats also showed that the fair zone allocation policy was definitely\nremoved as can be seen here;\n\n                             4.7.0-rc3   4.7.0-rc3\n                         mmotm-20160623 nodelru-v8\nDMA32 allocs               28794729769           0\nNormal allocs              48432501431 77227309877\nMovable allocs                       0           0\n\ntiobench on ext4\n----------------\n\ntiobench is a benchmark that artifically benefits if old pages remain resident\nwhile new pages get reclaimed. The fair zone allocation policy mitigates this\nproblem so pages age fairly. While the benchmark has problems, it is important\nthat tiobench performance remains constant as it implies that page aging\nproblems that the fair zone allocation policy fixes are not re-introduced.\n\n                                         4.7.0-rc4             4.7.0-rc4\n                                    mmotm-20160623            nodelru-v9\nMin      PotentialReadSpeed        89.65 (  0.00%)       90.21 (  0.62%)\nMin      SeqRead-MB/sec-1          82.68 (  0.00%)       82.01 ( -0.81%)\nMin      SeqRead-MB/sec-2          72.76 (  0.00%)       72.07 ( -0.95%)\nMin      SeqRead-MB/sec-4          75.13 (  0.00%)       74.92 ( -0.28%)\nMin      SeqRead-MB/sec-8          64.91 (  0.00%)       65.19 (  0.43%)\nMin      SeqRead-MB/sec-16         62.24 (  0.00%)       62.22 ( -0.03%)\nMin      RandRead-MB/sec-1          0.88 (  0.00%)        0.88 (  0.00%)\nMin      RandRead-MB/sec-2          0.95 (  0.00%)        0.92 ( -3.16%)\nMin      RandRead-MB/sec-4          1.43 (  0.00%)        1.34 ( -6.29%)\nMin      RandRead-MB/sec-8          1.61 (  0.00%)        1.60 ( -0.62%)\nMin      RandRead-MB/sec-16         1.80 (  0.00%)        1.90 (  5.56%)\nMin      SeqWrite-MB/sec-1         76.41 (  0.00%)       76.85 (  0.58%)\nMin      SeqWrite-MB/sec-2         74.11 (  0.00%)       73.54 ( -0.77%)\nMin      SeqWrite-MB/sec-4         80.05 (  0.00%)       80.13 (  0.10%)\nMin      SeqWrite-MB/sec-8         72.88 (  0.00%)       73.20 (  0.44%)\nMin      SeqWrite-MB/sec-16        75.91 (  0.00%)       76.44 (  0.70%)\nMin      RandWrite-MB/sec-1         1.18 (  0.00%)        1.14 ( -3.39%)\nMin      RandWrite-MB/sec-2         1.02 (  0.00%)        1.03 (  0.98%)\nMin      RandWrite-MB/sec-4         1.05 (  0.00%)        0.98 ( -6.67%)\nMin      RandWrite-MB/sec-8         0.89 (  0.00%)        0.92 (  3.37%)\nMin      RandWrite-MB/sec-16        0.92 (  0.00%)        0.93 (  1.09%)\n\n           4.7.0-rc4   4.7.0-rc4\n        mmotm-20160623 approx-v9\nUser          645.72      525.90\nSystem        403.85      331.75\nElapsed      6795.36     6783.67\n\nThis shows that the series has little or not impact on tiobench which is\ndesirable and a reduction in system CPU usage. It indicates that the fair\nzone allocation policy was removed in a manner that didn't reintroduce\none class of page aging bug. There were only minor differences in overall\nreclaim activity\n\n                             4.7.0-rc4   4.7.0-rc4\n                          mmotm-20160623nodelru-v8\nMinor Faults                    645838      647465\nMajor Faults                       573         640\nSwap Ins                             0           0\nSwap Outs                            0           0\nDMA allocs                           0           0\nDMA32 allocs                  46041453    44190646\nNormal allocs                 78053072    79887245\nMovable allocs                       0           0\nAllocation stalls                   24          67\nStall zone DMA                       0           0\nStall zone DMA32                     0           0\nStall zone Normal                    0           2\nStall zone HighMem                   0           0\nStall zone Movable                   0          65\nDirect pages scanned             10969       30609\nKswapd pages scanned          93375144    93492094\nKswapd pages reclaimed        93372243    93489370\nDirect pages reclaimed           10969       30609\nKswapd efficiency                  99%         99%\nKswapd velocity              13741.015   13781.934\nDirect efficiency                 100%        100%\nDirect velocity                  1.614       4.512\nPercentage direct scans             0%          0%\n\nkswapd activity was roughly comparable. There were differences in direct\nreclaim activity but negligible in the context of the overall workload\n(velocity of 4 pages per second with the patches applied, 1.6 pages per\nsecond in the baseline kernel).\n\npgbench read-only large configuration on ext4\n---------------------------------------------\n\npgbench is a database benchmark that can be sensitive to page reclaim\ndecisions. This also checks if removing the fair zone allocation policy\nis safe\n\npgbench Transactions\n                        4.7.0-rc4             4.7.0-rc4\n                   mmotm-20160623            nodelru-v8\nHmean    1       188.26 (  0.00%)      189.78 (  0.81%)\nHmean    5       330.66 (  0.00%)      328.69 ( -0.59%)\nHmean    12      370.32 (  0.00%)      380.72 (  2.81%)\nHmean    21      368.89 (  0.00%)      369.00 (  0.03%)\nHmean    30      382.14 (  0.00%)      360.89 ( -5.56%)\nHmean    32      428.87 (  0.00%)      432.96 (  0.95%)\n\nNegligible differences again. As with tiobench, overall reclaim activity\nwas comparable.\n\nbonnie++ on ext4\n----------------\n\nNo interesting performance difference, negligible differences on reclaim\nstats.\n\nparalleldd on ext4\n------------------\n\nThis workload uses varying numbers of dd instances to read large amounts of\ndata from disk.\n\n                               4.7.0-rc3             4.7.0-rc3\n                          mmotm-20160623            nodelru-v9\nAmean    Elapsd-1       186.04 (  0.00%)      189.41 ( -1.82%)\nAmean    Elapsd-3       192.27 (  0.00%)      191.38 (  0.46%)\nAmean    Elapsd-5       185.21 (  0.00%)      182.75 (  1.33%)\nAmean    Elapsd-7       183.71 (  0.00%)      182.11 (  0.87%)\nAmean    Elapsd-12      180.96 (  0.00%)      181.58 ( -0.35%)\nAmean    Elapsd-16      181.36 (  0.00%)      183.72 ( -1.30%)\n\n           4.7.0-rc4   4.7.0-rc4\n        mmotm-20160623 nodelru-v9\nUser         1548.01     1552.44\nSystem       8609.71     8515.08\nElapsed      3587.10     3594.54\n\nThere is little or no change in performance but some drop in system CPU usage.\n\n                             4.7.0-rc3   4.7.0-rc3\n                        mmotm-20160623  nodelru-v9\nMinor Faults                    362662      367360\nMajor Faults                      1204        1143\nSwap Ins                            22           0\nSwap Outs                         2855        1029\nDMA allocs                           0           0\nDMA32 allocs                  31409797    28837521\nNormal allocs                 46611853    49231282\nMovable allocs                       0           0\nDirect pages scanned                 0           0\nKswapd pages scanned          40845270    40869088\nKswapd pages reclaimed        40830976    40855294\nDirect pages reclaimed               0           0\nKswapd efficiency                  99%         99%\nKswapd velocity              11386.711   11369.769\nDirect efficiency                 100%        100%\nDirect velocity                  0.000       0.000\nPercentage direct scans             0%          0%\nPage writes by reclaim            2855        1029\nPage writes file                     0           0\nPage writes anon                  2855        1029\nPage reclaim immediate             771        1628\nSector Reads                 293312636   293536360\nSector Writes                 18213568    18186480\nPage rescued immediate               0           0\nSlabs scanned                   128257      132747\nDirect inode steals                181          56\nKswapd inode steals                 59        1131\n\nIt basically shows that kswapd was active at roughly the same rate in\nboth kernels. There was also comparable slab scanning activity and direct\nreclaim was avoided in both cases. There appears to be a large difference\nin numbers of inodes reclaimed but the workload has few active inodes and\nis likely a timing artifact.\n\nstutter\n-------\n\nstutter simulates a simple workload. One part uses a lot of anonymous\nmemory, a second measures mmap latency and a third copies a large file.\nThe primary metric is checking for mmap latency.\n\nstutter\n                             4.7.0-rc4             4.7.0-rc4\n                        mmotm-20160623            nodelru-v8\nMin         mmap     16.6283 (  0.00%)     13.4258 ( 19.26%)\n1st-qrtle   mmap     54.7570 (  0.00%)     34.9121 ( 36.24%)\n2nd-qrtle   mmap     57.3163 (  0.00%)     46.1147 ( 19.54%)\n3rd-qrtle   mmap     58.9976 (  0.00%)     47.1882 ( 20.02%)\nMax-90%     mmap     59.7433 (  0.00%)     47.4453 ( 20.58%)\nMax-93%     mmap     60.1298 (  0.00%)     47.6037 ( 20.83%)\nMax-95%     mmap     73.4112 (  0.00%)     82.8719 (-12.89%)\nMax-99%     mmap     92.8542 (  0.00%)     88.8870 (  4.27%)\nMax         mmap   1440.6569 (  0.00%)    121.4201 ( 91.57%)\nMean        mmap     59.3493 (  0.00%)     42.2991 ( 28.73%)\nBest99%Mean mmap     57.2121 (  0.00%)     41.8207 ( 26.90%)\nBest95%Mean mmap     55.9113 (  0.00%)     39.9620 ( 28.53%)\nBest90%Mean mmap     55.6199 (  0.00%)     39.3124 ( 29.32%)\nBest50%Mean mmap     53.2183 (  0.00%)     33.1307 ( 37.75%)\nBest10%Mean mmap     45.9842 (  0.00%)     20.4040 ( 55.63%)\nBest5%Mean  mmap     43.2256 (  0.00%)     17.9654 ( 58.44%)\nBest1%Mean  mmap     32.9388 (  0.00%)     16.6875 ( 49.34%)\n\nThis shows a number of improvements with the worst-case outlier greatly\nimproved.\n\nSome of the vmstats are interesting\n\n                             4.7.0-rc4   4.7.0-rc4\n                          mmotm-20160623nodelru-v8\nSwap Ins                           163         502\nSwap Outs                            0           0\nDMA allocs                           0           0\nDMA32 allocs                 618719206  1381662383\nNormal allocs                891235743   564138421\nMovable allocs                       0           0\nAllocation stalls                 2603           1\nDirect pages scanned            216787           2\nKswapd pages scanned          50719775    41778378\nKswapd pages reclaimed        41541765    41777639\nDirect pages reclaimed          209159           0\nKswapd efficiency                  81%         99%\nKswapd velocity              16859.554   14329.059\nDirect efficiency                  96%          0%\nDirect velocity                 72.061       0.001\nPercentage direct scans             0%          0%\nPage writes by reclaim         6215049           0\nPage writes file               6215049           0\nPage writes anon                     0           0\nPage reclaim immediate           70673          90\nSector Reads                  81940800    81680456\nSector Writes                100158984    98816036\nPage rescued immediate               0           0\nSlabs scanned                  1366954       22683\n\nWhile this is not guaranteed in all cases, this particular test showed\na large reduction in direct reclaim activity. It's also worth noting\nthat no page writes were issued from reclaim context.\n\nThis series is not without its hazards. There are at least three areas\nthat I'm concerned with even though I could not reproduce any problems in\nthat area.\n\n1. Reclaim/compaction is going to be affected because the amount of reclaim is\n   no longer targetted at a specific zone. Compaction works on a per-zone basis\n   so there is no guarantee that reclaiming a few THP's worth page pages will\n   have a positive impact on compaction success rates.\n\n2. The Slab/LRU reclaim ratio is affected because the frequency the shrinkers\n   are called is now different. This may or may not be a problem but if it\n   is, it'll be because shrinkers are not called enough and some balancing\n   is required.\n\n3. The anon/file reclaim ratio may be affected. Pages about to be dirtied are\n   distributed between zones and the fair zone allocation policy used to do\n   something very similar for anon. The distribution is now different but not\n   necessarily in any way that matters but it's still worth bearing in mind.\n\nVM statistic counters for reclaim decisions are zone-based.  If the kernel\nis to reclaim on a per-node basis then we need to track per-node\nstatistics but there is no infrastructure for that.  The most notable\nchange is that the old node_page_state is renamed to\nsum_zone_node_page_state.  The new node_page_state takes a pglist_data and\nuses per-node stats but none exist yet.  There is some renaming such as\nvm_stat to vm_zone_stat and the addition of vm_node_stat and the renaming\nof mod_state to mod_zone_state.  Otherwise, this is mostly a mechanical\npatch with no functional change.  There is a lot of similarity between the\nnode and zone helpers which is unfortunate but there was no obvious way of\nreusing the code and maintaining type safety.\n\nLink: http://lkml.kernel.org/r/1467970510-21195-2-git-send-email-mgorman@techsingularity.net\nSigned-off-by: Mel Gorman <mgorman@techsingularity.net>\nAcked-by: Johannes Weiner <hannes@cmpxchg.org>\nAcked-by: Vlastimil Babka <vbabka@suse.cz>\nCc: Rik van Riel <riel@surriel.com>\nCc: Vlastimil Babka <vbabka@suse.cz>\nCc: Johannes Weiner <hannes@cmpxchg.org>\nCc: Minchan Kim <minchan@kernel.org>\nCc: Joonsoo Kim <iamjoonsoo.kim@lge.com>\nCc: Hillf Danton <hillf.zj@alibaba-inc.com>\nCc: Michal Hocko <mhocko@kernel.org>\nCc: Minchan Kim <minchan@kernel.org>\nSigned-off-by: Andrew Morton <akpm@linux-foundation.org>\nSigned-off-by: Linus Torvalds <torvalds@linux-foundation.org>",
  "full_message": "mm, vmstat: add infrastructure for per-node vmstats\n\nPatchset: \"Move LRU page reclaim from zones to nodes v9\"\n\nThis series moves LRUs from the zones to the node.  While this is a\ncurrent rebase, the test results were based on mmotm as of June 23rd.\nConceptually, this series is simple but there are a lot of details.\nSome of the broad motivations for this are;\n\n1. The residency of a page partially depends on what zone the page was\n   allocated from.  This is partially combatted by the fair zone allocation\n   policy but that is a partial solution that introduces overhead in the\n   page allocator paths.\n\n2. Currently, reclaim on node 0 behaves slightly different to node 1. For\n   example, direct reclaim scans in zonelist order and reclaims even if\n   the zone is over the high watermark regardless of the age of pages\n   in that LRU. Kswapd on the other hand starts reclaim on the highest\n   unbalanced zone. A difference in distribution of file/anon pages due\n   to when they were allocated results can result in a difference in\n   again. While the fair zone allocation policy mitigates some of the\n   problems here, the page reclaim results on a multi-zone node will\n   always be different to a single-zone node.\n   it was scheduled on as a result.\n\n3. kswapd and the page allocator scan zones in the opposite order to\n   avoid interfering with each other but it's sensitive to timing.  This\n   mitigates the page allocator using pages that were allocated very recently\n   in the ideal case but it's sensitive to timing. When kswapd is allocating\n   from lower zones then it's great but during the rebalancing of the highest\n   zone, the page allocator and kswapd interfere with each other. It's worse\n   if the highest zone is small and difficult to balance.\n\n4. slab shrinkers are node-based which makes it harder to identify the exact\n   relationship between slab reclaim and LRU reclaim.\n\nThe reason we have zone-based reclaim is that we used to have\nlarge highmem zones in common configurations and it was necessary\nto quickly find ZONE_NORMAL pages for reclaim. Today, this is much\nless of a concern as machines with lots of memory will (or should) use\n64-bit kernels. Combinations of 32-bit hardware and 64-bit hardware are\nrare. Machines that do use highmem should have relatively low highmem:lowmem\nratios than we worried about in the past.\n\nConceptually, moving to node LRUs should be easier to understand. The\npage allocator plays fewer tricks to game reclaim and reclaim behaves\nsimilarly on all nodes.\n\nThe series has been tested on a 16 core UMA machine and a 2-socket 48\ncore NUMA machine. The UMA results are presented in most cases as the NUMA\nmachine behaved similarly.\n\npagealloc\n---------\n\nThis is a microbenchmark that shows the benefit of removing the fair zone\nallocation policy. It was tested uip to order-4 but only orders 0 and 1 are\nshown as the other orders were comparable.\n\n                                           4.7.0-rc4                  4.7.0-rc4\n                                      mmotm-20160623                 nodelru-v9\nMin      total-odr0-1               490.00 (  0.00%)           457.00 (  6.73%)\nMin      total-odr0-2               347.00 (  0.00%)           329.00 (  5.19%)\nMin      total-odr0-4               288.00 (  0.00%)           273.00 (  5.21%)\nMin      total-odr0-8               251.00 (  0.00%)           239.00 (  4.78%)\nMin      total-odr0-16              234.00 (  0.00%)           222.00 (  5.13%)\nMin      total-odr0-32              223.00 (  0.00%)           211.00 (  5.38%)\nMin      total-odr0-64              217.00 (  0.00%)           208.00 (  4.15%)\nMin      total-odr0-128             214.00 (  0.00%)           204.00 (  4.67%)\nMin      total-odr0-256             250.00 (  0.00%)           230.00 (  8.00%)\nMin      total-odr0-512             271.00 (  0.00%)           269.00 (  0.74%)\nMin      total-odr0-1024            291.00 (  0.00%)           282.00 (  3.09%)\nMin      total-odr0-2048            303.00 (  0.00%)           296.00 (  2.31%)\nMin      total-odr0-4096            311.00 (  0.00%)           309.00 (  0.64%)\nMin      total-odr0-8192            316.00 (  0.00%)           314.00 (  0.63%)\nMin      total-odr0-16384           317.00 (  0.00%)           315.00 (  0.63%)\nMin      total-odr1-1               742.00 (  0.00%)           712.00 (  4.04%)\nMin      total-odr1-2               562.00 (  0.00%)           530.00 (  5.69%)\nMin      total-odr1-4               457.00 (  0.00%)           433.00 (  5.25%)\nMin      total-odr1-8               411.00 (  0.00%)           381.00 (  7.30%)\nMin      total-odr1-16              381.00 (  0.00%)           356.00 (  6.56%)\nMin      total-odr1-32              372.00 (  0.00%)           346.00 (  6.99%)\nMin      total-odr1-64              372.00 (  0.00%)           343.00 (  7.80%)\nMin      total-odr1-128             375.00 (  0.00%)           351.00 (  6.40%)\nMin      total-odr1-256             379.00 (  0.00%)           351.00 (  7.39%)\nMin      total-odr1-512             385.00 (  0.00%)           355.00 (  7.79%)\nMin      total-odr1-1024            386.00 (  0.00%)           358.00 (  7.25%)\nMin      total-odr1-2048            390.00 (  0.00%)           362.00 (  7.18%)\nMin      total-odr1-4096            390.00 (  0.00%)           362.00 (  7.18%)\nMin      total-odr1-8192            388.00 (  0.00%)           363.00 (  6.44%)\n\nThis shows a steady improvement throughout. The primary benefit is from\nreduced system CPU usage which is obvious from the overall times;\n\n           4.7.0-rc4   4.7.0-rc4\n        mmotm-20160623nodelru-v8\nUser          189.19      191.80\nSystem       2604.45     2533.56\nElapsed      2855.30     2786.39\n\nThe vmstats also showed that the fair zone allocation policy was definitely\nremoved as can be seen here;\n\n                             4.7.0-rc3   4.7.0-rc3\n                         mmotm-20160623 nodelru-v8\nDMA32 allocs               28794729769           0\nNormal allocs              48432501431 77227309877\nMovable allocs                       0           0\n\ntiobench on ext4\n----------------\n\ntiobench is a benchmark that artifically benefits if old pages remain resident\nwhile new pages get reclaimed. The fair zone allocation policy mitigates this\nproblem so pages age fairly. While the benchmark has problems, it is important\nthat tiobench performance remains constant as it implies that page aging\nproblems that the fair zone allocation policy fixes are not re-introduced.\n\n                                         4.7.0-rc4             4.7.0-rc4\n                                    mmotm-20160623            nodelru-v9\nMin      PotentialReadSpeed        89.65 (  0.00%)       90.21 (  0.62%)\nMin      SeqRead-MB/sec-1          82.68 (  0.00%)       82.01 ( -0.81%)\nMin      SeqRead-MB/sec-2          72.76 (  0.00%)       72.07 ( -0.95%)\nMin      SeqRead-MB/sec-4          75.13 (  0.00%)       74.92 ( -0.28%)\nMin      SeqRead-MB/sec-8          64.91 (  0.00%)       65.19 (  0.43%)\nMin      SeqRead-MB/sec-16         62.24 (  0.00%)       62.22 ( -0.03%)\nMin      RandRead-MB/sec-1          0.88 (  0.00%)        0.88 (  0.00%)\nMin      RandRead-MB/sec-2          0.95 (  0.00%)        0.92 ( -3.16%)\nMin      RandRead-MB/sec-4          1.43 (  0.00%)        1.34 ( -6.29%)\nMin      RandRead-MB/sec-8          1.61 (  0.00%)        1.60 ( -0.62%)\nMin      RandRead-MB/sec-16         1.80 (  0.00%)        1.90 (  5.56%)\nMin      SeqWrite-MB/sec-1         76.41 (  0.00%)       76.85 (  0.58%)\nMin      SeqWrite-MB/sec-2         74.11 (  0.00%)       73.54 ( -0.77%)\nMin      SeqWrite-MB/sec-4         80.05 (  0.00%)       80.13 (  0.10%)\nMin      SeqWrite-MB/sec-8         72.88 (  0.00%)       73.20 (  0.44%)\nMin      SeqWrite-MB/sec-16        75.91 (  0.00%)       76.44 (  0.70%)\nMin      RandWrite-MB/sec-1         1.18 (  0.00%)        1.14 ( -3.39%)\nMin      RandWrite-MB/sec-2         1.02 (  0.00%)        1.03 (  0.98%)\nMin      RandWrite-MB/sec-4         1.05 (  0.00%)        0.98 ( -6.67%)\nMin      RandWrite-MB/sec-8         0.89 (  0.00%)        0.92 (  3.37%)\nMin      RandWrite-MB/sec-16        0.92 (  0.00%)        0.93 (  1.09%)\n\n           4.7.0-rc4   4.7.0-rc4\n        mmotm-20160623 approx-v9\nUser          645.72      525.90\nSystem        403.85      331.75\nElapsed      6795.36     6783.67\n\nThis shows that the series has little or not impact on tiobench which is\ndesirable and a reduction in system CPU usage. It indicates that the fair\nzone allocation policy was removed in a manner that didn't reintroduce\none class of page aging bug. There were only minor differences in overall\nreclaim activity\n\n                             4.7.0-rc4   4.7.0-rc4\n                          mmotm-20160623nodelru-v8\nMinor Faults                    645838      647465\nMajor Faults                       573         640\nSwap Ins                             0           0\nSwap Outs                            0           0\nDMA allocs                           0           0\nDMA32 allocs                  46041453    44190646\nNormal allocs                 78053072    79887245\nMovable allocs                       0           0\nAllocation stalls                   24          67\nStall zone DMA                       0           0\nStall zone DMA32                     0           0\nStall zone Normal                    0           2\nStall zone HighMem                   0           0\nStall zone Movable                   0          65\nDirect pages scanned             10969       30609\nKswapd pages scanned          93375144    93492094\nKswapd pages reclaimed        93372243    93489370\nDirect pages reclaimed           10969       30609\nKswapd efficiency                  99%         99%\nKswapd velocity              13741.015   13781.934\nDirect efficiency                 100%        100%\nDirect velocity                  1.614       4.512\nPercentage direct scans             0%          0%\n\nkswapd activity was roughly comparable. There were differences in direct\nreclaim activity but negligible in the context of the overall workload\n(velocity of 4 pages per second with the patches applied, 1.6 pages per\nsecond in the baseline kernel).\n\npgbench read-only large configuration on ext4\n---------------------------------------------\n\npgbench is a database benchmark that can be sensitive to page reclaim\ndecisions. This also checks if removing the fair zone allocation policy\nis safe\n\npgbench Transactions\n                        4.7.0-rc4             4.7.0-rc4\n                   mmotm-20160623            nodelru-v8\nHmean    1       188.26 (  0.00%)      189.78 (  0.81%)\nHmean    5       330.66 (  0.00%)      328.69 ( -0.59%)\nHmean    12      370.32 (  0.00%)      380.72 (  2.81%)\nHmean    21      368.89 (  0.00%)      369.00 (  0.03%)\nHmean    30      382.14 (  0.00%)      360.89 ( -5.56%)\nHmean    32      428.87 (  0.00%)      432.96 (  0.95%)\n\nNegligible differences again. As with tiobench, overall reclaim activity\nwas comparable.\n\nbonnie++ on ext4\n----------------\n\nNo interesting performance difference, negligible differences on reclaim\nstats.\n\nparalleldd on ext4\n------------------\n\nThis workload uses varying numbers of dd instances to read large amounts of\ndata from disk.\n\n                               4.7.0-rc3             4.7.0-rc3\n                          mmotm-20160623            nodelru-v9\nAmean    Elapsd-1       186.04 (  0.00%)      189.41 ( -1.82%)\nAmean    Elapsd-3       192.27 (  0.00%)      191.38 (  0.46%)\nAmean    Elapsd-5       185.21 (  0.00%)      182.75 (  1.33%)\nAmean    Elapsd-7       183.71 (  0.00%)      182.11 (  0.87%)\nAmean    Elapsd-12      180.96 (  0.00%)      181.58 ( -0.35%)\nAmean    Elapsd-16      181.36 (  0.00%)      183.72 ( -1.30%)\n\n           4.7.0-rc4   4.7.0-rc4\n        mmotm-20160623 nodelru-v9\nUser         1548.01     1552.44\nSystem       8609.71     8515.08\nElapsed      3587.10     3594.54\n\nThere is little or no change in performance but some drop in system CPU usage.\n\n                             4.7.0-rc3   4.7.0-rc3\n                        mmotm-20160623  nodelru-v9\nMinor Faults                    362662      367360\nMajor Faults                      1204        1143\nSwap Ins                            22           0\nSwap Outs                         2855        1029\nDMA allocs                           0           0\nDMA32 allocs                  31409797    28837521\nNormal allocs                 46611853    49231282\nMovable allocs                       0           0\nDirect pages scanned                 0           0\nKswapd pages scanned          40845270    40869088\nKswapd pages reclaimed        40830976    40855294\nDirect pages reclaimed               0           0\nKswapd efficiency                  99%         99%\nKswapd velocity              11386.711   11369.769\nDirect efficiency                 100%        100%\nDirect velocity                  0.000       0.000\nPercentage direct scans             0%          0%\nPage writes by reclaim            2855        1029\nPage writes file                     0           0\nPage writes anon                  2855        1029\nPage reclaim immediate             771        1628\nSector Reads                 293312636   293536360\nSector Writes                 18213568    18186480\nPage rescued immediate               0           0\nSlabs scanned                   128257      132747\nDirect inode steals                181          56\nKswapd inode steals                 59        1131\n\nIt basically shows that kswapd was active at roughly the same rate in\nboth kernels. There was also comparable slab scanning activity and direct\nreclaim was avoided in both cases. There appears to be a large difference\nin numbers of inodes reclaimed but the workload has few active inodes and\nis likely a timing artifact.\n\nstutter\n-------\n\nstutter simulates a simple workload. One part uses a lot of anonymous\nmemory, a second measures mmap latency and a third copies a large file.\nThe primary metric is checking for mmap latency.\n\nstutter\n                             4.7.0-rc4             4.7.0-rc4\n                        mmotm-20160623            nodelru-v8\nMin         mmap     16.6283 (  0.00%)     13.4258 ( 19.26%)\n1st-qrtle   mmap     54.7570 (  0.00%)     34.9121 ( 36.24%)\n2nd-qrtle   mmap     57.3163 (  0.00%)     46.1147 ( 19.54%)\n3rd-qrtle   mmap     58.9976 (  0.00%)     47.1882 ( 20.02%)\nMax-90%     mmap     59.7433 (  0.00%)     47.4453 ( 20.58%)\nMax-93%     mmap     60.1298 (  0.00%)     47.6037 ( 20.83%)\nMax-95%     mmap     73.4112 (  0.00%)     82.8719 (-12.89%)\nMax-99%     mmap     92.8542 (  0.00%)     88.8870 (  4.27%)\nMax         mmap   1440.6569 (  0.00%)    121.4201 ( 91.57%)\nMean        mmap     59.3493 (  0.00%)     42.2991 ( 28.73%)\nBest99%Mean mmap     57.2121 (  0.00%)     41.8207 ( 26.90%)\nBest95%Mean mmap     55.9113 (  0.00%)     39.9620 ( 28.53%)\nBest90%Mean mmap     55.6199 (  0.00%)     39.3124 ( 29.32%)\nBest50%Mean mmap     53.2183 (  0.00%)     33.1307 ( 37.75%)\nBest10%Mean mmap     45.9842 (  0.00%)     20.4040 ( 55.63%)\nBest5%Mean  mmap     43.2256 (  0.00%)     17.9654 ( 58.44%)\nBest1%Mean  mmap     32.9388 (  0.00%)     16.6875 ( 49.34%)\n\nThis shows a number of improvements with the worst-case outlier greatly\nimproved.\n\nSome of the vmstats are interesting\n\n                             4.7.0-rc4   4.7.0-rc4\n                          mmotm-20160623nodelru-v8\nSwap Ins                           163         502\nSwap Outs                            0           0\nDMA allocs                           0           0\nDMA32 allocs                 618719206  1381662383\nNormal allocs                891235743   564138421\nMovable allocs                       0           0\nAllocation stalls                 2603           1\nDirect pages scanned            216787           2\nKswapd pages scanned          50719775    41778378\nKswapd pages reclaimed        41541765    41777639\nDirect pages reclaimed          209159           0\nKswapd efficiency                  81%         99%\nKswapd velocity              16859.554   14329.059\nDirect efficiency                  96%          0%\nDirect velocity                 72.061       0.001\nPercentage direct scans             0%          0%\nPage writes by reclaim         6215049           0\nPage writes file               6215049           0\nPage writes anon                     0           0\nPage reclaim immediate           70673          90\nSector Reads                  81940800    81680456\nSector Writes                100158984    98816036\nPage rescued immediate               0           0\nSlabs scanned                  1366954       22683\n\nWhile this is not guaranteed in all cases, this particular test showed\na large reduction in direct reclaim activity. It's also worth noting\nthat no page writes were issued from reclaim context.\n\nThis series is not without its hazards. There are at least three areas\nthat I'm concerned with even though I could not reproduce any problems in\nthat area.\n\n1. Reclaim/compaction is going to be affected because the amount of reclaim is\n   no longer targetted at a specific zone. Compaction works on a per-zone basis\n   so there is no guarantee that reclaiming a few THP's worth page pages will\n   have a positive impact on compaction success rates.\n\n2. The Slab/LRU reclaim ratio is affected because the frequency the shrinkers\n   are called is now different. This may or may not be a problem but if it\n   is, it'll be because shrinkers are not called enough and some balancing\n   is required.\n\n3. The anon/file reclaim ratio may be affected. Pages about to be dirtied are\n   distributed between zones and the fair zone allocation policy used to do\n   something very similar for anon. The distribution is now different but not\n   necessarily in any way that matters but it's still worth bearing in mind.\n\nVM statistic counters for reclaim decisions are zone-based.  If the kernel\nis to reclaim on a per-node basis then we need to track per-node\nstatistics but there is no infrastructure for that.  The most notable\nchange is that the old node_page_state is renamed to\nsum_zone_node_page_state.  The new node_page_state takes a pglist_data and\nuses per-node stats but none exist yet.  There is some renaming such as\nvm_stat to vm_zone_stat and the addition of vm_node_stat and the renaming\nof mod_state to mod_zone_state.  Otherwise, this is mostly a mechanical\npatch with no functional change.  There is a lot of similarity between the\nnode and zone helpers which is unfortunate but there was no obvious way of\nreusing the code and maintaining type safety.\n\nLink: http://lkml.kernel.org/r/1467970510-21195-2-git-send-email-mgorman@techsingularity.net\nSigned-off-by: Mel Gorman <mgorman@techsingularity.net>\nAcked-by: Johannes Weiner <hannes@cmpxchg.org>\nAcked-by: Vlastimil Babka <vbabka@suse.cz>\nCc: Rik van Riel <riel@surriel.com>\nCc: Vlastimil Babka <vbabka@suse.cz>\nCc: Johannes Weiner <hannes@cmpxchg.org>\nCc: Minchan Kim <minchan@kernel.org>\nCc: Joonsoo Kim <iamjoonsoo.kim@lge.com>\nCc: Hillf Danton <hillf.zj@alibaba-inc.com>\nCc: Michal Hocko <mhocko@kernel.org>\nCc: Minchan Kim <minchan@kernel.org>\nSigned-off-by: Andrew Morton <akpm@linux-foundation.org>\nSigned-off-by: Linus Torvalds <torvalds@linux-foundation.org>",
  "author_name": "Mel Gorman",
  "author_email": "mgorman@techsingularity.net",
  "author_date": "Thu Jul 28 15:45:24 2016 -0700",
  "author_date_iso": "2016-07-28T15:45:24-07:00",
  "committer_name": "Linus Torvalds",
  "committer_email": "torvalds@linux-foundation.org",
  "committer_date": "Thu Jul 28 16:07:41 2016 -0700",
  "committer_date_iso": "2016-07-28T16:07:41-07:00",
  "files_changed": [
    "drivers/base/node.c",
    "include/linux/mm.h",
    "include/linux/mmzone.h",
    "include/linux/vmstat.h",
    "mm/page_alloc.c",
    "mm/vmstat.c",
    "mm/workingset.c"
  ],
  "files_changed_count": 7,
  "stats": [
    {
      "file": "drivers/base/node.c",
      "insertions": 41,
      "deletions": 35
    },
    {
      "file": "include/linux/mm.h",
      "insertions": 5,
      "deletions": 0
    },
    {
      "file": "include/linux/mmzone.h",
      "insertions": 13,
      "deletions": 0
    },
    {
      "file": "include/linux/vmstat.h",
      "insertions": 80,
      "deletions": 12
    },
    {
      "file": "mm/page_alloc.c",
      "insertions": 8,
      "deletions": 2
    },
    {
      "file": "mm/vmstat.c",
      "insertions": 272,
      "deletions": 23
    },
    {
      "file": "mm/workingset.c",
      "insertions": 5,
      "deletions": 4
    }
  ],
  "total_insertions": 424,
  "total_deletions": 76,
  "total_changes": 500,
  "parents": [
    "a621184ac6dd415664eae458ed1727ba235f5450"
  ],
  "branches": [
    "* development",
    "master",
    "remotes/origin/HEAD -> origin/master",
    "remotes/origin/master"
  ],
  "tags": [
    "v4.10",
    "v4.10-rc1",
    "v4.10-rc2",
    "v4.10-rc3",
    "v4.10-rc4",
    "v4.10-rc5",
    "v4.10-rc6",
    "v4.10-rc7",
    "v4.10-rc8",
    "v4.11"
  ],
  "is_merge": false,
  "security_info": {
    "cve_ids": [],
    "security_keywords": [
      "sec-1"
    ]
  },
  "fix_type": "security",
  "file_results": [
    {
      "file": "drivers/base/node.c",
      "pre_version": false,
      "post_version": true,
      "patch": true
    },
    {
      "file": "mm/workingset.c",
      "pre_version": false,
      "post_version": true,
      "patch": true
    },
    {
      "file": "include/linux/mm.h",
      "pre_version": false,
      "post_version": true,
      "patch": true
    },
    {
      "file": "include/linux/vmstat.h",
      "pre_version": false,
      "post_version": true,
      "patch": true
    },
    {
      "file": "mm/vmstat.c",
      "pre_version": false,
      "post_version": true,
      "patch": true
    },
    {
      "file": "include/linux/mmzone.h",
      "pre_version": false,
      "post_version": true,
      "patch": true
    },
    {
      "file": "mm/page_alloc.c",
      "pre_version": true,
      "post_version": true,
      "patch": true
    }
  ]
}