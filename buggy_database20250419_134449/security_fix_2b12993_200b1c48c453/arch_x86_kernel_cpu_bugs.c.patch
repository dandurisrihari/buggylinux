commit 2b1299322016731d56807aa49254a5ea3080b6b3
Author: Daniel Sneddon <daniel.sneddon@linux.intel.com>
Date:   Tue Aug 2 15:47:01 2022 -0700

    x86/speculation: Add RSB VM Exit protections
    
    tl;dr: The Enhanced IBRS mitigation for Spectre v2 does not work as
    documented for RET instructions after VM exits. Mitigate it with a new
    one-entry RSB stuffing mechanism and a new LFENCE.
    
    == Background ==
    
    Indirect Branch Restricted Speculation (IBRS) was designed to help
    mitigate Branch Target Injection and Speculative Store Bypass, i.e.
    Spectre, attacks. IBRS prevents software run in less privileged modes
    from affecting branch prediction in more privileged modes. IBRS requires
    the MSR to be written on every privilege level change.
    
    To overcome some of the performance issues of IBRS, Enhanced IBRS was
    introduced.  eIBRS is an "always on" IBRS, in other words, just turn
    it on once instead of writing the MSR on every privilege level change.
    When eIBRS is enabled, more privileged modes should be protected from
    less privileged modes, including protecting VMMs from guests.
    
    == Problem ==
    
    Here's a simplification of how guests are run on Linux' KVM:
    
    void run_kvm_guest(void)
    {
            // Prepare to run guest
            VMRESUME();
            // Clean up after guest runs
    }
    
    The execution flow for that would look something like this to the
    processor:
    
    1. Host-side: call run_kvm_guest()
    2. Host-side: VMRESUME
    3. Guest runs, does "CALL guest_function"
    4. VM exit, host runs again
    5. Host might make some "cleanup" function calls
    6. Host-side: RET from run_kvm_guest()
    
    Now, when back on the host, there are a couple of possible scenarios of
    post-guest activity the host needs to do before executing host code:
    
    * on pre-eIBRS hardware (legacy IBRS, or nothing at all), the RSB is not
    touched and Linux has to do a 32-entry stuffing.
    
    * on eIBRS hardware, VM exit with IBRS enabled, or restoring the host
    IBRS=1 shortly after VM exit, has a documented side effect of flushing
    the RSB except in this PBRSB situation where the software needs to stuff
    the last RSB entry "by hand".
    
    IOW, with eIBRS supported, host RET instructions should no longer be
    influenced by guest behavior after the host retires a single CALL
    instruction.
    
    However, if the RET instructions are "unbalanced" with CALLs after a VM
    exit as is the RET in #6, it might speculatively use the address for the
    instruction after the CALL in #3 as an RSB prediction. This is a problem
    since the (untrusted) guest controls this address.
    
    Balanced CALL/RET instruction pairs such as in step #5 are not affected.
    
    == Solution ==
    
    The PBRSB issue affects a wide variety of Intel processors which
    support eIBRS. But not all of them need mitigation. Today,
    X86_FEATURE_RSB_VMEXIT triggers an RSB filling sequence that mitigates
    PBRSB. Systems setting RSB_VMEXIT need no further mitigation - i.e.,
    eIBRS systems which enable legacy IBRS explicitly.
    
    However, such systems (X86_FEATURE_IBRS_ENHANCED) do not set RSB_VMEXIT
    and most of them need a new mitigation.
    
    Therefore, introduce a new feature flag X86_FEATURE_RSB_VMEXIT_LITE
    which triggers a lighter-weight PBRSB mitigation versus RSB_VMEXIT.
    
    The lighter-weight mitigation performs a CALL instruction which is
    immediately followed by a speculative execution barrier (INT3). This
    steers speculative execution to the barrier -- just like a retpoline
    -- which ensures that speculation can never reach an unbalanced RET.
    Then, ensure this CALL is retired before continuing execution with an
    LFENCE.
    
    In other words, the window of exposure is opened at VM exit where RET
    behavior is troublesome. While the window is open, force RSB predictions
    sampling for RET targets to a dead end at the INT3. Close the window
    with the LFENCE.
    
    There is a subset of eIBRS systems which are not vulnerable to PBRSB.
    Add these systems to the cpu_vuln_whitelist[] as NO_EIBRS_PBRSB.
    Future systems that aren't vulnerable will set ARCH_CAP_PBRSB_NO.
    
      [ bp: Massage, incorporate review comments from Andy Cooper. ]
    
    Signed-off-by: Daniel Sneddon <daniel.sneddon@linux.intel.com>
    Co-developed-by: Pawan Gupta <pawan.kumar.gupta@linux.intel.com>
    Signed-off-by: Pawan Gupta <pawan.kumar.gupta@linux.intel.com>
    Signed-off-by: Borislav Petkov <bp@suse.de>

diff --git a/arch/x86/kernel/cpu/bugs.c b/arch/x86/kernel/cpu/bugs.c
index 6761668100b9..9f7e751b91df 100644
--- a/arch/x86/kernel/cpu/bugs.c
+++ b/arch/x86/kernel/cpu/bugs.c
@@ -1335,6 +1335,53 @@ static void __init spec_ctrl_disable_kernel_rrsba(void)
 	}
 }
 
+static void __init spectre_v2_determine_rsb_fill_type_at_vmexit(enum spectre_v2_mitigation mode)
+{
+	/*
+	 * Similar to context switches, there are two types of RSB attacks
+	 * after VM exit:
+	 *
+	 * 1) RSB underflow
+	 *
+	 * 2) Poisoned RSB entry
+	 *
+	 * When retpoline is enabled, both are mitigated by filling/clearing
+	 * the RSB.
+	 *
+	 * When IBRS is enabled, while #1 would be mitigated by the IBRS branch
+	 * prediction isolation protections, RSB still needs to be cleared
+	 * because of #2.  Note that SMEP provides no protection here, unlike
+	 * user-space-poisoned RSB entries.
+	 *
+	 * eIBRS should protect against RSB poisoning, but if the EIBRS_PBRSB
+	 * bug is present then a LITE version of RSB protection is required,
+	 * just a single call needs to retire before a RET is executed.
+	 */
+	switch (mode) {
+	case SPECTRE_V2_NONE:
+		return;
+
+	case SPECTRE_V2_EIBRS_LFENCE:
+	case SPECTRE_V2_EIBRS:
+		if (boot_cpu_has_bug(X86_BUG_EIBRS_PBRSB)) {
+			setup_force_cpu_cap(X86_FEATURE_RSB_VMEXIT_LITE);
+			pr_info("Spectre v2 / PBRSB-eIBRS: Retire a single CALL on VMEXIT\n");
+		}
+		return;
+
+	case SPECTRE_V2_EIBRS_RETPOLINE:
+	case SPECTRE_V2_RETPOLINE:
+	case SPECTRE_V2_LFENCE:
+	case SPECTRE_V2_IBRS:
+		setup_force_cpu_cap(X86_FEATURE_RSB_VMEXIT);
+		pr_info("Spectre v2 / SpectreRSB : Filling RSB on VMEXIT\n");
+		return;
+	}
+
+	pr_warn_once("Unknown Spectre v2 mode, disabling RSB mitigation at VM exit");
+	dump_stack();
+}
+
 static void __init spectre_v2_select_mitigation(void)
 {
 	enum spectre_v2_mitigation_cmd cmd = spectre_v2_parse_cmdline();
@@ -1485,28 +1532,7 @@ static void __init spectre_v2_select_mitigation(void)
 	setup_force_cpu_cap(X86_FEATURE_RSB_CTXSW);
 	pr_info("Spectre v2 / SpectreRSB mitigation: Filling RSB on context switch\n");
 
-	/*
-	 * Similar to context switches, there are two types of RSB attacks
-	 * after vmexit:
-	 *
-	 * 1) RSB underflow
-	 *
-	 * 2) Poisoned RSB entry
-	 *
-	 * When retpoline is enabled, both are mitigated by filling/clearing
-	 * the RSB.
-	 *
-	 * When IBRS is enabled, while #1 would be mitigated by the IBRS branch
-	 * prediction isolation protections, RSB still needs to be cleared
-	 * because of #2.  Note that SMEP provides no protection here, unlike
-	 * user-space-poisoned RSB entries.
-	 *
-	 * eIBRS, on the other hand, has RSB-poisoning protections, so it
-	 * doesn't need RSB clearing after vmexit.
-	 */
-	if (boot_cpu_has(X86_FEATURE_RETPOLINE) ||
-	    boot_cpu_has(X86_FEATURE_KERNEL_IBRS))
-		setup_force_cpu_cap(X86_FEATURE_RSB_VMEXIT);
+	spectre_v2_determine_rsb_fill_type_at_vmexit(mode);
 
 	/*
 	 * Retpoline protects the kernel, but doesn't protect firmware.  IBRS
@@ -2292,6 +2318,19 @@ static char *ibpb_state(void)
 	return "";
 }
 
+static char *pbrsb_eibrs_state(void)
+{
+	if (boot_cpu_has_bug(X86_BUG_EIBRS_PBRSB)) {
+		if (boot_cpu_has(X86_FEATURE_RSB_VMEXIT_LITE) ||
+		    boot_cpu_has(X86_FEATURE_RSB_VMEXIT))
+			return ", PBRSB-eIBRS: SW sequence";
+		else
+			return ", PBRSB-eIBRS: Vulnerable";
+	} else {
+		return ", PBRSB-eIBRS: Not affected";
+	}
+}
+
 static ssize_t spectre_v2_show_state(char *buf)
 {
 	if (spectre_v2_enabled == SPECTRE_V2_LFENCE)
@@ -2304,12 +2343,13 @@ static ssize_t spectre_v2_show_state(char *buf)
 	    spectre_v2_enabled == SPECTRE_V2_EIBRS_LFENCE)
 		return sprintf(buf, "Vulnerable: eIBRS+LFENCE with unprivileged eBPF and SMT\n");
 
-	return sprintf(buf, "%s%s%s%s%s%s\n",
+	return sprintf(buf, "%s%s%s%s%s%s%s\n",
 		       spectre_v2_strings[spectre_v2_enabled],
 		       ibpb_state(),
 		       boot_cpu_has(X86_FEATURE_USE_IBRS_FW) ? ", IBRS_FW" : "",
 		       stibp_state(),
 		       boot_cpu_has(X86_FEATURE_RSB_CTXSW) ? ", RSB filling" : "",
+		       pbrsb_eibrs_state(),
 		       spectre_v2_module_string());
 }