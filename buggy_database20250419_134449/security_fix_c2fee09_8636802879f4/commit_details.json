{
  "hash": "c2fee09fc167c74a64adb08656cb993ea475197e",
  "hash_short": "c2fee09f",
  "subject": "KVM: x86: Load DR6 with guest value only before entering .vcpu_run() loop",
  "body": "Move the conditional loading of hardware DR6 with the guest's DR6 value\nout of the core .vcpu_run() loop to fix a bug where KVM can load hardware\nwith a stale vcpu->arch.dr6.\n\nWhen the guest accesses a DR and host userspace isn't debugging the guest,\nKVM disables DR interception and loads the guest's values into hardware on\nVM-Enter and saves them on VM-Exit.  This allows the guest to access DRs\nat will, e.g. so that a sequence of DR accesses to configure a breakpoint\nonly generates one VM-Exit.\n\nFor DR0-DR3, the logic/behavior is identical between VMX and SVM, and also\nidentical between KVM_DEBUGREG_BP_ENABLED (userspace debugging the guest)\nand KVM_DEBUGREG_WONT_EXIT (guest using DRs), and so KVM handles loading\nDR0-DR3 in common code, _outside_ of the core kvm_x86_ops.vcpu_run() loop.\n\nBut for DR6, the guest's value doesn't need to be loaded into hardware for\nKVM_DEBUGREG_BP_ENABLED, and SVM provides a dedicated VMCB field whereas\nVMX requires software to manually load the guest value, and so loading the\nguest's value into DR6 is handled by {svm,vmx}_vcpu_run(), i.e. is done\n_inside_ the core run loop.\n\nUnfortunately, saving the guest values on VM-Exit is initiated by common\nx86, again outside of the core run loop.  If the guest modifies DR6 (in\nhardware, when DR interception is disabled), and then the next VM-Exit is\na fastpath VM-Exit, KVM will reload hardware DR6 with vcpu->arch.dr6 and\nclobber the guest's actual value.\n\nThe bug shows up primarily with nested VMX because KVM handles the VMX\npreemption timer in the fastpath, and the window between hardware DR6\nbeing modified (in guest context) and DR6 being read by guest software is\norders of magnitude larger in a nested setup.  E.g. in non-nested, the\nVMX preemption timer would need to fire precisely between #DB injection\nand the #DB handler's read of DR6, whereas with a KVM-on-KVM setup, the\nwindow where hardware DR6 is \"dirty\" extends all the way from L1 writing\nDR6 to VMRESUME (in L1).\n\n    L1's view:\n    ==========\n    <L1 disables DR interception>\n           CPU 0/KVM-7289    [023] d....  2925.640961: kvm_entry: vcpu 0\n A:  L1 Writes DR6\n           CPU 0/KVM-7289    [023] d....  2925.640963: <hack>: Set DRs, DR6 = 0xffff0ff1\n\n B:        CPU 0/KVM-7289    [023] d....  2925.640967: kvm_exit: vcpu 0 reason EXTERNAL_INTERRUPT intr_info 0x800000ec\n\n D: L1 reads DR6, arch.dr6 = 0\n           CPU 0/KVM-7289    [023] d....  2925.640969: <hack>: Sync DRs, DR6 = 0xffff0ff0\n\n           CPU 0/KVM-7289    [023] d....  2925.640976: kvm_entry: vcpu 0\n    L2 reads DR6, L1 disables DR interception\n           CPU 0/KVM-7289    [023] d....  2925.640980: kvm_exit: vcpu 0 reason DR_ACCESS info1 0x0000000000000216\n           CPU 0/KVM-7289    [023] d....  2925.640983: kvm_entry: vcpu 0\n\n           CPU 0/KVM-7289    [023] d....  2925.640983: <hack>: Set DRs, DR6 = 0xffff0ff0\n\n    L2 detects failure\n           CPU 0/KVM-7289    [023] d....  2925.640987: kvm_exit: vcpu 0 reason HLT\n    L1 reads DR6 (confirms failure)\n           CPU 0/KVM-7289    [023] d....  2925.640990: <hack>: Sync DRs, DR6 = 0xffff0ff0\n\n    L0's view:\n    ==========\n    L2 reads DR6, arch.dr6 = 0\n          CPU 23/KVM-5046    [001] d....  3410.005610: kvm_exit: vcpu 23 reason DR_ACCESS info1 0x0000000000000216\n          CPU 23/KVM-5046    [001] .....  3410.005610: kvm_nested_vmexit: vcpu 23 reason DR_ACCESS info1 0x0000000000000216\n\n    L2 => L1 nested VM-Exit\n          CPU 23/KVM-5046    [001] .....  3410.005610: kvm_nested_vmexit_inject: reason: DR_ACCESS ext_inf1: 0x0000000000000216\n\n          CPU 23/KVM-5046    [001] d....  3410.005610: kvm_entry: vcpu 23\n          CPU 23/KVM-5046    [001] d....  3410.005611: kvm_exit: vcpu 23 reason VMREAD\n          CPU 23/KVM-5046    [001] d....  3410.005611: kvm_entry: vcpu 23\n          CPU 23/KVM-5046    [001] d....  3410.005612: kvm_exit: vcpu 23 reason VMREAD\n          CPU 23/KVM-5046    [001] d....  3410.005612: kvm_entry: vcpu 23\n\n    L1 writes DR7, L0 disables DR interception\n          CPU 23/KVM-5046    [001] d....  3410.005612: kvm_exit: vcpu 23 reason DR_ACCESS info1 0x0000000000000007\n          CPU 23/KVM-5046    [001] d....  3410.005613: kvm_entry: vcpu 23\n\n    L0 writes DR6 = 0 (arch.dr6)\n          CPU 23/KVM-5046    [001] d....  3410.005613: <hack>: Set DRs, DR6 = 0xffff0ff0\n\n A: <L1 writes DR6 = 1, no interception, arch.dr6 is still '0'>\n\n B:       CPU 23/KVM-5046    [001] d....  3410.005614: kvm_exit: vcpu 23 reason PREEMPTION_TIMER\n          CPU 23/KVM-5046    [001] d....  3410.005614: kvm_entry: vcpu 23\n\n C: L0 writes DR6 = 0 (arch.dr6)\n          CPU 23/KVM-5046    [001] d....  3410.005614: <hack>: Set DRs, DR6 = 0xffff0ff0\n\n    L1 => L2 nested VM-Enter\n          CPU 23/KVM-5046    [001] d....  3410.005616: kvm_exit: vcpu 23 reason VMRESUME\n\n    L0 reads DR6, arch.dr6 = 0\n\nReported-by: John Stultz <jstultz@google.com>\nCloses: https://lkml.kernel.org/r/CANDhNCq5_F3HfFYABqFGCA1bPd_%2BxgNj-iDQhH4tDk%2Bwi8iZZg%40mail.gmail.com\nFixes: 375e28ffc0cf (\"KVM: X86: Set host DR6 only on VMX and for KVM_DEBUGREG_WONT_EXIT\")\nFixes: d67668e9dd76 (\"KVM: x86, SVM: isolate vcpu->arch.dr6 from vmcb->save.dr6\")\nCc: stable@vger.kernel.org\nCc: Jim Mattson <jmattson@google.com>\nTested-by: John Stultz <jstultz@google.com>\nLink: https://lore.kernel.org/r/20250125011833.3644371-1-seanjc@google.com\nSigned-off-by: Sean Christopherson <seanjc@google.com>",
  "full_message": "KVM: x86: Load DR6 with guest value only before entering .vcpu_run() loop\n\nMove the conditional loading of hardware DR6 with the guest's DR6 value\nout of the core .vcpu_run() loop to fix a bug where KVM can load hardware\nwith a stale vcpu->arch.dr6.\n\nWhen the guest accesses a DR and host userspace isn't debugging the guest,\nKVM disables DR interception and loads the guest's values into hardware on\nVM-Enter and saves them on VM-Exit.  This allows the guest to access DRs\nat will, e.g. so that a sequence of DR accesses to configure a breakpoint\nonly generates one VM-Exit.\n\nFor DR0-DR3, the logic/behavior is identical between VMX and SVM, and also\nidentical between KVM_DEBUGREG_BP_ENABLED (userspace debugging the guest)\nand KVM_DEBUGREG_WONT_EXIT (guest using DRs), and so KVM handles loading\nDR0-DR3 in common code, _outside_ of the core kvm_x86_ops.vcpu_run() loop.\n\nBut for DR6, the guest's value doesn't need to be loaded into hardware for\nKVM_DEBUGREG_BP_ENABLED, and SVM provides a dedicated VMCB field whereas\nVMX requires software to manually load the guest value, and so loading the\nguest's value into DR6 is handled by {svm,vmx}_vcpu_run(), i.e. is done\n_inside_ the core run loop.\n\nUnfortunately, saving the guest values on VM-Exit is initiated by common\nx86, again outside of the core run loop.  If the guest modifies DR6 (in\nhardware, when DR interception is disabled), and then the next VM-Exit is\na fastpath VM-Exit, KVM will reload hardware DR6 with vcpu->arch.dr6 and\nclobber the guest's actual value.\n\nThe bug shows up primarily with nested VMX because KVM handles the VMX\npreemption timer in the fastpath, and the window between hardware DR6\nbeing modified (in guest context) and DR6 being read by guest software is\norders of magnitude larger in a nested setup.  E.g. in non-nested, the\nVMX preemption timer would need to fire precisely between #DB injection\nand the #DB handler's read of DR6, whereas with a KVM-on-KVM setup, the\nwindow where hardware DR6 is \"dirty\" extends all the way from L1 writing\nDR6 to VMRESUME (in L1).\n\n    L1's view:\n    ==========\n    <L1 disables DR interception>\n           CPU 0/KVM-7289    [023] d....  2925.640961: kvm_entry: vcpu 0\n A:  L1 Writes DR6\n           CPU 0/KVM-7289    [023] d....  2925.640963: <hack>: Set DRs, DR6 = 0xffff0ff1\n\n B:        CPU 0/KVM-7289    [023] d....  2925.640967: kvm_exit: vcpu 0 reason EXTERNAL_INTERRUPT intr_info 0x800000ec\n\n D: L1 reads DR6, arch.dr6 = 0\n           CPU 0/KVM-7289    [023] d....  2925.640969: <hack>: Sync DRs, DR6 = 0xffff0ff0\n\n           CPU 0/KVM-7289    [023] d....  2925.640976: kvm_entry: vcpu 0\n    L2 reads DR6, L1 disables DR interception\n           CPU 0/KVM-7289    [023] d....  2925.640980: kvm_exit: vcpu 0 reason DR_ACCESS info1 0x0000000000000216\n           CPU 0/KVM-7289    [023] d....  2925.640983: kvm_entry: vcpu 0\n\n           CPU 0/KVM-7289    [023] d....  2925.640983: <hack>: Set DRs, DR6 = 0xffff0ff0\n\n    L2 detects failure\n           CPU 0/KVM-7289    [023] d....  2925.640987: kvm_exit: vcpu 0 reason HLT\n    L1 reads DR6 (confirms failure)\n           CPU 0/KVM-7289    [023] d....  2925.640990: <hack>: Sync DRs, DR6 = 0xffff0ff0\n\n    L0's view:\n    ==========\n    L2 reads DR6, arch.dr6 = 0\n          CPU 23/KVM-5046    [001] d....  3410.005610: kvm_exit: vcpu 23 reason DR_ACCESS info1 0x0000000000000216\n          CPU 23/KVM-5046    [001] .....  3410.005610: kvm_nested_vmexit: vcpu 23 reason DR_ACCESS info1 0x0000000000000216\n\n    L2 => L1 nested VM-Exit\n          CPU 23/KVM-5046    [001] .....  3410.005610: kvm_nested_vmexit_inject: reason: DR_ACCESS ext_inf1: 0x0000000000000216\n\n          CPU 23/KVM-5046    [001] d....  3410.005610: kvm_entry: vcpu 23\n          CPU 23/KVM-5046    [001] d....  3410.005611: kvm_exit: vcpu 23 reason VMREAD\n          CPU 23/KVM-5046    [001] d....  3410.005611: kvm_entry: vcpu 23\n          CPU 23/KVM-5046    [001] d....  3410.005612: kvm_exit: vcpu 23 reason VMREAD\n          CPU 23/KVM-5046    [001] d....  3410.005612: kvm_entry: vcpu 23\n\n    L1 writes DR7, L0 disables DR interception\n          CPU 23/KVM-5046    [001] d....  3410.005612: kvm_exit: vcpu 23 reason DR_ACCESS info1 0x0000000000000007\n          CPU 23/KVM-5046    [001] d....  3410.005613: kvm_entry: vcpu 23\n\n    L0 writes DR6 = 0 (arch.dr6)\n          CPU 23/KVM-5046    [001] d....  3410.005613: <hack>: Set DRs, DR6 = 0xffff0ff0\n\n A: <L1 writes DR6 = 1, no interception, arch.dr6 is still '0'>\n\n B:       CPU 23/KVM-5046    [001] d....  3410.005614: kvm_exit: vcpu 23 reason PREEMPTION_TIMER\n          CPU 23/KVM-5046    [001] d....  3410.005614: kvm_entry: vcpu 23\n\n C: L0 writes DR6 = 0 (arch.dr6)\n          CPU 23/KVM-5046    [001] d....  3410.005614: <hack>: Set DRs, DR6 = 0xffff0ff0\n\n    L1 => L2 nested VM-Enter\n          CPU 23/KVM-5046    [001] d....  3410.005616: kvm_exit: vcpu 23 reason VMRESUME\n\n    L0 reads DR6, arch.dr6 = 0\n\nReported-by: John Stultz <jstultz@google.com>\nCloses: https://lkml.kernel.org/r/CANDhNCq5_F3HfFYABqFGCA1bPd_%2BxgNj-iDQhH4tDk%2Bwi8iZZg%40mail.gmail.com\nFixes: 375e28ffc0cf (\"KVM: X86: Set host DR6 only on VMX and for KVM_DEBUGREG_WONT_EXIT\")\nFixes: d67668e9dd76 (\"KVM: x86, SVM: isolate vcpu->arch.dr6 from vmcb->save.dr6\")\nCc: stable@vger.kernel.org\nCc: Jim Mattson <jmattson@google.com>\nTested-by: John Stultz <jstultz@google.com>\nLink: https://lore.kernel.org/r/20250125011833.3644371-1-seanjc@google.com\nSigned-off-by: Sean Christopherson <seanjc@google.com>",
  "author_name": "Sean Christopherson",
  "author_email": "seanjc@google.com",
  "author_date": "Fri Jan 24 17:18:33 2025 -0800",
  "author_date_iso": "2025-01-24T17:18:33-08:00",
  "committer_name": "Sean Christopherson",
  "committer_email": "seanjc@google.com",
  "committer_date": "Wed Feb 12 08:59:38 2025 -0800",
  "committer_date_iso": "2025-02-12T08:59:38-08:00",
  "files_changed": [
    "arch/x86/include/asm/kvm-x86-ops.h",
    "arch/x86/include/asm/kvm_host.h",
    "arch/x86/kvm/svm/svm.c",
    "arch/x86/kvm/vmx/main.c",
    "arch/x86/kvm/vmx/vmx.c",
    "arch/x86/kvm/vmx/x86_ops.h",
    "arch/x86/kvm/x86.c"
  ],
  "files_changed_count": 7,
  "stats": [
    {
      "file": "arch/x86/include/asm/kvm-x86-ops.h",
      "insertions": 1,
      "deletions": 0
    },
    {
      "file": "arch/x86/include/asm/kvm_host.h",
      "insertions": 1,
      "deletions": 0
    },
    {
      "file": "arch/x86/kvm/svm/svm.c",
      "insertions": 6,
      "deletions": 7
    },
    {
      "file": "arch/x86/kvm/vmx/main.c",
      "insertions": 1,
      "deletions": 0
    },
    {
      "file": "arch/x86/kvm/vmx/vmx.c",
      "insertions": 6,
      "deletions": 4
    },
    {
      "file": "arch/x86/kvm/vmx/x86_ops.h",
      "insertions": 1,
      "deletions": 0
    },
    {
      "file": "arch/x86/kvm/x86.c",
      "insertions": 3,
      "deletions": 0
    }
  ],
  "total_insertions": 19,
  "total_deletions": 11,
  "total_changes": 30,
  "parents": [
    "46d6c6f3ef0eaff71c2db6d77d4e2ebb7adac34f"
  ],
  "branches": [
    "* development",
    "remotes/origin/HEAD -> origin/master",
    "remotes/origin/master"
  ],
  "tags": [],
  "is_merge": false,
  "security_info": {
    "cve_ids": [],
    "security_keywords": [
      "injection"
    ]
  },
  "fix_type": "security",
  "file_results": [
    {
      "file": "arch/x86/include/asm/kvm-x86-ops.h",
      "pre_version": false,
      "post_version": true,
      "patch": true
    },
    {
      "file": "arch/x86/kvm/vmx/main.c",
      "pre_version": false,
      "post_version": true,
      "patch": true
    },
    {
      "file": "arch/x86/include/asm/kvm_host.h",
      "pre_version": false,
      "post_version": true,
      "patch": true
    },
    {
      "file": "arch/x86/kvm/svm/svm.c",
      "pre_version": false,
      "post_version": true,
      "patch": true
    },
    {
      "file": "arch/x86/kvm/vmx/x86_ops.h",
      "pre_version": false,
      "post_version": true,
      "patch": true
    },
    {
      "file": "arch/x86/kvm/x86.c",
      "pre_version": false,
      "post_version": true,
      "patch": true
    },
    {
      "file": "arch/x86/kvm/vmx/vmx.c",
      "pre_version": false,
      "post_version": true,
      "patch": true
    }
  ]
}