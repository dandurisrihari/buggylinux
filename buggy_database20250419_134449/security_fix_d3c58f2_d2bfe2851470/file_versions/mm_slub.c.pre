commit 8fc8d6664247a6e65cba000789e2e85e2288f6f7
Author: Vlastimil Babka <vbabka@suse.cz>
Date:   Thu Aug 6 23:18:58 2020 -0700

    mm, slub: extend checks guarded by slub_debug static key
    
    There are few more places in SLUB that could benefit from reduced overhead
    of the static key introduced by a previous patch:
    
    - setup_object_debug() called on each object in newly allocated slab page
    - setup_page_debug() called on newly allocated slab page
    - __free_slab() called on freed slab page
    
    Signed-off-by: Vlastimil Babka <vbabka@suse.cz>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Acked-by: Roman Gushchin <guro@fb.com>
    Acked-by: Christoph Lameter <cl@linux.com>
    Cc: Jann Horn <jannh@google.com>
    Cc: Kees Cook <keescook@chromium.org>
    Cc: Vijayanand Jitta <vjitta@codeaurora.org>
    Cc: David Rientjes <rientjes@google.com>
    Cc: Joonsoo Kim <iamjoonsoo.kim@lge.com>
    Cc: Pekka Enberg <penberg@kernel.org>
    Link: http://lkml.kernel.org/r/20200610163135.17364-9-vbabka@suse.cz
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/mm/slub.c b/mm/slub.c
index 97074631a2d1..0b80a8409836 100644
--- a/mm/slub.c
+++ b/mm/slub.c
@@ -1131,7 +1131,7 @@ static inline void dec_slabs_node(struct kmem_cache *s, int node, int objects)
 static void setup_object_debug(struct kmem_cache *s, struct page *page,
 								void *object)
 {
-	if (!(s->flags & (SLAB_STORE_USER|SLAB_RED_ZONE|__OBJECT_POISON)))
+	if (!kmem_cache_debug_flags(s, SLAB_STORE_USER|SLAB_RED_ZONE|__OBJECT_POISON))
 		return;
 
 	init_object(s, object, SLUB_RED_INACTIVE);
@@ -1141,7 +1141,7 @@ static void setup_object_debug(struct kmem_cache *s, struct page *page,
 static
 void setup_page_debug(struct kmem_cache *s, struct page *page, void *addr)
 {
-	if (!(s->flags & SLAB_POISON))
+	if (!kmem_cache_debug_flags(s, SLAB_POISON))
 		return;
 
 	metadata_access_enable();
@@ -1853,7 +1853,7 @@ static void __free_slab(struct kmem_cache *s, struct page *page)
 	int order = compound_order(page);
 	int pages = 1 << order;
 
-	if (s->flags & SLAB_CONSISTENCY_CHECKS) {
+	if (kmem_cache_debug_flags(s, SLAB_CONSISTENCY_CHECKS)) {
 		void *p;
 
 		slab_pad_check(s, page);