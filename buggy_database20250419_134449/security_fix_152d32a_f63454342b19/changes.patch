diff --cc arch/arm64/include/asm/barrier.h
index 065ba482daf0,5a8367a2b868..2175ec0004ed
--- a/arch/arm64/include/asm/barrier.h
+++ b/arch/arm64/include/asm/barrier.h
@@@ -23,8 -23,13 +23,9 @@@
  #define dsb(opt)	asm volatile("dsb " #opt : : : "memory")
  
  #define psb_csync()	asm volatile("hint #17" : : : "memory")
+ #define tsb_csync()	asm volatile("hint #18" : : : "memory")
  #define csdb()		asm volatile("hint #20" : : : "memory")
  
 -#define spec_bar()	asm volatile(ALTERNATIVE("dsb nsh\nisb\n",		\
 -						 SB_BARRIER_INSN"nop\n",	\
 -						 ARM64_HAS_SB))
 -
  #ifdef CONFIG_ARM64_PSEUDO_NMI
  #define pmr_sync()						\
  	do {							\
diff --cc arch/mips/include/asm/kvm_host.h
index 603ad562d101,d0944a75fc8d..fca4547d580f
--- a/arch/mips/include/asm/kvm_host.h
+++ b/arch/mips/include/asm/kvm_host.h
@@@ -822,15 -942,27 +815,10 @@@ bool kvm_mips_flush_gpa_pt(struct kvm *
  int kvm_mips_mkclean_gpa_pt(struct kvm *kvm, gfn_t start_gfn, gfn_t end_gfn);
  pgd_t *kvm_pgd_alloc(void);
  void kvm_mmu_free_memory_caches(struct kvm_vcpu *vcpu);
 -void kvm_trap_emul_invalidate_gva(struct kvm_vcpu *vcpu, unsigned long addr,
 -				  bool user);
 -void kvm_trap_emul_gva_lockless_begin(struct kvm_vcpu *vcpu);
 -void kvm_trap_emul_gva_lockless_end(struct kvm_vcpu *vcpu);
 -
 -enum kvm_mips_fault_result {
 -	KVM_MIPS_MAPPED = 0,
 -	KVM_MIPS_GVA,
 -	KVM_MIPS_GPA,
 -	KVM_MIPS_TLB,
 -	KVM_MIPS_TLBINV,
 -	KVM_MIPS_TLBMOD,
 -};
 -enum kvm_mips_fault_result kvm_trap_emul_gva_fault(struct kvm_vcpu *vcpu,
 -						   unsigned long gva,
 -						   bool write);
  
  #define KVM_ARCH_WANT_MMU_NOTIFIER
- int kvm_unmap_hva_range(struct kvm *kvm,
- 			unsigned long start, unsigned long end, unsigned flags);
- int kvm_set_spte_hva(struct kvm *kvm, unsigned long hva, pte_t pte);
- int kvm_age_hva(struct kvm *kvm, unsigned long start, unsigned long end);
- int kvm_test_age_hva(struct kvm *kvm, unsigned long hva);
  
  /* Emulation */
 -int kvm_get_inst(u32 *opc, struct kvm_vcpu *vcpu, u32 *out);
  enum emulation_result update_pc(struct kvm_vcpu *vcpu, u32 cause);
  int kvm_get_badinstr(u32 *opc, struct kvm_vcpu *vcpu, u32 *out);
  int kvm_get_badinstrp(u32 *opc, struct kvm_vcpu *vcpu, u32 *out);
diff --cc arch/x86/kernel/kvm.c
index 5d32fa477a62,bd01a6131edf..d307c22e5c18
--- a/arch/x86/kernel/kvm.c
+++ b/arch/x86/kernel/kvm.c
@@@ -574,6 -574,49 +574,54 @@@ static void kvm_smp_send_call_func_ipi(
  	}
  }
  
 -static void kvm_flush_tlb_others(const struct cpumask *cpumask,
++static void kvm_flush_tlb_multi(const struct cpumask *cpumask,
+ 			const struct flush_tlb_info *info)
+ {
+ 	u8 state;
+ 	int cpu;
+ 	struct kvm_steal_time *src;
+ 	struct cpumask *flushmask = this_cpu_cpumask_var_ptr(__pv_cpu_mask);
+ 
+ 	cpumask_copy(flushmask, cpumask);
+ 	/*
+ 	 * We have to call flush only on online vCPUs. And
+ 	 * queue flush_on_enter for pre-empted vCPUs
+ 	 */
+ 	for_each_cpu(cpu, flushmask) {
++		/*
++		 * The local vCPU is never preempted, so we do not explicitly
++		 * skip check for local vCPU - it will never be cleared from
++		 * flushmask.
++		 */
+ 		src = &per_cpu(steal_time, cpu);
+ 		state = READ_ONCE(src->preempted);
+ 		if ((state & KVM_VCPU_PREEMPTED)) {
+ 			if (try_cmpxchg(&src->preempted, &state,
+ 					state | KVM_VCPU_FLUSH_TLB))
+ 				__cpumask_clear_cpu(cpu, flushmask);
+ 		}
+ 	}
+ 
 -	native_flush_tlb_others(flushmask, info);
++	native_flush_tlb_multi(flushmask, info);
+ }
+ 
+ static __init int kvm_alloc_cpumask(void)
+ {
+ 	int cpu;
+ 
+ 	if (!kvm_para_available() || nopv)
+ 		return 0;
+ 
+ 	if (pv_tlb_flush_supported() || pv_ipi_supported())
+ 		for_each_possible_cpu(cpu) {
+ 			zalloc_cpumask_var_node(per_cpu_ptr(&__pv_cpu_mask, cpu),
+ 				GFP_KERNEL, cpu_to_node(cpu));
+ 		}
+ 
+ 	return 0;
+ }
+ arch_initcall(kvm_alloc_cpumask);
+ 
  static void __init kvm_smp_prepare_boot_cpu(void)
  {
  	/*
@@@ -655,15 -668,9 +673,9 @@@ static void __init kvm_guest_init(void
  
  	if (kvm_para_has_feature(KVM_FEATURE_STEAL_TIME)) {
  		has_steal_clock = 1;
 -		pv_ops.time.steal_clock = kvm_steal_clock;
 +		static_call_update(pv_steal_clock, kvm_steal_clock);
  	}
  
- 	if (pv_tlb_flush_supported()) {
- 		pv_ops.mmu.flush_tlb_multi = kvm_flush_tlb_multi;
- 		pv_ops.mmu.tlb_remove_table = tlb_remove_table;
- 		pr_info("KVM setup pv remote TLB flush\n");
- 	}
- 
  	if (kvm_para_has_feature(KVM_FEATURE_PV_EOI))
  		apic_set_eoi_write(kvm_guest_apic_eoi_write);
  
@@@ -673,6 -680,12 +685,12 @@@
  	}
  
  #ifdef CONFIG_SMP
+ 	if (pv_tlb_flush_supported()) {
 -		pv_ops.mmu.flush_tlb_others = kvm_flush_tlb_others;
++		pv_ops.mmu.flush_tlb_multi = kvm_flush_tlb_multi;
+ 		pv_ops.mmu.tlb_remove_table = tlb_remove_table;
+ 		pr_info("KVM setup pv remote TLB flush\n");
+ 	}
+ 
  	smp_ops.smp_prepare_boot_cpu = kvm_smp_prepare_boot_cpu;
  	if (pv_sched_yield_supported()) {
  		smp_ops.send_call_func_ipi = kvm_smp_send_call_func_ipi;
diff --cc arch/x86/mm/mem_encrypt_identity.c
index a19374d26101,0c2759b7f03a..04aba7e80a36
--- a/arch/x86/mm/mem_encrypt_identity.c
+++ b/arch/x86/mm/mem_encrypt_identity.c
@@@ -546,9 -535,16 +546,8 @@@ void __init sme_enable(struct boot_para
  		if (!(msr & MSR_K8_SYSCFG_MEM_ENCRYPT))
  			return;
  	} else {
 -		/* For SEV, check the SEV MSR */
 -		msr = __rdmsr(MSR_AMD64_SEV);
 -		if (!(msr & MSR_AMD64_SEV_ENABLED))
 -			return;
 -
 -		/* Save SEV_STATUS to avoid reading MSR again */
 -		sev_status = msr;
 -
  		/* SEV state cannot be controlled by a command line option */
  		sme_me_mask = me_mask;
- 		sev_enabled = true;
  		physical_mask &= ~sme_me_mask;
  		return;
  	}