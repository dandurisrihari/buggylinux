commit 16ca6a607d84bef0129698d8d808f501afd08d43
Author: Marc Zyngier <maz@kernel.org>
Date:   Tue Mar 6 21:48:01 2018 +0000

    KVM: arm/arm64: vgic: Don't populate multiple LRs with the same vintid
    
    The vgic code is trying to be clever when injecting GICv2 SGIs,
    and will happily populate LRs with the same interrupt number if
    they come from multiple vcpus (after all, they are distinct
    interrupt sources).
    
    Unfortunately, this is against the letter of the architecture,
    and the GICv2 architecture spec says "Each valid interrupt stored
    in the List registers must have a unique VirtualID for that
    virtual CPU interface.". GICv3 has similar (although slightly
    ambiguous) restrictions.
    
    This results in guests locking up when using GICv2-on-GICv3, for
    example. The obvious fix is to stop trying so hard, and inject
    a single vcpu per SGI per guest entry. After all, pending SGIs
    with multiple source vcpus are pretty rare, and are mostly seen
    in scenario where the physical CPUs are severely overcomitted.
    
    But as we now only inject a single instance of a multi-source SGI per
    vcpu entry, we may delay those interrupts for longer than strictly
    necessary, and run the risk of injecting lower priority interrupts
    in the meantime.
    
    In order to address this, we adopt a three stage strategy:
    - If we encounter a multi-source SGI in the AP list while computing
      its depth, we force the list to be sorted
    - When populating the LRs, we prevent the injection of any interrupt
      of lower priority than that of the first multi-source SGI we've
      injected.
    - Finally, the injection of a multi-source SGI triggers the request
      of a maintenance interrupt when there will be no pending interrupt
      in the LRs (HCR_NPIE).
    
    At the point where the last pending interrupt in the LRs switches
    from Pending to Active, the maintenance interrupt will be delivered,
    allowing us to add the remaining SGIs using the same process.
    
    Cc: stable@vger.kernel.org
    Fixes: 0919e84c0fc1 ("KVM: arm/arm64: vgic-new: Add IRQ sync/flush framework")
    Acked-by: Christoffer Dall <cdall@kernel.org>
    Signed-off-by: Marc Zyngier <marc.zyngier@arm.com>

diff --git a/virt/kvm/arm/vgic/vgic.c b/virt/kvm/arm/vgic/vgic.c
index 0001858a2c23..8201899126f6 100644
--- a/virt/kvm/arm/vgic/vgic.c
+++ b/virt/kvm/arm/vgic/vgic.c
@@ -710,22 +710,37 @@ static inline void vgic_set_underflow(struct kvm_vcpu *vcpu)
 		vgic_v3_set_underflow(vcpu);
 }
 
+static inline void vgic_set_npie(struct kvm_vcpu *vcpu)
+{
+	if (kvm_vgic_global_state.type == VGIC_V2)
+		vgic_v2_set_npie(vcpu);
+	else
+		vgic_v3_set_npie(vcpu);
+}
+
 /* Requires the ap_list_lock to be held. */
-static int compute_ap_list_depth(struct kvm_vcpu *vcpu)
+static int compute_ap_list_depth(struct kvm_vcpu *vcpu,
+				 bool *multi_sgi)
 {
 	struct vgic_cpu *vgic_cpu = &vcpu->arch.vgic_cpu;
 	struct vgic_irq *irq;
 	int count = 0;
 
+	*multi_sgi = false;
+
 	DEBUG_SPINLOCK_BUG_ON(!spin_is_locked(&vgic_cpu->ap_list_lock));
 
 	list_for_each_entry(irq, &vgic_cpu->ap_list_head, ap_list) {
 		spin_lock(&irq->irq_lock);
 		/* GICv2 SGIs can count for more than one... */
-		if (vgic_irq_is_sgi(irq->intid) && irq->source)
-			count += hweight8(irq->source);
-		else
+		if (vgic_irq_is_sgi(irq->intid) && irq->source) {
+			int w = hweight8(irq->source);
+
+			count += w;
+			*multi_sgi |= (w > 1);
+		} else {
 			count++;
+		}
 		spin_unlock(&irq->irq_lock);
 	}
 	return count;
@@ -736,28 +751,43 @@ static void vgic_flush_lr_state(struct kvm_vcpu *vcpu)
 {
 	struct vgic_cpu *vgic_cpu = &vcpu->arch.vgic_cpu;
 	struct vgic_irq *irq;
-	int count = 0;
+	int count;
+	bool npie = false;
+	bool multi_sgi;
+	u8 prio = 0xff;
 
 	DEBUG_SPINLOCK_BUG_ON(!spin_is_locked(&vgic_cpu->ap_list_lock));
 
-	if (compute_ap_list_depth(vcpu) > kvm_vgic_global_state.nr_lr)
+	count = compute_ap_list_depth(vcpu, &multi_sgi);
+	if (count > kvm_vgic_global_state.nr_lr || multi_sgi)
 		vgic_sort_ap_list(vcpu);
 
+	count = 0;
+
 	list_for_each_entry(irq, &vgic_cpu->ap_list_head, ap_list) {
 		spin_lock(&irq->irq_lock);
 
-		if (unlikely(vgic_target_oracle(irq) != vcpu))
-			goto next;
-
 		/*
-		 * If we get an SGI with multiple sources, try to get
-		 * them in all at once.
+		 * If we have multi-SGIs in the pipeline, we need to
+		 * guarantee that they are all seen before any IRQ of
+		 * lower priority. In that case, we need to filter out
+		 * these interrupts by exiting early. This is easy as
+		 * the AP list has been sorted already.
 		 */
-		do {
+		if (multi_sgi && irq->priority > prio) {
+			spin_unlock(&irq->irq_lock);
+			break;
+		}
+
+		if (likely(vgic_target_oracle(irq) == vcpu)) {
 			vgic_populate_lr(vcpu, irq, count++);
-		} while (irq->source && count < kvm_vgic_global_state.nr_lr);
 
-next:
+			if (irq->source) {
+				npie = true;
+				prio = irq->priority;
+			}
+		}
+
 		spin_unlock(&irq->irq_lock);
 
 		if (count == kvm_vgic_global_state.nr_lr) {
@@ -768,6 +798,9 @@ static void vgic_flush_lr_state(struct kvm_vcpu *vcpu)
 		}
 	}
 
+	if (npie)
+		vgic_set_npie(vcpu);
+
 	vcpu->arch.vgic_cpu.used_lrs = count;
 
 	/* Nuke remaining LRs */